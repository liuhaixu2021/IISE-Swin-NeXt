{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "K-WnVoAossqO",
      "metadata": {
        "id": "K-WnVoAossqO"
      },
      "source": [
        "The following two parts of the code are used for package installation and linking to cloud storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3acf86bb-e0da-4a0e-aae7-62d81d585820",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3acf86bb-e0da-4a0e-aae7-62d81d585820",
        "outputId": "04015de9-7c2a-41a7-9b20-eb2ef27ef6f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.10/dist-packages (2.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.23.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.1.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.1)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.2)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.3)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->catboost) (2023.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.23.3)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.1.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.6.1)\n",
            "Requirement already satisfied: lcensemble in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: catboost==1.1.1 in /usr/local/lib/python3.10/dist-packages (from lcensemble) (1.1.1)\n",
            "Requirement already satisfied: hyperopt==0.2.7 in /usr/local/lib/python3.10/dist-packages (from lcensemble) (0.2.7)\n",
            "Requirement already satisfied: lightgbm==3.3.5 in /usr/local/lib/python3.10/dist-packages (from lcensemble) (3.3.5)\n",
            "Requirement already satisfied: numpy==1.23.3 in /usr/local/lib/python3.10/dist-packages (from lcensemble) (1.23.3)\n",
            "Requirement already satisfied: pandas==1.5.0 in /usr/local/lib/python3.10/dist-packages (from lcensemble) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn==1.1.2 in /usr/local/lib/python3.10/dist-packages (from lcensemble) (1.1.2)\n",
            "Requirement already satisfied: xgboost==1.6.2 in /usr/local/lib/python3.10/dist-packages (from lcensemble) (1.6.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost==1.1.1->lcensemble) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost==1.1.1->lcensemble) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost==1.1.1->lcensemble) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost==1.1.1->lcensemble) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost==1.1.1->lcensemble) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt==0.2.7->lcensemble) (3.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt==0.2.7->lcensemble) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt==0.2.7->lcensemble) (4.66.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt==0.2.7->lcensemble) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt==0.2.7->lcensemble) (0.10.9.7)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.5->lcensemble) (0.42.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.0->lcensemble) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.0->lcensemble) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.2->lcensemble) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.2->lcensemble) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.1.1->lcensemble) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.1.1->lcensemble) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.1.1->lcensemble) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.1.1->lcensemble) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.1.1->lcensemble) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.1.1->lcensemble) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost==1.1.1->lcensemble) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost==1.1.1->lcensemble) (8.2.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.3)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: ignite in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ignite) (2.31.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from ignite) (1.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ignite) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ignite) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ignite) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ignite) (2024.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: monai==1.3.0 in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai==1.3.0) (1.23.3)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai==1.3.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai==1.3.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai==1.3.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai==1.3.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai==1.3.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai==1.3.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai==1.3.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai==1.3.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai==1.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai==1.3.0) (1.3.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.23.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: pynrrd in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from pynrrd) (1.23.3)\n",
            "Requirement already satisfied: nptyping in /usr/local/lib/python3.10/dist-packages (from pynrrd) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pynrrd) (4.10.0)\n",
            "Requirement already satisfied: pytorch_ignite in /usr/local/lib/python3.10/dist-packages (0.4.13)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_ignite) (2.1.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch_ignite) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch_ignite) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch_ignite) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch_ignite) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch_ignite) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch_ignite) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch_ignite) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch_ignite) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.3->pytorch_ignite) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.3->pytorch_ignite) (1.3.0)\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (1.23.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.3)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.23.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.10.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "load end\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders\n",
        "!pip install catboost\n",
        "!pip install pytorch-tabnet\n",
        "!pip install Levenshtein\n",
        "!pip install lcensemble\n",
        "!pip install seaborn\n",
        "!pip install openpyxl\n",
        "!pip install ignite\n",
        "!pip install matplotlib\n",
        "!pip install monai==1.3.0\n",
        "!pip install nibabel\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install pynrrd\n",
        "!pip install pytorch_ignite\n",
        "!pip install scikit_learn\n",
        "!pip install scipy\n",
        "!pip install SimpleITK\n",
        "!pip install scikit-image\n",
        "!pip install typing_extensions\n",
        "!pip install einops\n",
        "print('load end')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "L-iA_IZMxc43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-iA_IZMxc43",
        "outputId": "7004434c-3505-4c46-8a1f-3919f55f6ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2yDmPBawszbn",
      "metadata": {
        "id": "2yDmPBawszbn"
      },
      "source": [
        "\n",
        "The following section of the code defines a set of parameters for the preprocessing process in the style of Monai's dynUnet. Its specific parameters are determined by the results of data exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f0fe96cd-0138-4d20-8a11-a06d8caa00c9",
      "metadata": {
        "id": "f0fe96cd-0138-4d20-8a11-a06d8caa00c9"
      },
      "outputs": [],
      "source": [
        "task_name = {\n",
        "\n",
        "    \"08\": \"Dataset247_monaiData\",\n",
        "\n",
        "}\n",
        "\n",
        "patch_size = {\n",
        "\n",
        "    \"08\": [224, 224, 32],\n",
        "\n",
        "}\n",
        "\n",
        "spacing = {\n",
        "\n",
        "    \"08\": [0.78125, 0.78125, 2.4999999999999996],\n",
        "\n",
        "}\n",
        "\n",
        "clip_values = {\n",
        "\n",
        "    \"08\": [-60, 260],\n",
        "\n",
        "}\n",
        "\n",
        "normalize_values = {\n",
        "\n",
        "    \"08\": [100.78263092041016, 49.40095138549805],\n",
        "\n",
        "}\n",
        "\n",
        "data_loader_params = {\n",
        "\n",
        "    \"08\": {\"batch_size\": 2},\n",
        "\n",
        "}\n",
        "\n",
        "deep_supr_num = {\n",
        "\n",
        "    \"08\": 3,\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HvLzCT97tBnY",
      "metadata": {
        "id": "HvLzCT97tBnY"
      },
      "source": [
        "The following code defines how to obtain our own model and the models provided by the MONAI framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6bdeabfc-687d-4999-86fa-f9bc4d56fe28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bdeabfc-687d-4999-86fa-f9bc4d56fe28",
        "outputId": "af8cb46c-4bdc-49f3-8c6e-e861993f5a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run model\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from monai.networks.nets import DynUNet\n",
        "# from dynunet_check import DynUNet\n",
        "# from task_params import deep_supr_num, patch_size, spacing\n",
        "\n",
        "\n",
        "def get_kernels_strides(task_id):\n",
        "    \"\"\"\n",
        "    This function is only used for decathlon datasets with the provided patch sizes.\n",
        "    When refering this method for other tasks, please ensure that the patch size for each spatial dimension should\n",
        "    be divisible by the product of all strides in the corresponding dimension.\n",
        "    In addition, the minimal spatial size should have at least one dimension that has twice the size of\n",
        "    the product of all strides. For patch sizes that cannot find suitable strides, an error will be raised.\n",
        "\n",
        "    \"\"\"\n",
        "    sizes, spacings = patch_size[task_id], spacing[task_id]\n",
        "    input_size = sizes\n",
        "    strides, kernels = [], []\n",
        "    while True:\n",
        "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
        "        stride = [2 if ratio <= 2 and size >= 8 else 1 for (ratio, size) in zip(spacing_ratio, sizes)]\n",
        "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
        "        if all(s == 1 for s in stride):\n",
        "            break\n",
        "        for idx, (i, j) in enumerate(zip(sizes, stride)):\n",
        "            if i % j != 0:\n",
        "                raise ValueError(\n",
        "                    f\"Patch size is not supported, please try to modify the size {input_size[idx]} in the spatial dimension {idx}.\"\n",
        "                )\n",
        "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
        "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
        "        kernels.append(kernel)\n",
        "        strides.append(stride)\n",
        "\n",
        "    strides.insert(0, len(spacings) * [1])\n",
        "    kernels.append(len(spacings) * [3])\n",
        "    return kernels, strides\n",
        "\n",
        "\n",
        "def get_network(properties, task_id, pretrain_path, checkpoint=None):\n",
        "    n_class = 5  # len(properties[\"labels\"])\n",
        "    in_channels = 1  # len(properties[\"modality\"])\n",
        "    kernels, strides = get_kernels_strides(task_id)\n",
        "\n",
        "    # net = DynUNet(\n",
        "    #     spatial_dims=3,\n",
        "    #     in_channels=in_channels,\n",
        "    #     out_channels=n_class,\n",
        "    #     kernel_size=kernels,\n",
        "    #     strides=strides,\n",
        "    #     upsample_kernel_size=strides[1:],\n",
        "    #     norm_name=\"instance\",\n",
        "    #     deep_supervision=True,\n",
        "    #     deep_supr_num=deep_supr_num[task_id],\n",
        "    # )\n",
        "    # from net_swim import PlainConvUNet\n",
        "    #\n",
        "    # net = PlainConvUNet()\n",
        "    # from network_mx import MedNeXt\n",
        "    # net = MedNeXt(\n",
        "    #     in_channels=1,\n",
        "    #     n_channels=16,\n",
        "    #     n_classes=5,\n",
        "    #     exp_r=[2, 3, 4, 4, 4, 4, 4, 3, 2],  # Expansion ratio as in Swin Transformers\n",
        "    #     # exp_r = 2,\n",
        "    #     kernel_size=3,  # Can test kernel_size\n",
        "    #     deep_supervision=True,  # Can be used to test deep supervision\n",
        "    #     do_res=True,  # Can be used to individually test residual connection\n",
        "    #     do_res_up_down=True,\n",
        "    #     block_counts=[2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
        "    #     # block_counts = [3,4,8,8,8,8,8,4,3],\n",
        "    #     checkpoint_style=None,\n",
        "    #     dim='3d',\n",
        "    #     grn=True,\n",
        "    #     # expansion_type=\"cross\"\n",
        "    # )\n",
        "    # from monai.networks.nets import SwinUNETR\n",
        "    # net = SwinUNETR(img_size=(224, 224, 32), in_channels=1, out_channels=5, feature_size=24)\n",
        "    # from monai.networks.nets import UNETR\n",
        "    # print('run UNETR')\n",
        "    # net = UNETR(\n",
        "    #     in_channels=1,\n",
        "    #     out_channels=5,  # 对于n个类别的分割任务，out_channels应该是n\n",
        "    #     img_size=(224, 224, 32),\n",
        "    #     feature_size=16,\n",
        "    #     hidden_size=768,\n",
        "    #     mlp_dim=3072,\n",
        "    #     num_heads=12,\n",
        "    #     pos_embed='perceptron',\n",
        "    #     norm_name='instance',\n",
        "    #     conv_block=True,\n",
        "    #     res_block=True,\n",
        "    #     dropout_rate=0.0\n",
        "    # )\n",
        "\n",
        "    # from Swin_NeXt import SwinNext\n",
        "    print('run SwinNext cross')\n",
        "    net = SwinNext(\n",
        "        in_channels=1,\n",
        "        n_channels=12,\n",
        "        n_classes=5,\n",
        "        exp_r=[2, 3, 4, 4, 4, 4, 4, 3, 2, 2],\n",
        "        # exp_r = 2,\n",
        "        kernel_size=3,\n",
        "        deep_supervision=True,\n",
        "        do_res=True,\n",
        "        do_res_up_down=True,\n",
        "        block_counts=[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
        "        # block_counts = [3,4,8,8,8,8,8,4,3],\n",
        "        checkpoint_style=None,\n",
        "        dim='3d',\n",
        "        grn=True,\n",
        "\n",
        "        img_size=(224, 224, 32),\n",
        "        expansion_type=\"cross\"  # [\"nochange\", \"add\", \"cat\", \"cross\"]\n",
        "\n",
        "    )\n",
        "\n",
        "    # from MedNeXt_change import MedNeXt\n",
        "    # print('MedNext')\n",
        "    # net = MedNeXt(\n",
        "    #     in_channels=1,\n",
        "    #     n_channels=16,\n",
        "    #     n_classes=5,\n",
        "    #     exp_r=[2, 3, 4, 4, 4, 4, 4, 3, 2],  # Expansion ratio as in Swin Transformers\n",
        "    #     # exp_r = 2,\n",
        "    #     kernel_size=3,  # Can test kernel_size\n",
        "    #     deep_supervision=True,  # Can be used to test deep supervision\n",
        "    #     do_res=True,  # Can be used to individually test residual connection\n",
        "    #     do_res_up_down=True,\n",
        "    #     block_counts=[2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
        "    #     # block_counts = [3,4,8,8,8,8,8,4,3],\n",
        "    #     checkpoint_style=None,\n",
        "    #     dim='3d',\n",
        "    #     grn=True,\n",
        "    #     expansion_type=\"add\", # [\"nochange\", \"add\", \"cat\", \"cross\"]\n",
        "    #     trans_dim=30\n",
        "    # )\n",
        "    # [\"nochange\", \"add\", \"cat\", \"cross\"]\n",
        "    # from swinUNETR import SwinUNETR\n",
        "    # print('run SwinUnetR')\n",
        "    # net = SwinUNETR(img_size=(224, 224, 32),\n",
        "    #                 in_channels=1,\n",
        "    #                 out_channels=5,\n",
        "    #                 feature_size=24,\n",
        "    #                 expansion_type=\"cat\" )# [\"nochange\", \"add\", \"cat\", \"cross\"]\n",
        "\n",
        "    if checkpoint is not None:\n",
        "        pretrain_path = os.path.join(pretrain_path, checkpoint)\n",
        "        print('load prepath:', pretrain_path)\n",
        "        if os.path.exists(pretrain_path):\n",
        "            net.load_state_dict(torch.load(pretrain_path))\n",
        "            print(\"pretrained checkpoint: {} loaded\".format(pretrain_path))\n",
        "        else:\n",
        "            print(\"no pretrained checkpoint\")\n",
        "    return net\n",
        "if __name__ == '__main__':\n",
        "    print('run model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kccAWhHNtahM",
      "metadata": {
        "id": "kccAWhHNtahM"
      },
      "source": [
        "The following code defines the steps required in the preprocessing process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1b9189a6-dd06-45ea-a454-ab4f2283f294",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b9189a6-dd06-45ea-a454-ab4f2283f294",
        "outputId": "8a2293fc-f750-44f1-c47f-dd0481a45ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run transform\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from monai.transforms import (\n",
        "    CastToTyped,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    EnsureChannelFirstd,\n",
        "    LoadImaged,\n",
        "    NormalizeIntensity,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandFlipd,\n",
        "    RandGaussianNoised,\n",
        "    RandGaussianSmoothd,\n",
        "    RandScaleIntensityd,\n",
        "    RandZoomd,\n",
        "    SpatialCrop,\n",
        "    SpatialPadd,\n",
        "    ToTensord,\n",
        "    EnsureTyped,\n",
        ")\n",
        "from monai.transforms.compose import MapTransform\n",
        "from monai.transforms.utils import generate_spatial_bounding_box\n",
        "from skimage.transform import resize\n",
        "\n",
        "# from task_params import clip_values, normalize_values, patch_size, spacing\n",
        "from monai.utils import convert_to_tensor\n",
        "from monai.data.utils import affine_to_spacing\n",
        "import torch\n",
        "\n",
        "def get_task_transforms(mode, task_id, pos_sample_num, neg_sample_num, num_samples):\n",
        "    if mode != \"test\":\n",
        "        keys = [\"image\", \"label\"]\n",
        "    else:\n",
        "        keys = [\"image\"]\n",
        "\n",
        "    load_transforms = [\n",
        "        LoadImaged(keys=keys, image_only=False),\n",
        "        EnsureChannelFirstd(keys=keys),\n",
        "    ]\n",
        "    # 2. sampling\n",
        "    sample_transforms = [\n",
        "        PreprocessAnisotropic(\n",
        "            keys=keys,\n",
        "            clip_values=clip_values[task_id],\n",
        "            pixdim=spacing[task_id],\n",
        "            normalize_values=normalize_values[task_id],\n",
        "            model_mode=mode,\n",
        "        ),\n",
        "        ToTensord(keys=\"image\"),\n",
        "    ]\n",
        "    # 3. spatial transforms\n",
        "    if mode == \"train\":\n",
        "        other_transforms = [\n",
        "            SpatialPadd(keys=[\"image\", \"label\"], spatial_size=patch_size[task_id]),\n",
        "            RandCropByPosNegLabeld(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                label_key=\"label\",\n",
        "                spatial_size=patch_size[task_id],\n",
        "                pos=pos_sample_num,\n",
        "                neg=neg_sample_num,\n",
        "                num_samples=num_samples,\n",
        "                image_key=\"image\",\n",
        "                image_threshold=0,\n",
        "            ),\n",
        "            RandZoomd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                min_zoom=0.9,\n",
        "                max_zoom=1.2,\n",
        "                mode=(\"trilinear\", \"nearest\"),\n",
        "                align_corners=(True, None),\n",
        "                prob=0.15,\n",
        "            ),\n",
        "            RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.15),\n",
        "            RandGaussianSmoothd(\n",
        "                keys=[\"image\"],\n",
        "                sigma_x=(0.5, 1.15),\n",
        "                sigma_y=(0.5, 1.15),\n",
        "                sigma_z=(0.5, 1.15),\n",
        "                prob=0.15,\n",
        "            ),\n",
        "            RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.15),\n",
        "            RandFlipd([\"image\", \"label\"], spatial_axis=[0], prob=0.5),\n",
        "            RandFlipd([\"image\", \"label\"], spatial_axis=[1], prob=0.5),\n",
        "            RandFlipd([\"image\", \"label\"], spatial_axis=[2], prob=0.5),\n",
        "            CastToTyped(keys=[\"image\", \"label\"], dtype=(np.float32, np.uint8)),\n",
        "            EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ]\n",
        "    elif mode == \"validation\":\n",
        "        other_transforms = [\n",
        "            CastToTyped(keys=[\"image\", \"label\"], dtype=(np.float32, np.uint8)),\n",
        "            EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ]\n",
        "    else:\n",
        "        other_transforms = [\n",
        "            CastToTyped(keys=[\"image\"], dtype=(np.float32)),\n",
        "            EnsureTyped(keys=[\"image\"]),\n",
        "        ]\n",
        "\n",
        "    all_transforms = load_transforms + sample_transforms + other_transforms\n",
        "    return Compose(all_transforms)\n",
        "\n",
        "\n",
        "def resample_image(image, shape, anisotrophy_flag):\n",
        "    resized_channels = []\n",
        "    if anisotrophy_flag:\n",
        "        for image_c in image:\n",
        "            resized_slices = []\n",
        "            for i in range(image_c.shape[-1]):\n",
        "                image_c_2d_slice = image_c[:, :, i]\n",
        "                image_c_2d_slice = resize(\n",
        "                    image_c_2d_slice,\n",
        "                    shape[:-1],\n",
        "                    order=3,\n",
        "                    mode=\"edge\",\n",
        "                    cval=0,\n",
        "                    clip=True,\n",
        "                    anti_aliasing=False,\n",
        "                )\n",
        "                resized_slices.append(image_c_2d_slice)\n",
        "            resized = np.stack(resized_slices, axis=-1)\n",
        "            resized = resize(\n",
        "                resized,\n",
        "                shape,\n",
        "                order=0,\n",
        "                mode=\"constant\",\n",
        "                cval=0,\n",
        "                clip=True,\n",
        "                anti_aliasing=False,\n",
        "            )\n",
        "            resized_channels.append(resized)\n",
        "    else:\n",
        "        for image_c in image:\n",
        "            resized = resize(\n",
        "                image_c,\n",
        "                shape,\n",
        "                order=3,\n",
        "                mode=\"edge\",\n",
        "                cval=0,\n",
        "                clip=True,\n",
        "                anti_aliasing=False,\n",
        "            )\n",
        "            resized_channels.append(resized)\n",
        "    resized = np.stack(resized_channels, axis=0)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def resample_label(label, shape, anisotrophy_flag):\n",
        "    reshaped = np.zeros(shape, dtype=np.uint8)\n",
        "    n_class = np.max(label)\n",
        "    if anisotrophy_flag:\n",
        "        shape_2d = shape[:-1]\n",
        "        depth = label.shape[-1]\n",
        "        reshaped_2d = np.zeros((*shape_2d, depth), dtype=np.uint8)\n",
        "\n",
        "        for class_ in range(1, int(n_class) + 1):\n",
        "            for depth_ in range(depth):\n",
        "                mask = label[0, :, :, depth_] == class_\n",
        "                resized_2d = resize(\n",
        "                    mask.astype(float),\n",
        "                    shape_2d,\n",
        "                    order=1,\n",
        "                    mode=\"edge\",\n",
        "                    cval=0,\n",
        "                    clip=True,\n",
        "                    anti_aliasing=False,\n",
        "                )\n",
        "                reshaped_2d[:, :, depth_][resized_2d >= 0.5] = class_\n",
        "        for class_ in range(1, int(n_class) + 1):\n",
        "            mask = reshaped_2d == class_\n",
        "            resized = resize(\n",
        "                mask.astype(float),\n",
        "                shape,\n",
        "                order=0,\n",
        "                mode=\"constant\",\n",
        "                cval=0,\n",
        "                clip=True,\n",
        "                anti_aliasing=False,\n",
        "            )\n",
        "            reshaped[resized >= 0.5] = class_\n",
        "    else:\n",
        "        for class_ in range(1, int(n_class) + 1):\n",
        "            mask = label[0] == class_\n",
        "            resized = resize(\n",
        "                mask.astype(float),\n",
        "                shape,\n",
        "                order=1,\n",
        "                mode=\"edge\",\n",
        "                cval=0,\n",
        "                clip=True,\n",
        "                anti_aliasing=False,\n",
        "            )\n",
        "            reshaped[resized >= 0.5] = class_\n",
        "\n",
        "    reshaped = np.expand_dims(reshaped, 0)\n",
        "    return reshaped\n",
        "\n",
        "\n",
        "def recovery_prediction(prediction, shape, anisotrophy_flag):\n",
        "    reshaped = np.zeros(shape, dtype=np.uint8)\n",
        "    n_class = shape[0]\n",
        "    if anisotrophy_flag:\n",
        "        c, h, w = prediction.shape[:-1]\n",
        "        d = shape[-1]\n",
        "        reshaped_d = np.zeros((c, h, w, d), dtype=np.uint8)\n",
        "        for class_ in range(1, n_class):\n",
        "            mask = prediction[class_] == 1\n",
        "            resized_d = resize(\n",
        "                mask.astype(float),\n",
        "                (h, w, d),\n",
        "                order=0,\n",
        "                mode=\"constant\",\n",
        "                cval=0,\n",
        "                clip=True,\n",
        "                anti_aliasing=False,\n",
        "            )\n",
        "            reshaped_d[class_][resized_d >= 0.5] = 1\n",
        "\n",
        "        for class_ in range(1, n_class):\n",
        "            for depth_ in range(d):\n",
        "                mask = reshaped_d[class_, :, :, depth_] == 1\n",
        "                resized_hw = resize(\n",
        "                    mask.astype(float),\n",
        "                    shape[1:-1],\n",
        "                    order=1,\n",
        "                    mode=\"edge\",\n",
        "                    cval=0,\n",
        "                    clip=True,\n",
        "                    anti_aliasing=False,\n",
        "                )\n",
        "                reshaped[class_, :, :, depth_][resized_hw >= 0.5] = 1\n",
        "    else:\n",
        "        for class_ in range(1, n_class):\n",
        "            mask = prediction[class_] == 1\n",
        "            resized = resize(\n",
        "                mask.astype(float),\n",
        "                shape[1:],\n",
        "                order=1,\n",
        "                mode=\"edge\",\n",
        "                cval=0,\n",
        "                clip=True,\n",
        "                anti_aliasing=False,\n",
        "            )\n",
        "            reshaped[class_][resized >= 0.5] = 1\n",
        "\n",
        "    return reshaped\n",
        "\n",
        "\n",
        "class PreprocessAnisotropic(MapTransform):\n",
        "    \"\"\"\n",
        "    This transform class takes NNUNet's preprocessing method for reference.\n",
        "    That code is in:\n",
        "    https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/preprocessing/preprocessing.py\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        keys,\n",
        "        clip_values,\n",
        "        pixdim,\n",
        "        normalize_values,\n",
        "        model_mode,\n",
        "    ) -> None:\n",
        "        super().__init__(keys)\n",
        "        self.keys = keys\n",
        "        self.low = clip_values[0]\n",
        "        self.high = clip_values[1]\n",
        "        self.target_spacing = pixdim\n",
        "        self.mean = normalize_values[0]\n",
        "        self.std = normalize_values[1]\n",
        "        self.training = False\n",
        "        self.crop_foreg = CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\")\n",
        "        self.normalize_intensity = NormalizeIntensity(nonzero=True, channel_wise=True)\n",
        "        if model_mode in [\"train\"]:\n",
        "            self.training = True\n",
        "\n",
        "    def calculate_new_shape(self, spacing, shape):\n",
        "        spacing_ratio = np.array(spacing) / np.array(self.target_spacing)\n",
        "        new_shape = (spacing_ratio * np.array(shape)).astype(int).tolist()\n",
        "        return new_shape\n",
        "\n",
        "    def check_anisotrophy(self, spacing):\n",
        "        def check(spacing):\n",
        "\n",
        "            return np.max(spacing) / np.min(spacing) >= 3\n",
        "\n",
        "        return check(spacing) or check(self.target_spacing)\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # load data\n",
        "        d = dict(data)\n",
        "        image = d[\"image\"]\n",
        "        meta_dict = d[\"image\"].meta\n",
        "        # spacing_vals = convert_to_tensor(meta_dict['affine'], track_meta=False, wrap_sequence=True)\n",
        "        spacing_vals = meta_dict['affine']\n",
        "        if spacing_vals.ndim == 1:  # vector\n",
        "            image_spacings = spacing_vals[:3][None]\n",
        "        elif spacing_vals.ndim == 2:  # matrix\n",
        "            image_spacings = affine_to_spacing(spacing_vals, 3)[None]\n",
        "        else:\n",
        "            raise ValueError(\"data[spacing_key] must be a vector or a matrix.\")\n",
        "\n",
        "        # image_spacings = d[\"image_meta_dict\"][\"pixdim\"][1:4].tolist()\n",
        "        # print(image_spacings)\n",
        "        # image_spacings = [0.761719, 0.761719, 2.5]\n",
        "        image_spacings = image_spacings.tolist()[0]\n",
        "        if \"label\" in self.keys:\n",
        "            label = d[\"label\"]\n",
        "            label[label < 0] = 0\n",
        "\n",
        "        if self.training:\n",
        "            # only task 04 does not be impacted\n",
        "            cropped_data = self.crop_foreg({\"image\": image, \"label\": label})\n",
        "            image, label = cropped_data[\"image\"], cropped_data[\"label\"]\n",
        "        else:\n",
        "            d[\"original_shape\"] = np.array(image.shape[1:])\n",
        "            box_start, box_end = generate_spatial_bounding_box(image)\n",
        "            image = SpatialCrop(roi_start=box_start, roi_end=box_end)(image)\n",
        "            d[\"bbox\"] = np.vstack([box_start, box_end])\n",
        "            d[\"crop_shape\"] = np.array(image.shape[1:])\n",
        "\n",
        "        original_shape = image.shape[1:]\n",
        "        # calculate shape\n",
        "        resample_flag = False\n",
        "        anisotrophy_flag = False\n",
        "\n",
        "        image = image.numpy()\n",
        "        if self.target_spacing != image_spacings:\n",
        "            # resample\n",
        "            resample_flag = True\n",
        "            resample_shape = self.calculate_new_shape(image_spacings, original_shape)\n",
        "            anisotrophy_flag = self.check_anisotrophy(image_spacings)\n",
        "            image = resample_image(image, resample_shape, anisotrophy_flag)\n",
        "            if self.training:\n",
        "                label = resample_label(label, resample_shape, anisotrophy_flag)\n",
        "\n",
        "        d[\"resample_flag\"] = resample_flag\n",
        "        d[\"anisotrophy_flag\"] = anisotrophy_flag\n",
        "        # clip image for CT dataset\n",
        "        if self.low != 0 or self.high != 0:\n",
        "            image = np.clip(image, self.low, self.high)\n",
        "            image = (image - self.mean) / self.std\n",
        "        else:\n",
        "            image = self.normalize_intensity(image.copy())\n",
        "\n",
        "        # table_info_list = d['table']\n",
        "        # new_matrix = np.zeros_like(image)[:,:,:,0] # 1, 512, 512, n\n",
        "        # new_matrix = np.expand_dims(new_matrix, axis=-1)\n",
        "        # for id_z, str_value in enumerate(table_info_list):\n",
        "        #     if id_z == 0:\n",
        "        #         continue\n",
        "        #     feature_value = np.array(str_value)\n",
        "        #     new_matrix[0,id_z - 1,0,0] = feature_value\n",
        "        #\n",
        "        # new_image = np.concatenate((image, new_matrix), axis=-1)\n",
        "        new_image = image\n",
        "        d[\"image\"] = new_image\n",
        "\n",
        "        if \"label\" in self.keys:\n",
        "            d[\"label\"] = label\n",
        "\n",
        "        return d\n",
        "if __name__ == \"__main__\":\n",
        "    print('run transform')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uRArnfgWtjs1",
      "metadata": {
        "id": "uRArnfgWtjs1"
      },
      "source": [
        "The following code defines the sliding window inference process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9d816b2b-e245-4f13-8b40-1c6e23f6adf9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d816b2b-e245-4f13-8b40-1c6e23f6adf9",
        "outputId": "a1899b25-bec6-48c5-b28d-c58091eb45ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run utils\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import itertools\n",
        "from collections.abc import Callable, Mapping, Sequence\n",
        "from typing import Any, Iterable\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from monai.data.meta_tensor import MetaTensor\n",
        "from monai.data.utils import compute_importance_map, dense_patch_slices, get_valid_patch_size\n",
        "from monai.utils import (\n",
        "    BlendMode,\n",
        "    PytorchPadMode,\n",
        "    convert_data_type,\n",
        "    convert_to_dst_type,\n",
        "    ensure_tuple,\n",
        "    ensure_tuple_rep,\n",
        "    fall_back_tuple,\n",
        "    look_up_option,\n",
        "    optional_import,\n",
        "    pytorch_after,\n",
        ")\n",
        "\n",
        "tqdm, _ = optional_import(\"tqdm\", name=\"tqdm\")\n",
        "_nearest_mode = \"nearest-exact\" if pytorch_after(1, 11) else \"nearest\"\n",
        "\n",
        "__all__ = [\"sliding_window_inference\"]\n",
        "\n",
        "\n",
        "def sliding_window_inference(\n",
        "    inputs: torch.Tensor | MetaTensor,\n",
        "    roi_size: Sequence[int] | int,\n",
        "    sw_batch_size: int,\n",
        "    predictor: Callable[..., torch.Tensor | Sequence[torch.Tensor] | dict[Any, torch.Tensor]],\n",
        "    overlap: Sequence[float] | float = 0.25,\n",
        "    mode: BlendMode | str = BlendMode.CONSTANT,\n",
        "    sigma_scale: Sequence[float] | float = 0.125,\n",
        "    padding_mode: PytorchPadMode | str = PytorchPadMode.CONSTANT,\n",
        "    cval: float = 0.0,\n",
        "    sw_device: torch.device | str | None = None,\n",
        "    device: torch.device | str | None = None,\n",
        "    progress: bool = False,\n",
        "    roi_weight_map: torch.Tensor | None = None,\n",
        "    process_fn: Callable | None = None,\n",
        "    buffer_steps: int | None = None,\n",
        "    buffer_dim: int = -1,\n",
        "    with_coord: bool = False,\n",
        "    *args: Any,\n",
        "    **kwargs: Any,\n",
        ") -> torch.Tensor | tuple[torch.Tensor, ...] | dict[Any, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Sliding window inference on `inputs` with `predictor`.\n",
        "\n",
        "    The outputs of `predictor` could be a tensor, a tuple, or a dictionary of tensors.\n",
        "    Each output in the tuple or dict value is allowed to have different resolutions with respect to the input.\n",
        "    e.g., the input patch spatial size is [128,128,128], the output (a tuple of two patches) patch sizes\n",
        "    could be ([128,64,256], [64,32,128]).\n",
        "    In this case, the parameter `overlap` and `roi_size` need to be carefully chosen to ensure the output ROI is still\n",
        "    an integer. If the predictor's input and output spatial sizes are not equal, we recommend choosing the parameters\n",
        "    so that `overlap*roi_size*output_size/input_size` is an integer (for each spatial dimension).\n",
        "\n",
        "    When roi_size is larger than the inputs' spatial size, the input image are padded during inference.\n",
        "    To maintain the same spatial sizes, the output image will be cropped to the original input size.\n",
        "\n",
        "    Args:\n",
        "        inputs: input image to be processed (assuming NCHW[D])\n",
        "        roi_size: the spatial window size for inferences.\n",
        "            When its components have None or non-positives, the corresponding inputs dimension will be used.\n",
        "            if the components of the `roi_size` are non-positive values, the transform will use the\n",
        "            corresponding components of img size. For example, `roi_size=(32, -1)` will be adapted\n",
        "            to `(32, 64)` if the second spatial dimension size of img is `64`.\n",
        "        sw_batch_size: the batch size to run window slices.\n",
        "        predictor: given input tensor ``patch_data`` in shape NCHW[D],\n",
        "            The outputs of the function call ``predictor(patch_data)`` should be a tensor, a tuple, or a dictionary\n",
        "            with Tensor values. Each output in the tuple or dict value should have the same batch_size, i.e. NM'H'W'[D'];\n",
        "            where H'W'[D'] represents the output patch's spatial size, M is the number of output channels,\n",
        "            N is `sw_batch_size`, e.g., the input shape is (7, 1, 128,128,128),\n",
        "            the output could be a tuple of two tensors, with shapes: ((7, 5, 128, 64, 256), (7, 4, 64, 32, 128)).\n",
        "            In this case, the parameter `overlap` and `roi_size` need to be carefully chosen\n",
        "            to ensure the scaled output ROI sizes are still integers.\n",
        "            If the `predictor`'s input and output spatial sizes are different,\n",
        "            we recommend choosing the parameters so that ``overlap*roi_size*zoom_scale`` is an integer for each dimension.\n",
        "        overlap: Amount of overlap between scans along each spatial dimension, defaults to ``0.25``.\n",
        "        mode: {``\"constant\"``, ``\"gaussian\"``}\n",
        "            How to blend output of overlapping windows. Defaults to ``\"constant\"``.\n",
        "\n",
        "            - ``\"constant``\": gives equal weight to all predictions.\n",
        "            - ``\"gaussian``\": gives less weight to predictions on edges of windows.\n",
        "\n",
        "        sigma_scale: the standard deviation coefficient of the Gaussian window when `mode` is ``\"gaussian\"``.\n",
        "            Default: 0.125. Actual window sigma is ``sigma_scale`` * ``dim_size``.\n",
        "            When sigma_scale is a sequence of floats, the values denote sigma_scale at the corresponding\n",
        "            spatial dimensions.\n",
        "        padding_mode: {``\"constant\"``, ``\"reflect\"``, ``\"replicate\"``, ``\"circular\"``}\n",
        "            Padding mode for ``inputs``, when ``roi_size`` is larger than inputs. Defaults to ``\"constant\"``\n",
        "            See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
        "        cval: fill value for 'constant' padding mode. Default: 0\n",
        "        sw_device: device for the window data.\n",
        "            By default the device (and accordingly the memory) of the `inputs` is used.\n",
        "            Normally `sw_device` should be consistent with the device where `predictor` is defined.\n",
        "        device: device for the stitched output prediction.\n",
        "            By default the device (and accordingly the memory) of the `inputs` is used. If for example\n",
        "            set to device=torch.device('cpu') the gpu memory consumption is less and independent of the\n",
        "            `inputs` and `roi_size`. Output is on the `device`.\n",
        "        progress: whether to print a `tqdm` progress bar.\n",
        "        roi_weight_map: pre-computed (non-negative) weight map for each ROI.\n",
        "            If not given, and ``mode`` is not `constant`, this map will be computed on the fly.\n",
        "        process_fn: process inference output and adjust the importance map per window\n",
        "        buffer_steps: the number of sliding window iterations along the ``buffer_dim``\n",
        "            to be buffered on ``sw_device`` before writing to ``device``.\n",
        "            (Typically, ``sw_device`` is ``cuda`` and ``device`` is ``cpu``.)\n",
        "            default is None, no buffering. For the buffer dim, when spatial size is divisible by buffer_steps*roi_size,\n",
        "            (i.e. no overlapping among the buffers) non_blocking copy may be automatically enabled for efficiency.\n",
        "        buffer_dim: the spatial dimension along which the buffers are created.\n",
        "            0 indicates the first spatial dimension. Default is -1, the last spatial dimension.\n",
        "        with_coord: whether to pass the window coordinates to ``predictor``. Default is False.\n",
        "            If True, the signature of ``predictor`` should be ``predictor(patch_data, patch_coord, ...)``.\n",
        "        args: optional args to be passed to ``predictor``.\n",
        "        kwargs: optional keyword args to be passed to ``predictor``.\n",
        "\n",
        "    Note:\n",
        "        - input must be channel-first and have a batch dim, supports N-D sliding window.\n",
        "\n",
        "    \"\"\"\n",
        "    table = inputs[\"table\"]\n",
        "    inputs = inputs[\"inputs\"]\n",
        "    buffered = buffer_steps is not None and buffer_steps > 0\n",
        "    num_spatial_dims = len(inputs.shape) - 2\n",
        "    if buffered:\n",
        "        if buffer_dim < -num_spatial_dims or buffer_dim > num_spatial_dims:\n",
        "            raise ValueError(f\"buffer_dim must be in [{-num_spatial_dims}, {num_spatial_dims}], got {buffer_dim}.\")\n",
        "        if buffer_dim < 0:\n",
        "            buffer_dim += num_spatial_dims\n",
        "    overlap = ensure_tuple_rep(overlap, num_spatial_dims)\n",
        "    for o in overlap:\n",
        "        if o < 0 or o >= 1:\n",
        "            raise ValueError(f\"overlap must be >= 0 and < 1, got {overlap}.\")\n",
        "    compute_dtype = inputs.dtype\n",
        "\n",
        "    # determine image spatial size and batch size\n",
        "    # Note: all input images must have the same image size and batch size\n",
        "    batch_size, _, *image_size_ = inputs.shape\n",
        "    device = device or inputs.device\n",
        "    sw_device = sw_device or inputs.device\n",
        "\n",
        "    temp_meta = None\n",
        "    if isinstance(inputs, MetaTensor):\n",
        "        temp_meta = MetaTensor([]).copy_meta_from(inputs, copy_attr=False)\n",
        "    inputs = convert_data_type(inputs, torch.Tensor, wrap_sequence=True)[0]\n",
        "    roi_size = fall_back_tuple(roi_size, image_size_)\n",
        "\n",
        "    # in case that image size is smaller than roi size\n",
        "    image_size = tuple(max(image_size_[i], roi_size[i]) for i in range(num_spatial_dims))\n",
        "    pad_size = []\n",
        "    for k in range(len(inputs.shape) - 1, 1, -1):\n",
        "        diff = max(roi_size[k - 2] - inputs.shape[k], 0)\n",
        "        half = diff // 2\n",
        "        pad_size.extend([half, diff - half])\n",
        "    if any(pad_size):\n",
        "        inputs = F.pad(inputs, pad=pad_size, mode=look_up_option(padding_mode, PytorchPadMode), value=cval)\n",
        "\n",
        "    # Store all slices\n",
        "    scan_interval = _get_scan_interval(image_size, roi_size, num_spatial_dims, overlap)\n",
        "    slices = dense_patch_slices(image_size, roi_size, scan_interval, return_slice=not buffered)\n",
        "\n",
        "    num_win = len(slices)  # number of windows per image\n",
        "    total_slices = num_win * batch_size  # total number of windows\n",
        "    windows_range: Iterable\n",
        "    if not buffered:\n",
        "        non_blocking = False\n",
        "        windows_range = range(0, total_slices, sw_batch_size)\n",
        "    else:\n",
        "        slices, n_per_batch, b_slices, windows_range = _create_buffered_slices(\n",
        "            slices, batch_size, sw_batch_size, buffer_dim, buffer_steps\n",
        "        )\n",
        "        non_blocking, _ss = torch.cuda.is_available(), -1\n",
        "        for x in b_slices[:n_per_batch]:\n",
        "            if x[1] < _ss:  # detect overlapping slices\n",
        "                non_blocking = False\n",
        "                break\n",
        "            _ss = x[2]\n",
        "\n",
        "    # Create window-level importance map\n",
        "    valid_patch_size = get_valid_patch_size(image_size, roi_size)\n",
        "    if valid_patch_size == roi_size and (roi_weight_map is not None):\n",
        "        importance_map_ = roi_weight_map\n",
        "    else:\n",
        "        try:\n",
        "            valid_p_size = ensure_tuple(valid_patch_size)\n",
        "            importance_map_ = compute_importance_map(\n",
        "                valid_p_size, mode=mode, sigma_scale=sigma_scale, device=sw_device, dtype=compute_dtype\n",
        "            )\n",
        "            if len(importance_map_.shape) == num_spatial_dims and not process_fn:\n",
        "                importance_map_ = importance_map_[None, None]  # adds batch, channel dimensions\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                f\"patch size {valid_p_size}, mode={mode}, sigma_scale={sigma_scale}, device={device}\\n\"\n",
        "                \"Seems to be OOM. Please try smaller patch size or mode='constant' instead of mode='gaussian'.\"\n",
        "            ) from e\n",
        "    importance_map_ = convert_data_type(importance_map_, torch.Tensor, device=sw_device, dtype=compute_dtype)[0]\n",
        "\n",
        "    # stores output and count map\n",
        "    output_image_list, count_map_list, sw_device_buffer, b_s, b_i = [], [], [], 0, 0  # type: ignore\n",
        "    # for each patch\n",
        "    for slice_g in tqdm(windows_range) if progress else windows_range:\n",
        "        slice_range = range(slice_g, min(slice_g + sw_batch_size, b_slices[b_s][0] if buffered else total_slices))\n",
        "        unravel_slice = [\n",
        "            [slice(idx // num_win, idx // num_win + 1), slice(None)] + list(slices[idx % num_win])\n",
        "            for idx in slice_range\n",
        "        ]\n",
        "        if sw_batch_size > 1:\n",
        "            win_data = torch.cat([inputs[win_slice] for win_slice in unravel_slice]).to(sw_device)\n",
        "        else:\n",
        "            win_data = inputs[unravel_slice[0]].to(sw_device)\n",
        "        if with_coord:\n",
        "            temp = {\"inputs\":win_data, \"table\":table}\n",
        "            seg_prob_out = predictor(temp, unravel_slice, *args, **kwargs)  # batched patch\n",
        "        else:\n",
        "            temp = {\"inputs\":win_data, \"table\":table}\n",
        "            seg_prob_out = predictor(temp, *args, **kwargs)  # batched patch\n",
        "\n",
        "\n",
        "        # convert seg_prob_out to tuple seg_tuple, this does not allocate new memory.\n",
        "        dict_keys, seg_tuple = _flatten_struct(seg_prob_out)\n",
        "        if process_fn:\n",
        "            seg_tuple, w_t = process_fn(seg_tuple, win_data, importance_map_)\n",
        "        else:\n",
        "            w_t = importance_map_\n",
        "        if len(w_t.shape) == num_spatial_dims:\n",
        "            w_t = w_t[None, None]\n",
        "        w_t = w_t.to(dtype=compute_dtype, device=sw_device)\n",
        "        if buffered:\n",
        "            c_start, c_end = b_slices[b_s][1:]\n",
        "            if not sw_device_buffer:\n",
        "                k = seg_tuple[0].shape[1]  # len(seg_tuple) > 1 is currently ignored\n",
        "                sp_size = list(image_size)\n",
        "                sp_size[buffer_dim] = c_end - c_start\n",
        "                sw_device_buffer = [torch.zeros(size=[1, k, *sp_size], dtype=compute_dtype, device=sw_device)]\n",
        "            for p, s in zip(seg_tuple[0], unravel_slice):\n",
        "                offset = s[buffer_dim + 2].start - c_start\n",
        "                s[buffer_dim + 2] = slice(offset, offset + roi_size[buffer_dim])\n",
        "                s[0] = slice(0, 1)\n",
        "                sw_device_buffer[0][s] += p * w_t\n",
        "            b_i += len(unravel_slice)\n",
        "            if b_i < b_slices[b_s][0]:\n",
        "                continue\n",
        "        else:\n",
        "            sw_device_buffer = list(seg_tuple)\n",
        "\n",
        "        for ss in range(len(sw_device_buffer)):\n",
        "            b_shape = sw_device_buffer[ss].shape\n",
        "            seg_chns, seg_shape = b_shape[1], b_shape[2:]\n",
        "            z_scale = None\n",
        "            if not buffered and seg_shape != roi_size:\n",
        "                z_scale = [out_w_i / float(in_w_i) for out_w_i, in_w_i in zip(seg_shape, roi_size)]\n",
        "                w_t = F.interpolate(w_t, seg_shape, mode=_nearest_mode)\n",
        "            if len(output_image_list) <= ss:\n",
        "                output_shape = [batch_size, seg_chns]\n",
        "                output_shape += [int(_i * _z) for _i, _z in zip(image_size, z_scale)] if z_scale else list(image_size)\n",
        "                # allocate memory to store the full output and the count for overlapping parts\n",
        "                new_tensor: Callable = torch.empty if non_blocking else torch.zeros  # type: ignore\n",
        "                output_image_list.append(new_tensor(output_shape, dtype=compute_dtype, device=device))\n",
        "                count_map_list.append(torch.zeros([1, 1] + output_shape[2:], dtype=compute_dtype, device=device))\n",
        "                w_t_ = w_t.to(device)\n",
        "                for __s in slices:\n",
        "                    if z_scale is not None:\n",
        "                        __s = tuple(slice(int(_si.start * z_s), int(_si.stop * z_s)) for _si, z_s in zip(__s, z_scale))\n",
        "                    count_map_list[-1][(slice(None), slice(None), *__s)] += w_t_\n",
        "            if buffered:\n",
        "                o_slice = [slice(None)] * len(inputs.shape)\n",
        "                o_slice[buffer_dim + 2] = slice(c_start, c_end)\n",
        "                img_b = b_s // n_per_batch  # image batch index\n",
        "                o_slice[0] = slice(img_b, img_b + 1)\n",
        "                if non_blocking:\n",
        "                    output_image_list[0][o_slice].copy_(sw_device_buffer[0], non_blocking=non_blocking)\n",
        "                else:\n",
        "                    output_image_list[0][o_slice] += sw_device_buffer[0].to(device=device)\n",
        "            else:\n",
        "                sw_device_buffer[ss] *= w_t\n",
        "                sw_device_buffer[ss] = sw_device_buffer[ss].to(device)\n",
        "                _compute_coords(unravel_slice, z_scale, output_image_list[ss], sw_device_buffer[ss])\n",
        "        sw_device_buffer = []\n",
        "        if buffered:\n",
        "            b_s += 1\n",
        "\n",
        "    if non_blocking:\n",
        "        torch.cuda.current_stream().synchronize()\n",
        "\n",
        "    # account for any overlapping sections\n",
        "    for ss in range(len(output_image_list)):\n",
        "        output_image_list[ss] /= count_map_list.pop(0)\n",
        "\n",
        "    # remove padding if image_size smaller than roi_size\n",
        "    if any(pad_size):\n",
        "        for ss, output_i in enumerate(output_image_list):\n",
        "            zoom_scale = [_shape_d / _roi_size_d for _shape_d, _roi_size_d in zip(output_i.shape[2:], roi_size)]\n",
        "            final_slicing: list[slice] = []\n",
        "            for sp in range(num_spatial_dims):\n",
        "                si = num_spatial_dims - sp - 1\n",
        "                slice_dim = slice(\n",
        "                    int(round(pad_size[sp * 2] * zoom_scale[si])),\n",
        "                    int(round((pad_size[sp * 2] + image_size_[si]) * zoom_scale[si])),\n",
        "                )\n",
        "                final_slicing.insert(0, slice_dim)\n",
        "            output_image_list[ss] = output_i[(slice(None), slice(None), *final_slicing)]\n",
        "\n",
        "    final_output = _pack_struct(output_image_list, dict_keys)\n",
        "    if temp_meta is not None:\n",
        "        final_output = convert_to_dst_type(final_output, temp_meta, device=device)[0]\n",
        "    else:\n",
        "        final_output = convert_to_dst_type(final_output, inputs, device=device)[0]\n",
        "\n",
        "    return final_output  # type: ignore\n",
        "\n",
        "\n",
        "def _create_buffered_slices(slices, batch_size, sw_batch_size, buffer_dim, buffer_steps):\n",
        "    \"\"\"rearrange slices for buffering\"\"\"\n",
        "    slices_np = np.asarray(slices)\n",
        "    slices_np = slices_np[np.argsort(slices_np[:, buffer_dim, 0], kind=\"mergesort\")]\n",
        "    slices = [tuple(slice(c[0], c[1]) for c in i) for i in slices_np]\n",
        "    slices_np = slices_np[:, buffer_dim]\n",
        "\n",
        "    _, _, _b_lens = np.unique(slices_np[:, 0], return_counts=True, return_index=True)\n",
        "    b_ends = np.cumsum(_b_lens).tolist()  # possible buffer flush boundaries\n",
        "    x = [0, *b_ends][:: min(len(b_ends), int(buffer_steps))]\n",
        "    if x[-1] < b_ends[-1]:\n",
        "        x.append(b_ends[-1])\n",
        "    n_per_batch = len(x) - 1\n",
        "    windows_range = [\n",
        "        range(b * x[-1] + x[i], b * x[-1] + x[i + 1], sw_batch_size)\n",
        "        for b in range(batch_size)\n",
        "        for i in range(n_per_batch)\n",
        "    ]\n",
        "    b_slices = []\n",
        "    for _s, _r in enumerate(windows_range):\n",
        "        s_s = slices_np[windows_range[_s - 1].stop % len(slices) if _s > 0 else 0, 0]\n",
        "        s_e = slices_np[(_r.stop - 1) % len(slices), 1]\n",
        "        b_slices.append((_r.stop, s_s, s_e))  # buffer index, slice start, slice end\n",
        "    windows_range = itertools.chain(*windows_range)  # type: ignore\n",
        "    return slices, n_per_batch, b_slices, windows_range\n",
        "\n",
        "\n",
        "def _compute_coords(coords, z_scale, out, patch):\n",
        "    \"\"\"sliding window batch spatial scaling indexing for multi-resolution outputs.\"\"\"\n",
        "    for original_idx, p in zip(coords, patch):\n",
        "        idx_zm = list(original_idx)  # 4D for 2D image, 5D for 3D image\n",
        "        if z_scale:\n",
        "            for axis in range(2, len(idx_zm)):\n",
        "                idx_zm[axis] = slice(\n",
        "                    int(original_idx[axis].start * z_scale[axis - 2]), int(original_idx[axis].stop * z_scale[axis - 2])\n",
        "                )\n",
        "        out[idx_zm] += p\n",
        "\n",
        "\n",
        "def _get_scan_interval(\n",
        "    image_size: Sequence[int], roi_size: Sequence[int], num_spatial_dims: int, overlap: Sequence[float]\n",
        ") -> tuple[int, ...]:\n",
        "    \"\"\"\n",
        "    Compute scan interval according to the image size, roi size and overlap.\n",
        "    Scan interval will be `int((1 - overlap) * roi_size)`, if interval is 0,\n",
        "    use 1 instead to make sure sliding window works.\n",
        "\n",
        "    \"\"\"\n",
        "    if len(image_size) != num_spatial_dims:\n",
        "        raise ValueError(f\"len(image_size) {len(image_size)} different from spatial dims {num_spatial_dims}.\")\n",
        "    if len(roi_size) != num_spatial_dims:\n",
        "        raise ValueError(f\"len(roi_size) {len(roi_size)} different from spatial dims {num_spatial_dims}.\")\n",
        "\n",
        "    scan_interval = []\n",
        "    for i, o in zip(range(num_spatial_dims), overlap):\n",
        "        if roi_size[i] == image_size[i]:\n",
        "            scan_interval.append(int(roi_size[i]))\n",
        "        else:\n",
        "            interval = int(roi_size[i] * (1 - o))\n",
        "            scan_interval.append(interval if interval > 0 else 1)\n",
        "    return tuple(scan_interval)\n",
        "\n",
        "\n",
        "def _flatten_struct(seg_out):\n",
        "    dict_keys = None\n",
        "    seg_probs: tuple[torch.Tensor, ...]\n",
        "    if isinstance(seg_out, torch.Tensor):\n",
        "        seg_probs = (seg_out,)\n",
        "    elif isinstance(seg_out, Mapping):\n",
        "        dict_keys = sorted(seg_out.keys())  # track predictor's output keys\n",
        "        seg_probs = tuple(seg_out[k] for k in dict_keys)\n",
        "    else:\n",
        "        seg_probs = ensure_tuple(seg_out)\n",
        "    return dict_keys, seg_probs\n",
        "\n",
        "\n",
        "def _pack_struct(seg_out, dict_keys=None):\n",
        "    if dict_keys is not None:\n",
        "        return dict(zip(dict_keys, seg_out))\n",
        "    if isinstance(seg_out, (list, tuple)) and len(seg_out) == 1:\n",
        "        return seg_out[0]\n",
        "    return ensure_tuple(seg_out)\n",
        "if __name__ == \"__main__\":\n",
        "    print('run utils')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "19d44e60-3f38-4ed7-a30d-2c78d5e25649",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19d44e60-3f38-4ed7-a30d-2c78d5e25649",
        "outputId": "368bcb7c-e148-4d64-e8af-4f8ed59a5368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run utils_windows\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import warnings\n",
        "from abc import ABC, abstractmethod\n",
        "from collections.abc import Callable, Iterable, Iterator, Mapping, Sequence\n",
        "from pydoc import locate\n",
        "from typing import Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from monai.apps.utils import get_logger\n",
        "from monai.data.meta_tensor import MetaTensor\n",
        "from monai.data.thread_buffer import ThreadBuffer\n",
        "from monai.inferers.merger import AvgMerger, Merger\n",
        "from monai.inferers.splitter import Splitter\n",
        "from monai.inferers.utils import compute_importance_map#, sliding_window_inference\n",
        "# from inferWindows import sliding_window_inference\n",
        "from monai.utils import BlendMode, PatchKeys, PytorchPadMode, ensure_tuple, optional_import\n",
        "from monai.visualize import CAM, GradCAM, GradCAMpp\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "__all__ = [\n",
        "    \"Inferer\",\n",
        "    \"PatchInferer\",\n",
        "    \"SimpleInferer\",\n",
        "    \"SlidingWindowInferer\",\n",
        "    \"SaliencyInferer\",\n",
        "    \"SliceInferer\",\n",
        "    \"SlidingWindowInfererAdapt\",\n",
        "]\n",
        "\n",
        "\n",
        "class Inferer(ABC):\n",
        "    \"\"\"\n",
        "    A base class for model inference.\n",
        "    Extend this class to support operations during inference, e.g. a sliding window method.\n",
        "\n",
        "    Example code::\n",
        "\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        transform = Compose([ToTensor(), LoadImage(image_only=True)])\n",
        "        data = transform(img_path).to(device)\n",
        "        model = UNet(...).to(device)\n",
        "        inferer = SlidingWindowInferer(...)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = inferer(inputs=data, network=model)\n",
        "        ...\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, inputs: torch.Tensor, network: Callable, *args: Any, **kwargs: Any) -> Any:\n",
        "        \"\"\"\n",
        "        Run inference on `inputs` with the `network` model.\n",
        "\n",
        "        Args:\n",
        "            inputs: input of the model inference.\n",
        "            network: model for inference.\n",
        "            args: optional args to be passed to ``network``.\n",
        "            kwargs: optional keyword args to be passed to ``network``.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: When the subclass does not override this method.\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(f\"Subclass {self.__class__.__name__} must implement this method.\")\n",
        "\n",
        "\n",
        "class PatchInferer(Inferer):\n",
        "    \"\"\"\n",
        "    Inference on patches instead of the whole image based on Splitter and Merger.\n",
        "    This splits the input image into patches and then merge the resulted patches.\n",
        "\n",
        "    Args:\n",
        "        splitter: a `Splitter` object that split the inputs into patches. Defaults to None.\n",
        "            If not provided or None, the inputs are considered to be already split into patches.\n",
        "            In this case, the output `merged_shape` and the optional `cropped_shape` cannot be inferred\n",
        "            and should be explicitly provided.\n",
        "        merger_cls: a `Merger` subclass that can be instantiated to merges patch outputs.\n",
        "            It can also be a string that matches the name of a class inherited from `Merger` class.\n",
        "            Defaults to `AvgMerger`.\n",
        "        batch_size: batch size for patches. If the input tensor is already batched [BxCxWxH],\n",
        "            this adds additional batching [(Bp*B)xCxWpxHp] for inference on patches.\n",
        "            Defaults to 1.\n",
        "        preprocessing: a callable that process patches before the being fed to the network.\n",
        "            Defaults to None.\n",
        "        postprocessing: a callable that process the output of the network.\n",
        "            Defaults to None.\n",
        "        output_keys: if the network output is a dictionary, this defines the keys of\n",
        "            the output dictionary to be used for merging.\n",
        "            Defaults to None, where all the keys are used.\n",
        "        match_spatial_shape: whether to crop the output to match the input shape. Defaults to True.\n",
        "        buffer_size: number of patches to be held in the buffer with a separate thread for batch sampling. Defaults to 0.\n",
        "        merger_kwargs: arguments to be passed to `merger_cls` for instantiation.\n",
        "            `merged_shape` is calculated automatically based on the input shape and\n",
        "            the output patch shape unless it is passed here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        splitter: Splitter | None = None,\n",
        "        merger_cls: type[Merger] | str = AvgMerger,\n",
        "        batch_size: int = 1,\n",
        "        preprocessing: Callable | None = None,\n",
        "        postprocessing: Callable | None = None,\n",
        "        output_keys: Sequence | None = None,\n",
        "        match_spatial_shape: bool = True,\n",
        "        buffer_size: int = 0,\n",
        "        **merger_kwargs: Any,\n",
        "    ) -> None:\n",
        "        Inferer.__init__(self)\n",
        "        # splitter\n",
        "        if not isinstance(splitter, (Splitter, type(None))):\n",
        "            if not isinstance(splitter, Splitter):\n",
        "                raise TypeError(\n",
        "                    f\"'splitter' should be a `Splitter` object that returns: \"\n",
        "                    \"an iterable of pairs of (patch, location) or a MetaTensor that has `PatchKeys.LOCATION` metadata).\"\n",
        "                    f\"{type(splitter)} is given.\"\n",
        "                )\n",
        "        self.splitter = splitter\n",
        "\n",
        "        # merger\n",
        "        if isinstance(merger_cls, str):\n",
        "            valid_merger_cls: type[Merger]\n",
        "            # search amongst implemented mergers in MONAI\n",
        "            valid_merger_cls, merger_found = optional_import(\"monai.inferers.merger\", name=merger_cls)\n",
        "            if not merger_found:\n",
        "                # try to locate the requested merger class (with dotted path)\n",
        "                valid_merger_cls = locate(merger_cls)  # type: ignore\n",
        "            if valid_merger_cls is None:\n",
        "                raise ValueError(f\"The requested `merger_cls` ['{merger_cls}'] does not exist.\")\n",
        "            merger_cls = valid_merger_cls\n",
        "        if not issubclass(merger_cls, Merger):\n",
        "            raise TypeError(f\"'merger' should be a subclass of `Merger`, {merger_cls} is given.\")\n",
        "        self.merger_cls = merger_cls\n",
        "        self.merger_kwargs = merger_kwargs\n",
        "\n",
        "        # pre-processor (process patch before the network)\n",
        "        if preprocessing is not None and not callable(preprocessing):\n",
        "            raise TypeError(f\"'preprocessing' should be a callable object, {type(preprocessing)} is given.\")\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "        # post-processor (process the output of the network)\n",
        "        if postprocessing is not None and not callable(postprocessing):\n",
        "            raise TypeError(f\"'postprocessing' should be a callable object, {type(postprocessing)} is given.\")\n",
        "        self.postprocessing = postprocessing\n",
        "\n",
        "        # batch size for patches\n",
        "        if batch_size < 1:\n",
        "            raise ValueError(f\"`batch_size` must be a positive number, {batch_size} is given.\")\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # model output keys\n",
        "        self.output_keys = output_keys\n",
        "\n",
        "        # whether to crop the output to match the input shape\n",
        "        self.match_spatial_shape = match_spatial_shape\n",
        "\n",
        "        # buffer size for multithreaded batch sampling\n",
        "        self.buffer_size = buffer_size\n",
        "\n",
        "    def _batch_sampler(\n",
        "        self, patches: Iterable[tuple[torch.Tensor, Sequence[int]]] | MetaTensor\n",
        "    ) -> Iterator[tuple[torch.Tensor, Sequence, int]]:\n",
        "        \"\"\"Generate batch of patches and locations\n",
        "\n",
        "        Args:\n",
        "            patches: a tensor or list of tensors\n",
        "\n",
        "        Yields:\n",
        "            A batch of patches (torch.Tensor or MetaTensor), a sequence of location tuples, and the batch size\n",
        "        \"\"\"\n",
        "        if isinstance(patches, MetaTensor):\n",
        "            total_size = len(patches)\n",
        "            for i in range(0, total_size, self.batch_size):\n",
        "                batch_size = min(self.batch_size, total_size - i)\n",
        "                yield patches[i : i + batch_size], patches[i : i + batch_size].meta[PatchKeys.LOCATION], batch_size  # type: ignore\n",
        "        else:\n",
        "            buffer: Iterable | ThreadBuffer\n",
        "            if self.buffer_size > 0:\n",
        "                # Use multi-threading to sample patches with a buffer\n",
        "                buffer = ThreadBuffer(patches, buffer_size=self.buffer_size, timeout=0.1)\n",
        "            else:\n",
        "                buffer = patches\n",
        "            patch_batch: list[Any] = [None] * self.batch_size\n",
        "            location_batch: list[Any] = [None] * self.batch_size\n",
        "            idx_in_batch = 0\n",
        "            for sample in buffer:\n",
        "                patch_batch[idx_in_batch] = sample[0]\n",
        "                location_batch[idx_in_batch] = sample[1]\n",
        "                idx_in_batch += 1\n",
        "                if idx_in_batch == self.batch_size:\n",
        "                    # concatenate batch of patches to create a tensor\n",
        "                    yield torch.cat(patch_batch), location_batch, idx_in_batch\n",
        "                    patch_batch = [None] * self.batch_size\n",
        "                    location_batch = [None] * self.batch_size\n",
        "                    idx_in_batch = 0\n",
        "            if idx_in_batch > 0:\n",
        "                # concatenate batch of patches to create a tensor\n",
        "                yield torch.cat(patch_batch[:idx_in_batch]), location_batch, idx_in_batch\n",
        "\n",
        "    def _ensure_tuple_outputs(self, outputs: Any) -> tuple:\n",
        "        if isinstance(outputs, dict):\n",
        "            if self.output_keys is None:\n",
        "                self.output_keys = list(outputs.keys())  # model's output keys\n",
        "            return tuple(outputs[k] for k in self.output_keys)\n",
        "        return ensure_tuple(outputs, wrap_array=True)\n",
        "\n",
        "    def _run_inference(self, network: Callable, patch: torch.Tensor, *args: Any, **kwargs: Any) -> tuple:\n",
        "        # pre-process\n",
        "        if self.preprocessing:\n",
        "            patch = self.preprocessing(patch)\n",
        "        # inference\n",
        "        outputs = network(patch, *args, **kwargs)\n",
        "        # post-process\n",
        "        if self.postprocessing:\n",
        "            outputs = self.postprocessing(outputs)\n",
        "        # ensure we have a tuple of model outputs to support multiple outputs\n",
        "        return self._ensure_tuple_outputs(outputs)\n",
        "\n",
        "    def _initialize_mergers(self, inputs, outputs, patches, batch_size):\n",
        "        in_patch = torch.chunk(patches, batch_size)[0]\n",
        "        mergers = []\n",
        "        ratios = []\n",
        "        for out_patch_batch in outputs:\n",
        "            out_patch = torch.chunk(out_patch_batch, batch_size)[0]\n",
        "            # calculate the ratio of input and output patch sizes\n",
        "            ratio = tuple(op / ip for ip, op in zip(in_patch.shape[2:], out_patch.shape[2:]))\n",
        "\n",
        "            # calculate merged_shape and cropped_shape\n",
        "            merger_kwargs = self.merger_kwargs.copy()\n",
        "            cropped_shape, merged_shape = self._get_merged_shapes(inputs, out_patch, ratio)\n",
        "            if \"merged_shape\" not in merger_kwargs:\n",
        "                merger_kwargs[\"merged_shape\"] = merged_shape\n",
        "                if merger_kwargs[\"merged_shape\"] is None:\n",
        "                    raise ValueError(\"`merged_shape` cannot be `None`.\")\n",
        "            if \"cropped_shape\" not in merger_kwargs:\n",
        "                merger_kwargs[\"cropped_shape\"] = cropped_shape\n",
        "\n",
        "            # initialize the merger\n",
        "            merger = self.merger_cls(**merger_kwargs)\n",
        "\n",
        "            # store mergers and input/output ratios\n",
        "            mergers.append(merger)\n",
        "            ratios.append(ratio)\n",
        "\n",
        "        return mergers, ratios\n",
        "\n",
        "    def _aggregate(self, outputs, locations, batch_size, mergers, ratios):\n",
        "        for output_patches, merger, ratio in zip(outputs, mergers, ratios):\n",
        "            # split batched output into individual patches and then aggregate\n",
        "            for in_loc, out_patch in zip(locations, torch.chunk(output_patches, batch_size)):\n",
        "                out_loc = [round(l * r) for l, r in zip(in_loc, ratio)]\n",
        "                merger.aggregate(out_patch, out_loc)\n",
        "\n",
        "    def _get_merged_shapes(self, inputs, out_patch, ratio):\n",
        "        \"\"\"Define the shape of merged tensors (non-padded and padded)\"\"\"\n",
        "        if self.splitter is None:\n",
        "            return None, None\n",
        "\n",
        "        # input spatial shapes\n",
        "        original_spatial_shape = self.splitter.get_input_shape(inputs)\n",
        "        padded_spatial_shape = self.splitter.get_padded_shape(inputs)\n",
        "\n",
        "        # output spatial shapes\n",
        "        output_spatial_shape = tuple(round(s * r) for s, r in zip(original_spatial_shape, ratio))\n",
        "        padded_output_spatial_shape = tuple(round(s * r) for s, r in zip(padded_spatial_shape, ratio))\n",
        "\n",
        "        # output shapes\n",
        "        cropped_shape = out_patch.shape[:2] + output_spatial_shape\n",
        "        merged_shape = out_patch.shape[:2] + padded_output_spatial_shape\n",
        "\n",
        "        if not self.match_spatial_shape:\n",
        "            cropped_shape = merged_shape\n",
        "\n",
        "        return cropped_shape, merged_shape\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: torch.Tensor,\n",
        "        network: Callable[..., torch.Tensor | Sequence[torch.Tensor] | dict[Any, torch.Tensor]],\n",
        "        *args: Any,\n",
        "        **kwargs: Any,\n",
        "    ) -> Any:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: input data for inference, a torch.Tensor, representing an image or batch of images.\n",
        "                However if the data is already split, it can be fed by providing a list of tuple (patch, location),\n",
        "                or a MetaTensor that has metadata for `PatchKeys.LOCATION`. In both cases no splitter should be provided.\n",
        "            network: target model to execute inference.\n",
        "                supports callables such as ``lambda x: my_torch_model(x, additional_config)``\n",
        "            args: optional args to be passed to ``network``.\n",
        "            kwargs: optional keyword args to be passed to ``network``.\n",
        "\n",
        "        \"\"\"\n",
        "        patches_locations: Iterable[tuple[torch.Tensor, Sequence[int]]] | MetaTensor\n",
        "        if self.splitter is None:\n",
        "            # handle situations where the splitter is not provided\n",
        "            if isinstance(inputs, torch.Tensor):\n",
        "                if isinstance(inputs, MetaTensor):\n",
        "                    if PatchKeys.LOCATION not in inputs.meta:\n",
        "                        raise ValueError(\n",
        "                            \"`PatchKey.LOCATION` does not exists in `inputs.meta`. \"\n",
        "                            \"If the inputs are already split into patches, the location of patches needs to be \"\n",
        "                            \"provided as `PatchKey.LOCATION` metadata in a MetaTensor. \"\n",
        "                            \"If the input is not already split, please provide `splitter`.\"\n",
        "                        )\n",
        "                else:\n",
        "                    raise ValueError(\n",
        "                        \"`splitter` should be set if the input is not already split into patches. \"\n",
        "                        \"For inputs that are split, the location of patches needs to be provided as \"\n",
        "                        \"(image, location) pairs, or as `PatchKey.LOCATION` metadata in a MetaTensor. \"\n",
        "                        f\"The provided inputs type is {type(inputs)}.\"\n",
        "                    )\n",
        "            patches_locations = inputs\n",
        "        else:\n",
        "            # apply splitter\n",
        "            patches_locations = self.splitter(inputs)\n",
        "\n",
        "        ratios: list[float] = []\n",
        "        mergers: list[Merger] = []\n",
        "        for patches, locations, batch_size in self._batch_sampler(patches_locations):\n",
        "            # run inference\n",
        "            outputs = self._run_inference(network, patches, *args, **kwargs)\n",
        "            # initialize the mergers\n",
        "            if not mergers:\n",
        "                mergers, ratios = self._initialize_mergers(inputs, outputs, patches, batch_size)\n",
        "            # aggregate outputs\n",
        "            self._aggregate(outputs, locations, batch_size, mergers, ratios)\n",
        "\n",
        "        # finalize the mergers and get the results\n",
        "        merged_outputs = [merger.finalize() for merger in mergers]\n",
        "\n",
        "        # return according to the model output\n",
        "        if self.output_keys:\n",
        "            return dict(zip(self.output_keys, merged_outputs))\n",
        "        if len(merged_outputs) == 1:\n",
        "            return merged_outputs[0]\n",
        "        return merged_outputs\n",
        "\n",
        "\n",
        "class SimpleInferer(Inferer):\n",
        "    \"\"\"\n",
        "    SimpleInferer is the normal inference method that run model forward() directly.\n",
        "    Usage example can be found in the :py:class:`monai.inferers.Inferer` base class.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        Inferer.__init__(self)\n",
        "\n",
        "    def __call__(\n",
        "        self, inputs: torch.Tensor, network: Callable[..., torch.Tensor], *args: Any, **kwargs: Any\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Unified callable function API of Inferers.\n",
        "\n",
        "        Args:\n",
        "            inputs: model input data for inference.\n",
        "            network: target model to execute inference.\n",
        "                supports callables such as ``lambda x: my_torch_model(x, additional_config)``\n",
        "            args: optional args to be passed to ``network``.\n",
        "            kwargs: optional keyword args to be passed to ``network``.\n",
        "\n",
        "        \"\"\"\n",
        "        return network(inputs, *args, **kwargs)\n",
        "\n",
        "\n",
        "class SlidingWindowInferer(Inferer):\n",
        "    \"\"\"\n",
        "    Sliding window method for model inference,\n",
        "    with `sw_batch_size` windows for every model.forward().\n",
        "    Usage example can be found in the :py:class:`monai.inferers.Inferer` base class.\n",
        "\n",
        "    Args:\n",
        "        roi_size: the window size to execute SlidingWindow evaluation.\n",
        "            If it has non-positive components, the corresponding `inputs` size will be used.\n",
        "            if the components of the `roi_size` are non-positive values, the transform will use the\n",
        "            corresponding components of img size. For example, `roi_size=(32, -1)` will be adapted\n",
        "            to `(32, 64)` if the second spatial dimension size of img is `64`.\n",
        "        sw_batch_size: the batch size to run window slices.\n",
        "        overlap: Amount of overlap between scans along each spatial dimension, defaults to ``0.25``.\n",
        "        mode: {``\"constant\"``, ``\"gaussian\"``}\n",
        "            How to blend output of overlapping windows. Defaults to ``\"constant\"``.\n",
        "\n",
        "            - ``\"constant``\": gives equal weight to all predictions.\n",
        "            - ``\"gaussian``\": gives less weight to predictions on edges of windows.\n",
        "\n",
        "        sigma_scale: the standard deviation coefficient of the Gaussian window when `mode` is ``\"gaussian\"``.\n",
        "            Default: 0.125. Actual window sigma is ``sigma_scale`` * ``dim_size``.\n",
        "            When sigma_scale is a sequence of floats, the values denote sigma_scale at the corresponding\n",
        "            spatial dimensions.\n",
        "        padding_mode: {``\"constant\"``, ``\"reflect\"``, ``\"replicate\"``, ``\"circular\"``}\n",
        "            Padding mode when ``roi_size`` is larger than inputs. Defaults to ``\"constant\"``\n",
        "            See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
        "        cval: fill value for 'constant' padding mode. Default: 0\n",
        "        sw_device: device for the window data.\n",
        "            By default the device (and accordingly the memory) of the `inputs` is used.\n",
        "            Normally `sw_device` should be consistent with the device where `predictor` is defined.\n",
        "        device: device for the stitched output prediction.\n",
        "            By default the device (and accordingly the memory) of the `inputs` is used. If for example\n",
        "            set to device=torch.device('cpu') the gpu memory consumption is less and independent of the\n",
        "            `inputs` and `roi_size`. Output is on the `device`.\n",
        "        progress: whether to print a tqdm progress bar.\n",
        "        cache_roi_weight_map: whether to precompute the ROI weight map.\n",
        "        cpu_thresh: when provided, dynamically switch to stitching on cpu (to save gpu memory)\n",
        "            when input image volume is larger than this threshold (in pixels/voxels).\n",
        "            Otherwise use ``\"device\"``. Thus, the output may end-up on either cpu or gpu.\n",
        "        buffer_steps: the number of sliding window iterations along the ``buffer_dim``\n",
        "            to be buffered on ``sw_device`` before writing to ``device``.\n",
        "            (Typically, ``sw_device`` is ``cuda`` and ``device`` is ``cpu``.)\n",
        "            default is None, no buffering. For the buffer dim, when spatial size is divisible by buffer_steps*roi_size,\n",
        "            (i.e. no overlapping among the buffers) non_blocking copy may be automatically enabled for efficiency.\n",
        "        buffer_dim: the spatial dimension along which the buffers are created.\n",
        "            0 indicates the first spatial dimension. Default is -1, the last spatial dimension.\n",
        "        with_coord: whether to pass the window coordinates to ``network``. Defaults to False.\n",
        "            If True, the ``network``'s 2nd input argument should accept the window coordinates.\n",
        "\n",
        "    Note:\n",
        "        ``sw_batch_size`` denotes the max number of windows per network inference iteration,\n",
        "        not the batch size of inputs.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        roi_size: Sequence[int] | int,\n",
        "        sw_batch_size: int = 1,\n",
        "        overlap: Sequence[float] | float = 0.25,\n",
        "        mode: BlendMode | str = BlendMode.CONSTANT,\n",
        "        sigma_scale: Sequence[float] | float = 0.125,\n",
        "        padding_mode: PytorchPadMode | str = PytorchPadMode.CONSTANT,\n",
        "        cval: float = 0.0,\n",
        "        sw_device: torch.device | str | None = None,\n",
        "        device: torch.device | str | None = None,\n",
        "        progress: bool = False,\n",
        "        cache_roi_weight_map: bool = False,\n",
        "        cpu_thresh: int | None = None,\n",
        "        buffer_steps: int | None = None,\n",
        "        buffer_dim: int = -1,\n",
        "        with_coord: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.roi_size = roi_size\n",
        "        self.sw_batch_size = sw_batch_size\n",
        "        self.overlap = overlap\n",
        "        self.mode: BlendMode = BlendMode(mode)\n",
        "        self.sigma_scale = sigma_scale\n",
        "        self.padding_mode = padding_mode\n",
        "        self.cval = cval\n",
        "        self.sw_device = sw_device\n",
        "        self.device = device\n",
        "        self.progress = progress\n",
        "        self.cpu_thresh = cpu_thresh\n",
        "        self.buffer_steps = buffer_steps\n",
        "        self.buffer_dim = buffer_dim\n",
        "        self.with_coord = with_coord\n",
        "\n",
        "        # compute_importance_map takes long time when computing on cpu. We thus\n",
        "        # compute it once if it's static and then save it for future usage\n",
        "        self.roi_weight_map = None\n",
        "        try:\n",
        "            if cache_roi_weight_map and isinstance(roi_size, Sequence) and min(roi_size) > 0:  # non-dynamic roi size\n",
        "                if device is None:\n",
        "                    device = \"cpu\"\n",
        "                self.roi_weight_map = compute_importance_map(\n",
        "                    ensure_tuple(self.roi_size), mode=mode, sigma_scale=sigma_scale, device=device\n",
        "                )\n",
        "            if cache_roi_weight_map and self.roi_weight_map is None:\n",
        "                warnings.warn(\"cache_roi_weight_map=True, but cache is not created. (dynamic roi_size?)\")\n",
        "        except BaseException as e:\n",
        "            raise RuntimeError(\n",
        "                f\"roi size {self.roi_size}, mode={mode}, sigma_scale={sigma_scale}, device={device}\\n\"\n",
        "                \"Seems to be OOM. Please try smaller patch size or mode='constant' instead of mode='gaussian'.\"\n",
        "            ) from e\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: torch.Tensor,\n",
        "        network: Callable[..., torch.Tensor | Sequence[torch.Tensor] | dict[Any, torch.Tensor]],\n",
        "        *args: Any,\n",
        "        **kwargs: Any,\n",
        "    ) -> torch.Tensor | tuple[torch.Tensor, ...] | dict[Any, torch.Tensor]:\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            inputs: model input data for inference.\n",
        "            network: target model to execute inference.\n",
        "                supports callables such as ``lambda x: my_torch_model(x, additional_config)``\n",
        "            args: optional args to be passed to ``network``.\n",
        "            kwargs: optional keyword args to be passed to ``network``.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        device = kwargs.pop(\"device\", self.device)\n",
        "        buffer_steps = kwargs.pop(\"buffer_steps\", self.buffer_steps)\n",
        "        buffer_dim = kwargs.pop(\"buffer_dim\", self.buffer_dim)\n",
        "\n",
        "        if device is None and self.cpu_thresh is not None and inputs.shape[2:].numel() > self.cpu_thresh:\n",
        "            device = \"cpu\"  # stitch in cpu memory if image is too large\n",
        "\n",
        "        return sliding_window_inference(\n",
        "            inputs,\n",
        "            self.roi_size,\n",
        "            self.sw_batch_size,\n",
        "            network,\n",
        "            self.overlap,\n",
        "            self.mode,\n",
        "            self.sigma_scale,\n",
        "            self.padding_mode,\n",
        "            self.cval,\n",
        "            self.sw_device,\n",
        "            device,\n",
        "            self.progress,\n",
        "            self.roi_weight_map,\n",
        "            None,\n",
        "            buffer_steps,\n",
        "            buffer_dim,\n",
        "            self.with_coord,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "\n",
        "class SlidingWindowInfererAdapt(SlidingWindowInferer):\n",
        "    \"\"\"\n",
        "    SlidingWindowInfererAdapt extends SlidingWindowInferer to automatically switch to buffered and then to CPU stitching,\n",
        "    when OOM on GPU. It also records a size of such large images to automatically\n",
        "    try CPU stitching for the next large image of a similar size.  If the stitching 'device' input parameter is provided,\n",
        "    automatic adaptation won't be attempted, please keep the default option device = None for adaptive behavior.\n",
        "    Note: the output might be on CPU (even if the input was on GPU), if the GPU memory was not sufficient.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: torch.Tensor,\n",
        "        network: Callable[..., torch.Tensor | Sequence[torch.Tensor] | dict[Any, torch.Tensor]],\n",
        "        *args: Any,\n",
        "        **kwargs: Any,\n",
        "    ) -> torch.Tensor | tuple[torch.Tensor, ...] | dict[Any, torch.Tensor]:\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            inputs: model input data for inference.\n",
        "            network: target model to execute inference.\n",
        "                supports callables such as ``lambda x: my_torch_model(x, additional_config)``\n",
        "            args: optional args to be passed to ``network``.\n",
        "            kwargs: optional keyword args to be passed to ``network``.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # if device is provided, use without any adaptations\n",
        "        if self.device is not None:\n",
        "            return super().__call__(inputs, network, *args, **kwargs)\n",
        "\n",
        "        skip_buffer = self.buffer_steps is not None and self.buffer_steps <= 0\n",
        "        cpu_cond = self.cpu_thresh is not None and inputs.shape[2:].numel() > self.cpu_thresh\n",
        "        gpu_stitching = inputs.is_cuda and not cpu_cond\n",
        "        buffered_stitching = inputs.is_cuda and cpu_cond and not skip_buffer\n",
        "        buffer_steps = max(1, self.buffer_steps) if self.buffer_steps is not None else 1\n",
        "        buffer_dim = -1\n",
        "\n",
        "        sh = list(inputs.shape[2:])\n",
        "        max_dim = sh.index(max(sh))\n",
        "        if inputs.shape[max_dim + 2] / inputs.shape[-1] >= 2:\n",
        "            buffer_dim = max_dim\n",
        "\n",
        "        for _ in range(10):  # at most 10 trials\n",
        "            try:\n",
        "                return super().__call__(\n",
        "                    inputs,\n",
        "                    network,\n",
        "                    device=inputs.device if gpu_stitching else torch.device(\"cpu\"),\n",
        "                    buffer_steps=buffer_steps if buffered_stitching else None,\n",
        "                    buffer_dim=buffer_dim,\n",
        "                    *args,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            except RuntimeError as e:\n",
        "                if not gpu_stitching and not buffered_stitching or \"OutOfMemoryError\" not in str(type(e).__name__):\n",
        "                    raise e\n",
        "\n",
        "                logger.info(e)\n",
        "\n",
        "                if gpu_stitching:  # if failed on gpu\n",
        "                    gpu_stitching = False\n",
        "                    self.cpu_thresh = inputs.shape[2:].numel() - 1  # update thresh\n",
        "\n",
        "                    if skip_buffer:\n",
        "                        buffered_stitching = False\n",
        "                        logger.warning(f\"GPU stitching failed, attempting on CPU, image dim {inputs.shape}.\")\n",
        "\n",
        "                    else:\n",
        "                        buffered_stitching = True\n",
        "                        self.buffer_steps = buffer_steps\n",
        "                        logger.warning(\n",
        "                            f\"GPU stitching failed, buffer {buffer_steps} dim {buffer_dim}, image dim {inputs.shape}.\"\n",
        "                        )\n",
        "                elif buffer_steps > 1:\n",
        "                    buffer_steps = max(1, buffer_steps // 2)\n",
        "                    self.buffer_steps = buffer_steps\n",
        "                    logger.warning(\n",
        "                        f\"GPU buffered stitching failed, image dim {inputs.shape} reducing buffer to {buffer_steps}.\"\n",
        "                    )\n",
        "                else:\n",
        "                    buffered_stitching = False\n",
        "                    logger.warning(f\"GPU buffered stitching failed, attempting on CPU, image dim {inputs.shape}.\")\n",
        "        raise RuntimeError(  # not possible to finish after the trials\n",
        "            f\"SlidingWindowInfererAdapt {skip_buffer} {cpu_cond} {gpu_stitching} {buffered_stitching} {buffer_steps}\"\n",
        "        )\n",
        "\n",
        "\n",
        "class SaliencyInferer(Inferer):\n",
        "    \"\"\"\n",
        "    SaliencyInferer is inference with activation maps.\n",
        "\n",
        "    Args:\n",
        "        cam_name: expected CAM method name, should be: \"CAM\", \"GradCAM\" or \"GradCAMpp\".\n",
        "        target_layers: name of the model layer to generate the feature map.\n",
        "        class_idx: index of the class to be visualized. if None, default to argmax(logits).\n",
        "        args: other optional args to be passed to the `__init__` of cam.\n",
        "        kwargs: other optional keyword args to be passed to `__init__` of cam.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, cam_name: str, target_layers: str, class_idx: int | None = None, *args: Any, **kwargs: Any\n",
        "    ) -> None:\n",
        "        Inferer.__init__(self)\n",
        "        if cam_name.lower() not in (\"cam\", \"gradcam\", \"gradcampp\"):\n",
        "            raise ValueError(\"cam_name should be: 'CAM', 'GradCAM' or 'GradCAMpp'.\")\n",
        "        self.cam_name = cam_name.lower()\n",
        "        self.target_layers = target_layers\n",
        "        self.class_idx = class_idx\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def __call__(self, inputs: torch.Tensor, network: nn.Module, *args: Any, **kwargs: Any):  # type: ignore\n",
        "        \"\"\"Unified callable function API of Inferers.\n",
        "\n",
        "        Args:\n",
        "            inputs: model input data for inference.\n",
        "            network: target model to execute inference.\n",
        "                supports callables such as ``lambda x: my_torch_model(x, additional_config)``\n",
        "            args: other optional args to be passed to the `__call__` of cam.\n",
        "            kwargs: other optional keyword args to be passed to `__call__` of cam.\n",
        "\n",
        "        \"\"\"\n",
        "        cam: CAM | GradCAM | GradCAMpp\n",
        "        if self.cam_name == \"cam\":\n",
        "            cam = CAM(network, self.target_layers, *self.args, **self.kwargs)\n",
        "        elif self.cam_name == \"gradcam\":\n",
        "            cam = GradCAM(network, self.target_layers, *self.args, **self.kwargs)\n",
        "        else:\n",
        "            cam = GradCAMpp(network, self.target_layers, *self.args, **self.kwargs)\n",
        "\n",
        "        return cam(inputs, self.class_idx, *args, **kwargs)\n",
        "\n",
        "\n",
        "class SliceInferer(SlidingWindowInferer):\n",
        "    \"\"\"\n",
        "    SliceInferer extends SlidingWindowInferer to provide slice-by-slice (2D) inference when provided a 3D volume.\n",
        "    A typical use case could be a 2D model (like 2D segmentation UNet) operates on the slices from a 3D volume,\n",
        "    and the output is a 3D volume with 2D slices aggregated. Example::\n",
        "\n",
        "        # sliding over the `spatial_dim`\n",
        "        inferer = SliceInferer(roi_size=(64, 256), sw_batch_size=1, spatial_dim=1)\n",
        "        output = inferer(input_volume, net)\n",
        "\n",
        "    Args:\n",
        "        spatial_dim: Spatial dimension over which the slice-by-slice inference runs on the 3D volume.\n",
        "            For example ``0`` could slide over axial slices. ``1`` over coronal slices and ``2`` over sagittal slices.\n",
        "        args: other optional args to be passed to the `__init__` of base class SlidingWindowInferer.\n",
        "        kwargs: other optional keyword args to be passed to `__init__` of base class SlidingWindowInferer.\n",
        "\n",
        "    Note:\n",
        "        ``roi_size`` in SliceInferer is expected to be a 2D tuple when a 3D volume is provided. This allows\n",
        "        sliding across slices along the 3D volume using a selected ``spatial_dim``.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spatial_dim: int = 0, *args: Any, **kwargs: Any) -> None:\n",
        "        self.spatial_dim = spatial_dim\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.orig_roi_size = ensure_tuple(self.roi_size)\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        inputs: torch.Tensor,\n",
        "        network: Callable[..., torch.Tensor | Sequence[torch.Tensor] | dict[Any, torch.Tensor]],\n",
        "        *args: Any,\n",
        "        **kwargs: Any,\n",
        "    ) -> torch.Tensor | tuple[torch.Tensor, ...] | dict[Any, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: 3D input for inference\n",
        "            network: 2D model to execute inference on slices in the 3D input\n",
        "            args: optional args to be passed to ``network``.\n",
        "            kwargs: optional keyword args to be passed to ``network``.\n",
        "        \"\"\"\n",
        "        if self.spatial_dim > 2:\n",
        "            raise ValueError(\"`spatial_dim` can only be `0, 1, 2` with `[H, W, D]` respectively.\")\n",
        "\n",
        "        # Check if ``roi_size`` tuple is 2D and ``inputs`` tensor is 3D\n",
        "        self.roi_size = ensure_tuple(self.roi_size)\n",
        "        if len(self.orig_roi_size) == 2 and len(inputs.shape[2:]) == 3:\n",
        "            self.roi_size = list(self.orig_roi_size)\n",
        "            self.roi_size.insert(self.spatial_dim, 1)\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                f\"Currently, only 2D `roi_size` ({self.orig_roi_size}) with 3D `inputs` tensor (shape={inputs.shape}) is supported.\"\n",
        "            )\n",
        "\n",
        "        return super().__call__(inputs=inputs, network=lambda x: self.network_wrapper(network, x, *args, **kwargs))\n",
        "\n",
        "    def network_wrapper(\n",
        "        self,\n",
        "        network: Callable[..., torch.Tensor | Sequence[torch.Tensor] | dict[Any, torch.Tensor]],\n",
        "        x: torch.Tensor,\n",
        "        *args: Any,\n",
        "        **kwargs: Any,\n",
        "    ) -> torch.Tensor | tuple[torch.Tensor, ...] | dict[Any, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Wrapper handles inference for 2D models over 3D volume inputs.\n",
        "        \"\"\"\n",
        "        #  Pass 4D input [N, C, H, W]/[N, C, D, W]/[N, C, D, H] to the model as it is 2D.\n",
        "        x = x.squeeze(dim=self.spatial_dim + 2)\n",
        "        out = network(x, *args, **kwargs)\n",
        "\n",
        "        #  Unsqueeze the network output so it is [N, C, D, H, W] as expected by\n",
        "        # the default SlidingWindowInferer class\n",
        "        if isinstance(out, torch.Tensor):\n",
        "            return out.unsqueeze(dim=self.spatial_dim + 2)\n",
        "\n",
        "        if isinstance(out, Mapping):\n",
        "            for k in out.keys():\n",
        "                out[k] = out[k].unsqueeze(dim=self.spatial_dim + 2)\n",
        "            return out\n",
        "\n",
        "        return tuple(out_i.unsqueeze(dim=self.spatial_dim + 2) for out_i in out)\n",
        "if __name__ == \"__main__\":\n",
        "    print('run utils_windows')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lkUIxA2EtzSc",
      "metadata": {
        "id": "lkUIxA2EtzSc"
      },
      "source": [
        "\n",
        "The following code defines the process for acquiring and loading training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b028017e-7c44-4f46-a5dd-5a971558b92b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b028017e-7c44-4f46-a5dd-5a971558b92b",
        "outputId": "34b0d015-ad40-468a-bd08-a8c0ddb5d4f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_data() run\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import torch.distributed as dist\n",
        "from monai.data import Dataset, CacheDataset, DataLoader, load_decathlon_datalist, load_decathlon_properties, partition_dataset\n",
        "\n",
        "# from task_params import task_name\n",
        "# from transforms import get_task_transforms\n",
        "import pandas as pd\n",
        "\n",
        "def get_data(root_dir='/root', batch_size=1, mode=\"train\"):\n",
        "    # get necessary parameters:\n",
        "    fold = 0\n",
        "    task_id = '08'\n",
        "    root_dir = root_dir\n",
        "    datalist_path = root_dir\n",
        "    dataset_path = os.path.join(root_dir, task_name[task_id])\n",
        "    transform_params = (0.7, 0.3, 1)\n",
        "    multi_gpu_flag = False\n",
        "\n",
        "    transform = get_task_transforms(mode, task_id, *transform_params)\n",
        "    if mode == \"test\":\n",
        "        list_key = \"test\"\n",
        "    else:\n",
        "        list_key = \"{}_fold{}\".format(mode, fold)\n",
        "    datalist_name = \"dataset_task{}.json\".format(task_id)\n",
        "\n",
        "    property_keys = [\n",
        "        \"name\",\n",
        "        \"description\",\n",
        "        \"reference\",\n",
        "        \"licence\",\n",
        "        \"tensorImageSize\",\n",
        "        \"modality\",\n",
        "        \"labels\",\n",
        "        \"numTraining\",\n",
        "        \"numTest\",\n",
        "    ]\n",
        "\n",
        "    datalist = load_decathlon_datalist(os.path.join(datalist_path, datalist_name), True, list_key, '') # datalist_path\n",
        "    support_dataset = pd.read_csv(os.path.join(root_dir, \"support_data.csv\"))\n",
        "    for item in datalist:\n",
        "        # 解析ID\n",
        "        hcc_id = item['image'].split('/')[-1].replace('_0000.nrrd', '')\n",
        "        # 在DataFrame中查找匹配的行\n",
        "        tag_value = int(hcc_id.split('_')[1])\n",
        "\n",
        "        if tag_value > 200:\n",
        "            hcc_id = 'HCC_' + str(tag_value - 500).zfill(3)\n",
        "\n",
        "        matched_row = support_dataset.loc[support_dataset['TCIA_ID'] == hcc_id].iloc[0].tolist()\n",
        "        item['table'] = matched_row[1:]\n",
        "    properties = []#load_decathlon_properties(os.path.join(datalist_path, datalist_name), property_keys)\n",
        "    if mode in [\"validation\", \"test\"]:\n",
        "        if multi_gpu_flag:\n",
        "            datalist = partition_dataset(\n",
        "                data=datalist,\n",
        "                shuffle=False,\n",
        "                num_partitions=dist.get_world_size(),\n",
        "                even_divisible=False,\n",
        "            )[dist.get_rank()]\n",
        "\n",
        "        # val_ds = CacheDataset(\n",
        "        #     data=datalist,\n",
        "        #     transform=transform,\n",
        "        #     num_workers=4,\n",
        "        # )\n",
        "        val_ds = Dataset(\n",
        "            data=datalist,\n",
        "            transform=transform,\n",
        "\n",
        "        )\n",
        "\n",
        "        data_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=1,\n",
        "        )\n",
        "    elif mode == \"train\":\n",
        "        if multi_gpu_flag:\n",
        "            datalist = partition_dataset(\n",
        "                data=datalist,\n",
        "                shuffle=True,\n",
        "                num_partitions=dist.get_world_size(),\n",
        "                even_divisible=True,\n",
        "            )[dist.get_rank()]\n",
        "\n",
        "        # train_ds = CacheDataset(\n",
        "        #     data=datalist,\n",
        "        #     transform=transform,\n",
        "        #     num_workers=8,\n",
        "        #     cache_rate=1.0,\n",
        "        # )\n",
        "        train_ds = Dataset(\n",
        "            data=datalist,\n",
        "            transform=transform\n",
        "        )\n",
        "        data_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=1,\n",
        "            drop_last=True,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"mode should be train, validation or test.\")\n",
        "\n",
        "    return properties, data_loader\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # get_data()\n",
        "    print('get_data() run')\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vp15VGbQt7AN",
      "metadata": {
        "id": "Vp15VGbQt7AN"
      },
      "source": [
        "The following module is from the MONAI framework, used to assist us in building the model evaluation module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "aa1348fb-d550-422f-bfa1-09ef9ebd1894",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa1348fb-d550-422f-bfa1-09ef9ebd1894",
        "outputId": "24707be3-b058-42a7-b895-c9d07d4606a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run evaluator\n"
          ]
        }
      ],
      "source": [
        "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from ignite.engine import Engine\n",
        "from ignite.metrics import Metric\n",
        "from monai.data import decollate_batch\n",
        "from monai.engines import SupervisedEvaluator\n",
        "from monai.engines.utils import CommonKeys as Keys\n",
        "from monai.engines.utils import IterationEvents, default_prepare_batch\n",
        "from monai.inferers import Inferer\n",
        "from monai.networks.utils import eval_mode\n",
        "from monai.transforms import AsDiscrete, Transform\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from transforms import recovery_prediction\n",
        "\n",
        "\n",
        "class DynUNetEvaluator(SupervisedEvaluator):\n",
        "    \"\"\"\n",
        "    This class inherits from SupervisedEvaluator in MONAI, and is used with DynUNet\n",
        "    on Decathlon datasets.\n",
        "\n",
        "    Args:\n",
        "        device: an object representing the device on which to run.\n",
        "        val_data_loader: Ignite engine use data_loader to run, must be\n",
        "            torch.DataLoader.\n",
        "        network: use the network to run model forward.\n",
        "        num_classes: the number of classes (output channels) for the task.\n",
        "        epoch_length: number of iterations for one epoch, default to\n",
        "            `len(val_data_loader)`.\n",
        "        non_blocking: if True and this copy is between CPU and GPU, the copy may occur asynchronously\n",
        "            with respect to the host. For other cases, this argument has no effect.\n",
        "        prepare_batch: function to parse image and label for current iteration.\n",
        "        iteration_update: the callable function for every iteration, expect to accept `engine`\n",
        "            and `batchdata` as input parameters. if not provided, use `self._iteration()` instead.\n",
        "        inferer: inference method that execute model forward on input data, like: SlidingWindow, etc.\n",
        "        postprocessing: execute additional transformation for the model output data.\n",
        "            Typically, several Tensor based transforms composed by `Compose`.\n",
        "        key_val_metric: compute metric when every iteration completed, and save average value to\n",
        "            engine.state.metrics when epoch completed. key_val_metric is the main metric to compare and save the\n",
        "            checkpoint into files.\n",
        "        additional_metrics: more Ignite metrics that also attach to Ignite Engine.\n",
        "        val_handlers: every handler is a set of Ignite Event-Handlers, must have `attach` function, like:\n",
        "            CheckpointHandler, StatsHandler, etc.\n",
        "        amp: whether to enable auto-mixed-precision evaluation, default is False.\n",
        "        tta_val: whether to do the 8 flips (8 = 2 ** 3, where 3 represents the three dimensions)\n",
        "            test time augmentation, default is False.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        device: torch.device,\n",
        "        val_data_loader: DataLoader,\n",
        "        network: torch.nn.Module,\n",
        "        num_classes: Union[str, int],\n",
        "        epoch_length: Optional[int] = None,\n",
        "        non_blocking: bool = False,\n",
        "        prepare_batch: Callable = default_prepare_batch,\n",
        "        iteration_update: Optional[Callable] = None,\n",
        "        inferer: Optional[Inferer] = None,\n",
        "        postprocessing: Optional[Transform] = None,\n",
        "        key_val_metric: Optional[Dict[str, Metric]] = None,\n",
        "        additional_metrics: Optional[Dict[str, Metric]] = None,\n",
        "        val_handlers: Optional[Sequence] = None,\n",
        "        amp: bool = False,\n",
        "        tta_val: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(\n",
        "            device=device,\n",
        "            val_data_loader=val_data_loader,\n",
        "            network=network,\n",
        "            epoch_length=epoch_length,\n",
        "            non_blocking=non_blocking,\n",
        "            prepare_batch=prepare_batch,\n",
        "            iteration_update=iteration_update,\n",
        "            inferer=inferer,\n",
        "            postprocessing=postprocessing,\n",
        "            key_val_metric=key_val_metric,\n",
        "            additional_metrics=additional_metrics,\n",
        "            val_handlers=val_handlers,\n",
        "            amp=amp,\n",
        "        )\n",
        "\n",
        "        if not isinstance(num_classes, int):\n",
        "            num_classes = int(num_classes)\n",
        "        self.num_classes = num_classes\n",
        "        self.post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
        "        self.post_label = AsDiscrete(to_onehot=num_classes)\n",
        "        self.tta_val = tta_val\n",
        "\n",
        "    def _iteration(self, engine: Engine, batchdata: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n",
        "        Return below items in a dictionary:\n",
        "            - IMAGE: image Tensor data for model input, already moved to device.\n",
        "            - LABEL: label Tensor data corresponding to the image, already moved to device.\n",
        "            - PRED: prediction result of model.\n",
        "\n",
        "        Args:\n",
        "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
        "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: When ``batchdata`` is None.\n",
        "\n",
        "        \"\"\"\n",
        "        if batchdata is None:\n",
        "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
        "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
        "        table = batchdata[\"table\"]\n",
        "        if len(batch) == 2:\n",
        "            inputs, targets = batch\n",
        "            args: Tuple = ()\n",
        "            kwargs: Dict = {}\n",
        "            temp = {\"inputs\": inputs, \"table\": table}\n",
        "        else:\n",
        "            inputs, targets, args, kwargs = batch\n",
        "            temp = {\"inputs\": inputs, \"table\": table}\n",
        "\n",
        "        targets = targets.cpu()\n",
        "\n",
        "        def _compute_pred():\n",
        "            ct = 1.0\n",
        "            pred = self.inferer(temp, self.network, *args, **kwargs).cpu()\n",
        "            pred = nn.functional.softmax(pred, dim=1)\n",
        "            if not self.tta_val:\n",
        "                return pred\n",
        "            else:\n",
        "                for dims in [[2], [3], [4], (2, 3), (2, 4), (3, 4), (2, 3, 4)]:\n",
        "                    flip_inputs = torch.flip(inputs, dims=dims)\n",
        "                    flip_pred = torch.flip(self.inferer(flip_inputs, self.network).cpu(), dims=dims)\n",
        "                    flip_pred = nn.functional.softmax(flip_pred, dim=1)\n",
        "                    del flip_inputs\n",
        "                    pred += flip_pred\n",
        "                    del flip_pred\n",
        "                    ct += 1\n",
        "                return pred / ct\n",
        "\n",
        "        # execute forward computation\n",
        "        with eval_mode(self.network):\n",
        "            if self.amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    predictions = _compute_pred()\n",
        "            else:\n",
        "                predictions = _compute_pred()\n",
        "\n",
        "        inputs = inputs.cpu()\n",
        "\n",
        "        predictions = self.post_pred(decollate_batch(predictions)[0])\n",
        "        targets = self.post_label(decollate_batch(targets)[0])\n",
        "\n",
        "        resample_flag = batchdata[\"resample_flag\"]\n",
        "        anisotrophy_flag = batchdata[\"anisotrophy_flag\"]\n",
        "        crop_shape = batchdata[\"crop_shape\"][0].tolist()\n",
        "        original_shape = batchdata[\"original_shape\"][0].tolist()\n",
        "        if resample_flag:\n",
        "            # convert the prediction back to the original (after cropped) shape\n",
        "            predictions = recovery_prediction(predictions.numpy(), [self.num_classes, *crop_shape], anisotrophy_flag)\n",
        "            predictions = torch.tensor(predictions)\n",
        "\n",
        "        # put iteration outputs into engine.state\n",
        "        engine.state.output = {Keys.IMAGE: inputs, Keys.LABEL: targets.unsqueeze(0)}\n",
        "        engine.state.output[Keys.PRED] = torch.zeros([1, self.num_classes, *original_shape])\n",
        "        # pad the prediction back to the original shape\n",
        "        box_start, box_end = batchdata[\"bbox\"][0]\n",
        "        h_start, w_start, d_start = box_start\n",
        "        h_end, w_end, d_end = box_end\n",
        "\n",
        "        engine.state.output[Keys.PRED][0, :, h_start:h_end, w_start:w_end, d_start:d_end] = predictions\n",
        "        del predictions\n",
        "\n",
        "        engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
        "        engine.fire_event(IterationEvents.MODEL_COMPLETED)\n",
        "\n",
        "        return engine.state.output\n",
        "if __name__ == \"__main__\":\n",
        "    print('run evaluator')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3AaJNeYpuQC-",
      "metadata": {
        "id": "3AaJNeYpuQC-"
      },
      "source": [
        "\n",
        "The following code includes our custom-designed model, along with the Swinunetr and MedNeXt models enhanced with a feature fusion module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "35718b0a-c5d2-4eec-b5ac-5060210f2b82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35718b0a-c5d2-4eec-b5ac-5060210f2b82",
        "outputId": "2039086f-ba91-4e11-8e0a-251badee70c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run network\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "from typing import Union, Sequence\n",
        "import itertools\n",
        "from collections.abc import Sequence\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch.nn import LayerNorm\n",
        "from typing_extensions import Final\n",
        "\n",
        "from monai.networks.blocks import MLPBlock as Mlp\n",
        "from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n",
        "from monai.networks.layers import DropPath, trunc_normal_\n",
        "from monai.utils import ensure_tuple_rep, look_up_option, optional_import\n",
        "from monai.utils.deprecate_utils import deprecated_arg\n",
        "from einops import rearrange\n",
        "\n",
        "class MedNeXtBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                in_channels:int,\n",
        "                out_channels:int,\n",
        "                exp_r:int=4,\n",
        "                kernel_size:int=7,\n",
        "                do_res:int=True,\n",
        "                norm_type:str = 'group',\n",
        "                n_groups:int or None = None,\n",
        "                dim = '3d',\n",
        "                grn = False\n",
        "                ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.do_res = do_res\n",
        "\n",
        "        assert dim in ['2d', '3d']\n",
        "        self.dim = dim\n",
        "        if self.dim == '2d':\n",
        "            conv = nn.Conv2d\n",
        "        elif self.dim == '3d':\n",
        "            conv = nn.Conv3d\n",
        "\n",
        "        # First convolution layer with DepthWise Convolutions\n",
        "        self.conv1 = conv(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = in_channels,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = 1,\n",
        "            padding = kernel_size//2,\n",
        "            groups = in_channels if n_groups is None else n_groups,\n",
        "        )\n",
        "\n",
        "        # Normalization Layer. GroupNorm is used by default.\n",
        "        if norm_type=='group':\n",
        "            self.norm = nn.GroupNorm(\n",
        "                num_groups=in_channels,\n",
        "                num_channels=in_channels\n",
        "                )\n",
        "        elif norm_type=='layer':\n",
        "            self.norm = LayerNorm(\n",
        "                normalized_shape=in_channels,\n",
        "                data_format='channels_first'\n",
        "                )\n",
        "\n",
        "        # Second convolution (Expansion) layer with Conv3D 1x1x1\n",
        "        self.conv2 = conv(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = exp_r*in_channels,\n",
        "            kernel_size = 1,\n",
        "            stride = 1,\n",
        "            padding = 0\n",
        "        )\n",
        "\n",
        "        # GeLU activations\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        # Third convolution (Compression) layer with Conv3D 1x1x1\n",
        "        self.conv3 = conv(\n",
        "            in_channels = exp_r*in_channels,\n",
        "            out_channels = out_channels,\n",
        "            kernel_size = 1,\n",
        "            stride = 1,\n",
        "            padding = 0\n",
        "        )\n",
        "\n",
        "        self.grn = grn\n",
        "        if grn:\n",
        "            if dim == '3d':\n",
        "                self.grn_beta = nn.Parameter(torch.zeros(1,exp_r*in_channels,1,1,1), requires_grad=True)\n",
        "                self.grn_gamma = nn.Parameter(torch.zeros(1,exp_r*in_channels,1,1,1), requires_grad=True)\n",
        "            elif dim == '2d':\n",
        "                self.grn_beta = nn.Parameter(torch.zeros(1,exp_r*in_channels,1,1), requires_grad=True)\n",
        "                self.grn_gamma = nn.Parameter(torch.zeros(1,exp_r*in_channels,1,1), requires_grad=True)\n",
        "\n",
        "\n",
        "    def forward(self, x, dummy_tensor=None):\n",
        "        #table = x[\"table\"]\n",
        "        #x = x[\"inputs\"]\n",
        "        #print(table)\n",
        "        x1 = x\n",
        "        x1 = self.conv1(x1)\n",
        "        x1 = self.act(self.conv2(self.norm(x1)))\n",
        "        if self.grn:\n",
        "            # gamma, beta: learnable affine transform parameters\n",
        "            # X: input of shape (N,C,H,W,D)\n",
        "            if self.dim == '3d':\n",
        "                gx = torch.norm(x1, p=2, dim=(-3, -2, -1), keepdim=True)\n",
        "            elif self.dim == '2d':\n",
        "                gx = torch.norm(x1, p=2, dim=(-2, -1), keepdim=True)\n",
        "            nx = gx / (gx.mean(dim=1, keepdim=True)+1e-6)\n",
        "            x1 = self.grn_gamma * (x1 * nx) + self.grn_beta + x1\n",
        "        x1 = self.conv3(x1)\n",
        "        if self.do_res:\n",
        "            x1 = x + x1\n",
        "        return x1\n",
        "\n",
        "\n",
        "class MedNeXtDownBlock(MedNeXtBlock):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, exp_r=4, kernel_size=7,\n",
        "                do_res=False, norm_type = 'group', dim='3d', grn=False):\n",
        "\n",
        "        super().__init__(in_channels, out_channels, exp_r, kernel_size,\n",
        "                        do_res = False, norm_type = norm_type, dim=dim,\n",
        "                        grn=grn)\n",
        "\n",
        "        if dim == '2d':\n",
        "            conv = nn.Conv2d\n",
        "        elif dim == '3d':\n",
        "            conv = nn.Conv3d\n",
        "        self.resample_do_res = do_res\n",
        "        if do_res:\n",
        "            self.res_conv = conv(\n",
        "                in_channels = in_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size = 1,\n",
        "                stride = 2\n",
        "            )\n",
        "\n",
        "        self.conv1 = conv(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = in_channels,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = 2,\n",
        "            padding = kernel_size//2,\n",
        "            groups = in_channels,\n",
        "        )\n",
        "\n",
        "    def forward(self, x, dummy_tensor=None):\n",
        "\n",
        "        x1 = super().forward(x)\n",
        "\n",
        "        if self.resample_do_res:\n",
        "            res = self.res_conv(x)\n",
        "            x1 = x1 + res\n",
        "\n",
        "        return x1\n",
        "\n",
        "\n",
        "class MedNeXtUpBlock(MedNeXtBlock):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, exp_r=4, kernel_size=7,\n",
        "                do_res=False, norm_type = 'group', dim='3d', grn = False):\n",
        "        super().__init__(in_channels, out_channels, exp_r, kernel_size,\n",
        "                         do_res=False, norm_type = norm_type, dim=dim,\n",
        "                         grn=grn)\n",
        "\n",
        "        self.resample_do_res = do_res\n",
        "\n",
        "        self.dim = dim\n",
        "        if dim == '2d':\n",
        "            conv = nn.ConvTranspose2d\n",
        "        elif dim == '3d':\n",
        "            conv = nn.ConvTranspose3d\n",
        "        if do_res:\n",
        "            self.res_conv = conv(\n",
        "                in_channels = in_channels,\n",
        "                out_channels = out_channels,\n",
        "                kernel_size = 1,\n",
        "                stride = 2\n",
        "                )\n",
        "\n",
        "        self.conv1 = conv(\n",
        "            in_channels = in_channels,\n",
        "            out_channels = in_channels,\n",
        "            kernel_size = kernel_size,\n",
        "            stride = 2,\n",
        "            padding = kernel_size//2,\n",
        "            groups = in_channels,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, dummy_tensor=None):\n",
        "\n",
        "        x1 = super().forward(x)\n",
        "        # Asymmetry but necessary to match shape\n",
        "\n",
        "        if self.dim == '2d':\n",
        "            x1 = torch.nn.functional.pad(x1, (1,0,1,0))\n",
        "        elif self.dim == '3d':\n",
        "            x1 = torch.nn.functional.pad(x1, (1,0,1,0,1,0))\n",
        "\n",
        "        if self.resample_do_res:\n",
        "            res = self.res_conv(x)\n",
        "            if self.dim == '2d':\n",
        "                res = torch.nn.functional.pad(res, (1,0,1,0))\n",
        "            elif self.dim == '3d':\n",
        "                res = torch.nn.functional.pad(res, (1,0,1,0,1,0))\n",
        "            x1 = x1 + res\n",
        "\n",
        "        return x1\n",
        "\n",
        "\n",
        "class OutBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, n_classes, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        if dim == '2d':\n",
        "            conv = nn.ConvTranspose2d\n",
        "        elif dim == '3d':\n",
        "            conv = nn.ConvTranspose3d\n",
        "        self.conv_out = conv(in_channels, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, dummy_tensor=None):\n",
        "        return self.conv_out(x)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
        "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
        "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
        "    with shape (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "    def __init__(self, normalized_shape, eps=1e-5, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))        # beta\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))         # gamma\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError\n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "\n",
        "    def forward(self, x, dummy_tensor=False):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
        "            return x\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, num_channels, aux_vector_size):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.query_proj = nn.Linear(aux_vector_size, num_channels).to(\"cuda:0\")\n",
        "        self.key_proj = nn.Linear(num_channels, num_channels).to(\"cuda:0\")\n",
        "        self.value_proj = nn.Linear(num_channels, num_channels).to(\"cuda:0\")\n",
        "\n",
        "    def forward(self, x, aux_vector):\n",
        "        # 假设x的形状是 [batch, channel, depth, height, width]\n",
        "        # 辅助向量aux_vector的形状是 [batch, features]\n",
        "\n",
        "        batch_x, C, D, H, W = x.shape\n",
        "        # print(x.shape)\n",
        "        batch_v, _ = aux_vector.shape\n",
        "        # 将x重塑为 [batch, channel, D*H*W] 以方便后续处理\n",
        "        x_reshaped = x.view(batch_x, C, -1).transpose(1, 2).contiguous()\n",
        "\n",
        "        # 生成查询（Q）、键（K）和值（V）\n",
        "        Q = self.query_proj(aux_vector).unsqueeze(-1)  # [batch, channel, 1]\n",
        "        # print(Q.shape)\n",
        "        Q = Q.repeat(int(batch_x/batch_v), 1, 1)\n",
        "        # print(Q.shape)\n",
        "        K = self.key_proj(x_reshaped)  # [batch, channel, D*H*W]\n",
        "        V = self.value_proj(x_reshaped)  # [batch, channel, D*H*W]\n",
        "\n",
        "        # 计算注意力权重\n",
        "        attention_scores = torch.matmul(Q.transpose(1, 2), K.transpose(1, 2))  # [batch, 1, D*H*W]\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)  # 归一化\n",
        "\n",
        "        # 加权和聚合\n",
        "        weighted_features = torch.matmul(attention_weights, V)  # [batch, channel, 1]\n",
        "        weighted_features = weighted_features.view(batch_x, C, 1, 1, 1)  # 重塑回原始维度\n",
        "\n",
        "        # 整合加权特征到x中，这里简单地相加\n",
        "        x_integrated = x + weighted_features\n",
        "\n",
        "        return x_integrated\n",
        "\n",
        "class Catmixture(nn.Module):\n",
        "    def __init__(self, trans_dim):\n",
        "        super(Catmixture, self).__init__()\n",
        "        self.fc = nn.Linear(59, trans_dim).to(\"cuda:0\")\n",
        "\n",
        "    def forward(self, x, vector):\n",
        "        vector = self.fc(vector)\n",
        "        # Reshape and repeat vector to match the target shape and concatenate it with x\n",
        "        vector = vector.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "        batch_x, _, D, H, W = x.shape\n",
        "        batch_v = vector.size(0)\n",
        "        expanded_vector = vector.repeat(int(batch_x/batch_v), 1, D, H, W)\n",
        "\n",
        "        # Concatenate along the channel dimension\n",
        "        x_concatenated = torch.cat([x, expanded_vector], dim=1)\n",
        "\n",
        "        return x_concatenated\n",
        "\n",
        "class Addmixture(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super(Addmixture, self).__init__()\n",
        "        self.fc = nn.Linear(59, num_channels).to(\"cuda:0\")\n",
        "\n",
        "    def forward(self, x, vector):\n",
        "        batch_x, C, D, H, W = x.shape\n",
        "        batch_v, _ = vector.shape\n",
        "        expanded_vector = self.fc(vector)\n",
        "        expanded_vector = expanded_vector.view(batch_v, C, 1, 1, 1).repeat(int(batch_x / batch_v), 1, D, H, W)\n",
        "\n",
        "        result = x + expanded_vector\n",
        "\n",
        "        return result\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"window partition operation based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "\n",
        "     Args:\n",
        "        x: input tensor.\n",
        "        window_size: local window size.\n",
        "    \"\"\"\n",
        "    x_shape = x.size()\n",
        "    if len(x_shape) == 5:\n",
        "        b, d, h, w, c = x_shape\n",
        "        x = x.view(\n",
        "            b,\n",
        "            d // window_size[0],\n",
        "            window_size[0],\n",
        "            h // window_size[1],\n",
        "            window_size[1],\n",
        "            w // window_size[2],\n",
        "            window_size[2],\n",
        "            c,\n",
        "        )\n",
        "        windows = (\n",
        "            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n",
        "        )\n",
        "    elif len(x_shape) == 4:\n",
        "        b, h, w, c = x.shape\n",
        "        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n",
        "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, dims):\n",
        "    \"\"\"window reverse operation based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "\n",
        "     Args:\n",
        "        windows: windows tensor.\n",
        "        window_size: local window size.\n",
        "        dims: dimension values.\n",
        "    \"\"\"\n",
        "    if len(dims) == 4:\n",
        "        b, d, h, w = dims\n",
        "        x = windows.view(\n",
        "            b,\n",
        "            d // window_size[0],\n",
        "            h // window_size[1],\n",
        "            w // window_size[2],\n",
        "            window_size[0],\n",
        "            window_size[1],\n",
        "            window_size[2],\n",
        "            -1,\n",
        "        )\n",
        "        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n",
        "\n",
        "    elif len(dims) == 3:\n",
        "        b, h, w = dims\n",
        "        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n",
        "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_window_size(x_size, window_size, shift_size=None):\n",
        "    \"\"\"Computing window size based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "\n",
        "     Args:\n",
        "        x_size: input size.\n",
        "        window_size: local window size.\n",
        "        shift_size: window shifting size.\n",
        "    \"\"\"\n",
        "\n",
        "    use_window_size = list(window_size)\n",
        "    if shift_size is not None:\n",
        "        use_shift_size = list(shift_size)\n",
        "    for i in range(len(x_size)):\n",
        "        if x_size[i] <= window_size[i]:\n",
        "            use_window_size[i] = x_size[i]\n",
        "            if shift_size is not None:\n",
        "                use_shift_size[i] = 0\n",
        "\n",
        "    if shift_size is None:\n",
        "        return tuple(use_window_size)\n",
        "    else:\n",
        "        return tuple(use_window_size), tuple(use_shift_size)\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        window_size: Sequence[int],\n",
        "        qkv_bias: bool = False,\n",
        "        attn_drop: float = 0.0,\n",
        "        proj_drop: float = 0.0,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: number of feature channels.\n",
        "            num_heads: number of attention heads.\n",
        "            window_size: local window size.\n",
        "            qkv_bias: add a learnable bias to query, key, value.\n",
        "            attn_drop: attention dropout rate.\n",
        "            proj_drop: dropout rate of output.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim**-0.5\n",
        "        mesh_args = torch.meshgrid.__kwdefaults__\n",
        "\n",
        "        if len(self.window_size) == 3:\n",
        "            self.relative_position_bias_table = nn.Parameter(\n",
        "                torch.zeros(\n",
        "                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n",
        "                    num_heads,\n",
        "                )\n",
        "            )\n",
        "            coords_d = torch.arange(self.window_size[0])\n",
        "            coords_h = torch.arange(self.window_size[1])\n",
        "            coords_w = torch.arange(self.window_size[2])\n",
        "            if mesh_args is not None:\n",
        "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n",
        "            else:\n",
        "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n",
        "            coords_flatten = torch.flatten(coords, 1)\n",
        "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
        "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "            relative_coords[:, :, 2] += self.window_size[2] - 1\n",
        "            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
        "            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
        "        elif len(self.window_size) == 2:\n",
        "            self.relative_position_bias_table = nn.Parameter(\n",
        "                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
        "            )\n",
        "            coords_h = torch.arange(self.window_size[0])\n",
        "            coords_w = torch.arange(self.window_size[1])\n",
        "            if mesh_args is not None:\n",
        "                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n",
        "            else:\n",
        "                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n",
        "            coords_flatten = torch.flatten(coords, 1)\n",
        "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
        "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        b, n, c = x.shape\n",
        "        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        q = q * self.scale\n",
        "        attn = q @ k.transpose(-2, -1)\n",
        "        relative_position_bias = self.relative_position_bias_table[\n",
        "            self.relative_position_index.clone()[:n, :n].reshape(-1)\n",
        "        ].reshape(n, n, -1)\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "        if mask is not None:\n",
        "            nw = mask.shape[0]\n",
        "            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, n, n)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn).to(v.dtype)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer block based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        window_size: Sequence[int],\n",
        "        shift_size: Sequence[int],\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = True,\n",
        "        drop: float = 0.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        drop_path: float = 0.0,\n",
        "        act_layer: str = \"GELU\",\n",
        "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
        "        use_checkpoint: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: number of feature channels.\n",
        "            num_heads: number of attention heads.\n",
        "            window_size: local window size.\n",
        "            shift_size: window shift size.\n",
        "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
        "            qkv_bias: add a learnable bias to query, key, value.\n",
        "            drop: dropout rate.\n",
        "            attn_drop: attention dropout rate.\n",
        "            drop_path: stochastic depth rate.\n",
        "            act_layer: activation layer.\n",
        "            norm_layer: normalization layer.\n",
        "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(\n",
        "            dim,\n",
        "            window_size=self.window_size,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "        )\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n",
        "\n",
        "    def forward_part1(self, x, mask_matrix):\n",
        "        x_shape = x.size()\n",
        "        x = self.norm1(x)\n",
        "        if len(x_shape) == 5:\n",
        "            b, d, h, w, c = x.shape\n",
        "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
        "            pad_l = pad_t = pad_d0 = 0\n",
        "            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n",
        "            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n",
        "            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n",
        "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n",
        "            _, dp, hp, wp, _ = x.shape\n",
        "            dims = [b, dp, hp, wp]\n",
        "\n",
        "        elif len(x_shape) == 4:\n",
        "            b, h, w, c = x.shape\n",
        "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
        "            pad_l = pad_t = 0\n",
        "            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n",
        "            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n",
        "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
        "            _, hp, wp, _ = x.shape\n",
        "            dims = [b, hp, wp]\n",
        "\n",
        "        if any(i > 0 for i in shift_size):\n",
        "            if len(x_shape) == 5:\n",
        "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
        "            elif len(x_shape) == 4:\n",
        "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
        "            attn_mask = mask_matrix\n",
        "        else:\n",
        "            shifted_x = x\n",
        "            attn_mask = None\n",
        "        x_windows = window_partition(shifted_x, window_size)\n",
        "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
        "        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n",
        "        shifted_x = window_reverse(attn_windows, window_size, dims)\n",
        "        if any(i > 0 for i in shift_size):\n",
        "            if len(x_shape) == 5:\n",
        "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
        "            elif len(x_shape) == 4:\n",
        "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        if len(x_shape) == 5:\n",
        "            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
        "                x = x[:, :d, :h, :w, :].contiguous()\n",
        "        elif len(x_shape) == 4:\n",
        "            if pad_r > 0 or pad_b > 0:\n",
        "                x = x[:, :h, :w, :].contiguous()\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_part2(self, x):\n",
        "        return self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "    def load_from(self, weights, n_block, layer):\n",
        "        root = f\"module.{layer}.0.blocks.{n_block}.\"\n",
        "        block_names = [\n",
        "            \"norm1.weight\",\n",
        "            \"norm1.bias\",\n",
        "            \"attn.relative_position_bias_table\",\n",
        "            \"attn.relative_position_index\",\n",
        "            \"attn.qkv.weight\",\n",
        "            \"attn.qkv.bias\",\n",
        "            \"attn.proj.weight\",\n",
        "            \"attn.proj.bias\",\n",
        "            \"norm2.weight\",\n",
        "            \"norm2.bias\",\n",
        "            \"mlp.fc1.weight\",\n",
        "            \"mlp.fc1.bias\",\n",
        "            \"mlp.fc2.weight\",\n",
        "            \"mlp.fc2.bias\",\n",
        "        ]\n",
        "        with torch.no_grad():\n",
        "            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n",
        "            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n",
        "            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n",
        "            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])\n",
        "            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n",
        "            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n",
        "            self.attn.proj.weight.copy_(weights[\"state_dict\"][root + block_names[6]])\n",
        "            self.attn.proj.bias.copy_(weights[\"state_dict\"][root + block_names[7]])\n",
        "            self.norm2.weight.copy_(weights[\"state_dict\"][root + block_names[8]])\n",
        "            self.norm2.bias.copy_(weights[\"state_dict\"][root + block_names[9]])\n",
        "            self.mlp.linear1.weight.copy_(weights[\"state_dict\"][root + block_names[10]])\n",
        "            self.mlp.linear1.bias.copy_(weights[\"state_dict\"][root + block_names[11]])\n",
        "            self.mlp.linear2.weight.copy_(weights[\"state_dict\"][root + block_names[12]])\n",
        "            self.mlp.linear2.bias.copy_(weights[\"state_dict\"][root + block_names[13]])\n",
        "\n",
        "    def forward(self, x, mask_matrix):\n",
        "        shortcut = x\n",
        "        if self.use_checkpoint:\n",
        "            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n",
        "        else:\n",
        "            x = self.forward_part1(x, mask_matrix)\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        if self.use_checkpoint:\n",
        "            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n",
        "        else:\n",
        "            x = x + self.forward_part2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMergingV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch merging layer based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, norm_layer: type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: number of feature channels.\n",
        "            norm_layer: normalization layer.\n",
        "            spatial_dims: number of spatial dims.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        if spatial_dims == 3:\n",
        "            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
        "            self.norm = norm_layer(8 * dim)\n",
        "        elif spatial_dims == 2:\n",
        "            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "            self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shape = x.size()\n",
        "        if len(x_shape) == 5:\n",
        "            b, d, h, w, c = x_shape\n",
        "            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
        "            if pad_input:\n",
        "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
        "            x = torch.cat(\n",
        "                [x[:, i::2, j::2, k::2, :] for i, j, k in itertools.product(range(2), range(2), range(2))], -1\n",
        "            )\n",
        "\n",
        "        elif len(x_shape) == 4:\n",
        "            b, h, w, c = x_shape\n",
        "            pad_input = (h % 2 == 1) or (w % 2 == 1)\n",
        "            if pad_input:\n",
        "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n",
        "            x = torch.cat([x[:, j::2, i::2, :] for i, j in itertools.product(range(2), range(2))], -1)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMerging(PatchMergingV2):\n",
        "    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shape = x.size()\n",
        "        if len(x_shape) == 4:\n",
        "            return super().forward(x)\n",
        "        if len(x_shape) != 5:\n",
        "            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n",
        "        b, d, h, w, c = x_shape\n",
        "        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
        "        if pad_input:\n",
        "            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
        "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
        "        x3 = x[:, 0::2, 0::2, 1::2, :]\n",
        "        x4 = x[:, 1::2, 0::2, 1::2, :]\n",
        "        x5 = x[:, 0::2, 1::2, 0::2, :]\n",
        "        x6 = x[:, 0::2, 0::2, 1::2, :]\n",
        "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
        "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n",
        "\n",
        "\n",
        "def compute_mask(dims, window_size, shift_size, device):\n",
        "    \"\"\"Computing region masks based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "\n",
        "     Args:\n",
        "        dims: dimension values.\n",
        "        window_size: local window size.\n",
        "        shift_size: shift size.\n",
        "        device: device.\n",
        "    \"\"\"\n",
        "\n",
        "    cnt = 0\n",
        "\n",
        "    if len(dims) == 3:\n",
        "        d, h, w = dims\n",
        "        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n",
        "        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
        "            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
        "                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
        "                    img_mask[:, d, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "\n",
        "    elif len(dims) == 2:\n",
        "        h, w = dims\n",
        "        img_mask = torch.zeros((1, h, w, 1), device=device)\n",
        "        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
        "            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
        "                img_mask[:, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "\n",
        "    mask_windows = window_partition(img_mask, window_size)\n",
        "    mask_windows = mask_windows.squeeze(-1)\n",
        "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "\n",
        "    return attn_mask\n",
        "\n",
        "\n",
        "class BasicLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        depth: int,\n",
        "        num_heads: int,\n",
        "        window_size: Sequence[int],\n",
        "        drop_path: list,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = False,\n",
        "        drop: float = 0.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
        "        downsample: nn.Module | None = None,\n",
        "        use_checkpoint: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: number of feature channels.\n",
        "            depth: number of layers in each stage.\n",
        "            num_heads: number of attention heads.\n",
        "            window_size: local window size.\n",
        "            drop_path: stochastic depth rate.\n",
        "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
        "            qkv_bias: add a learnable bias to query, key, value.\n",
        "            drop: dropout rate.\n",
        "            attn_drop: attention dropout rate.\n",
        "            norm_layer: normalization layer.\n",
        "            downsample: an optional downsampling layer at the end of the layer.\n",
        "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = tuple(i // 2 for i in window_size)\n",
        "        self.no_shift = tuple(0 for i in window_size)\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                SwinTransformerBlock(\n",
        "                    dim=dim,\n",
        "                    num_heads=num_heads,\n",
        "                    window_size=self.window_size,\n",
        "                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    qkv_bias=qkv_bias,\n",
        "                    drop=drop,\n",
        "                    attn_drop=attn_drop,\n",
        "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                    norm_layer=norm_layer,\n",
        "                    use_checkpoint=use_checkpoint,\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        self.downsample = downsample\n",
        "        if callable(self.downsample):\n",
        "            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_shape = x.size()\n",
        "        if len(x_shape) == 5:\n",
        "            b, c, d, h, w = x_shape\n",
        "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
        "            x = rearrange(x, \"b c d h w -> b d h w c\")\n",
        "            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n",
        "            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n",
        "            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n",
        "            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n",
        "            for blk in self.blocks:\n",
        "                x = blk(x, attn_mask)\n",
        "            x = x.view(b, d, h, w, -1)\n",
        "            if self.downsample is not None:\n",
        "                x = self.downsample(x)\n",
        "            x = rearrange(x, \"b d h w c -> b c d h w\")\n",
        "\n",
        "        elif len(x_shape) == 4:\n",
        "            b, c, h, w = x_shape\n",
        "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
        "            x = rearrange(x, \"b c h w -> b h w c\")\n",
        "            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n",
        "            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n",
        "            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)\n",
        "            for blk in self.blocks:\n",
        "                x = blk(x, attn_mask)\n",
        "            x = x.view(b, h, w, -1)\n",
        "            if self.downsample is not None:\n",
        "                x = self.downsample(x)\n",
        "            x = rearrange(x, \"b h w c -> b c h w\")\n",
        "        return x\n",
        "\n",
        "\n",
        "class SwinTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer based on: \"Liu et al.,\n",
        "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
        "    <https://arxiv.org/abs/2103.14030>\"\n",
        "    https://github.com/microsoft/Swin-Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_chans: int,\n",
        "        embed_dim: int,\n",
        "        window_size: Sequence[int],\n",
        "        patch_size: Sequence[int],\n",
        "        depths: Sequence[int],\n",
        "        num_heads: Sequence[int],\n",
        "        mlp_ratio: float = 4.0,\n",
        "        qkv_bias: bool = True,\n",
        "        drop_rate: float = 0.0,\n",
        "        attn_drop_rate: float = 0.0,\n",
        "        drop_path_rate: float = 0.0,\n",
        "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
        "        patch_norm: bool = False,\n",
        "        use_checkpoint: bool = False,\n",
        "        spatial_dims: int = 3,\n",
        "        downsample=\"merging\",\n",
        "        use_v2=False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_chans: dimension of input channels.\n",
        "            embed_dim: number of linear projection output channels.\n",
        "            window_size: local window size.\n",
        "            patch_size: patch size.\n",
        "            depths: number of layers in each stage.\n",
        "            num_heads: number of attention heads.\n",
        "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
        "            qkv_bias: add a learnable bias to query, key, value.\n",
        "            drop_rate: dropout rate.\n",
        "            attn_drop_rate: attention dropout rate.\n",
        "            drop_path_rate: stochastic depth rate.\n",
        "            norm_layer: normalization layer.\n",
        "            patch_norm: add normalization after patch embedding.\n",
        "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
        "            spatial_dims: spatial dimension.\n",
        "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n",
        "                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n",
        "                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n",
        "            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_norm = patch_norm\n",
        "        self.window_size = window_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            patch_size=self.patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n",
        "            spatial_dims=spatial_dims,\n",
        "        )\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        self.use_v2 = use_v2\n",
        "        self.layers1 = nn.ModuleList()\n",
        "        self.layers2 = nn.ModuleList()\n",
        "        self.layers3 = nn.ModuleList()\n",
        "        self.layers4 = nn.ModuleList()\n",
        "        if self.use_v2:\n",
        "            self.layers1c = nn.ModuleList()\n",
        "            self.layers2c = nn.ModuleList()\n",
        "            self.layers3c = nn.ModuleList()\n",
        "            self.layers4c = nn.ModuleList()\n",
        "        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = BasicLayer(\n",
        "                dim=int(embed_dim * 2**i_layer),\n",
        "                depth=depths[i_layer],\n",
        "                num_heads=num_heads[i_layer],\n",
        "                window_size=self.window_size,\n",
        "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=down_sample_mod,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "            )\n",
        "            if i_layer == 0:\n",
        "                self.layers1.append(layer)\n",
        "            elif i_layer == 1:\n",
        "                self.layers2.append(layer)\n",
        "            elif i_layer == 2:\n",
        "                self.layers3.append(layer)\n",
        "            elif i_layer == 3:\n",
        "                self.layers4.append(layer)\n",
        "            if self.use_v2:\n",
        "                layerc = UnetrBasicBlock(\n",
        "                    spatial_dims=spatial_dims,\n",
        "                    in_channels=embed_dim * 2**i_layer,\n",
        "                    out_channels=embed_dim * 2**i_layer,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    norm_name=\"instance\",\n",
        "                    res_block=True,\n",
        "                )\n",
        "                if i_layer == 0:\n",
        "                    self.layers1c.append(layerc)\n",
        "                elif i_layer == 1:\n",
        "                    self.layers2c.append(layerc)\n",
        "                elif i_layer == 2:\n",
        "                    self.layers3c.append(layerc)\n",
        "                elif i_layer == 3:\n",
        "                    self.layers4c.append(layerc)\n",
        "\n",
        "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
        "\n",
        "    def proj_out(self, x, normalize=False):\n",
        "        if normalize:\n",
        "            x_shape = x.size()\n",
        "            if len(x_shape) == 5:\n",
        "                n, ch, d, h, w = x_shape\n",
        "                x = rearrange(x, \"n c d h w -> n d h w c\")\n",
        "                x = F.layer_norm(x, [ch])\n",
        "                x = rearrange(x, \"n d h w c -> n c d h w\")\n",
        "            elif len(x_shape) == 4:\n",
        "                n, ch, h, w = x_shape\n",
        "                x = rearrange(x, \"n c h w -> n h w c\")\n",
        "                x = F.layer_norm(x, [ch])\n",
        "                x = rearrange(x, \"n h w c -> n c h w\")\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, normalize=True):\n",
        "        x0 = self.patch_embed(x)\n",
        "        x0 = self.pos_drop(x0)\n",
        "        x0_out = self.proj_out(x0, normalize)\n",
        "        if self.use_v2:\n",
        "            x0 = self.layers1c[0](x0.contiguous())\n",
        "        x1 = self.layers1[0](x0.contiguous())\n",
        "        x1_out = self.proj_out(x1, normalize)\n",
        "        if self.use_v2:\n",
        "            x1 = self.layers2c[0](x1.contiguous())\n",
        "        x2 = self.layers2[0](x1.contiguous())\n",
        "        x2_out = self.proj_out(x2, normalize)\n",
        "        if self.use_v2:\n",
        "            x2 = self.layers3c[0](x2.contiguous())\n",
        "        x3 = self.layers3[0](x2.contiguous())\n",
        "        x3_out = self.proj_out(x3, normalize)\n",
        "        if self.use_v2:\n",
        "            x3 = self.layers4c[0](x3.contiguous())\n",
        "        x4 = self.layers4[0](x3.contiguous())\n",
        "        x4_out = self.proj_out(x4, normalize)\n",
        "        return [x0_out, x1_out, x2_out, x3_out, x4_out]\n",
        "\n",
        "\n",
        "def filter_swinunetr(key, value):\n",
        "    \"\"\"\n",
        "    A filter function used to filter the pretrained weights from [1], then the weights can be loaded into MONAI SwinUNETR Model.\n",
        "    This function is typically used with `monai.networks.copy_model_state`\n",
        "    [1] \"Valanarasu JM et al., Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training\n",
        "    <https://arxiv.org/abs/2307.16896>\"\n",
        "\n",
        "    Args:\n",
        "        key: the key in the source state dict used for the update.\n",
        "        value: the value in the source state dict used for the update.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        import torch\n",
        "        from monai.apps import download_url\n",
        "        from monai.networks.utils import copy_model_state\n",
        "        from monai.networks.nets.swin_unetr import SwinUNETR, filter_swinunetr\n",
        "\n",
        "        model = SwinUNETR(img_size=(96, 96, 96), in_channels=1, out_channels=3, feature_size=48)\n",
        "        resource = (\n",
        "            \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/ssl_pretrained_weights.pth\"\n",
        "        )\n",
        "        ssl_weights_path = \"./ssl_pretrained_weights.pth\"\n",
        "        download_url(resource, ssl_weights_path)\n",
        "        ssl_weights = torch.load(ssl_weights_path)[\"model\"]\n",
        "\n",
        "        dst_dict, loaded, not_loaded = copy_model_state(model, ssl_weights, filter_func=filter_swinunetr)\n",
        "\n",
        "    \"\"\"\n",
        "    if key in [\n",
        "        \"encoder.mask_token\",\n",
        "        \"encoder.norm.weight\",\n",
        "        \"encoder.norm.bias\",\n",
        "        \"out.conv.conv.weight\",\n",
        "        \"out.conv.conv.bias\",\n",
        "    ]:\n",
        "        return None\n",
        "\n",
        "    if key[:8] == \"encoder.\":\n",
        "        if key[8:19] == \"patch_embed\":\n",
        "            new_key = \"swinViT.\" + key[8:]\n",
        "        else:\n",
        "            new_key = \"swinViT.\" + key[8:18] + key[20:]\n",
        "\n",
        "        return new_key, value\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "class SwinNext(nn.Module):\n",
        "\n",
        "\n",
        "    patch_size: Final[int] = 2\n",
        "\n",
        "    def __init__(self,\n",
        "                 img_size: Union[Sequence[int], int],\n",
        "                 in_channels: int,\n",
        "                 n_channels: int,\n",
        "                 n_classes: int,\n",
        "                 exp_r: int = 4,  # Expansion ratio as in Swin Transformers\n",
        "                 kernel_size: int = 7,  # Ofcourse can test kernel_size\n",
        "                 enc_kernel_size: int = None,\n",
        "                 dec_kernel_size: int = None,\n",
        "                 deep_supervision: bool = False,  # Can be used to test deep supervision\n",
        "                 do_res: bool = False,  # Can be used to individually test residual connection\n",
        "                 do_res_up_down: bool = False,  # Additional 'res' connection on up and down convs\n",
        "                 checkpoint_style: bool = None,  # Either inside block or outside block\n",
        "                 block_counts: list = [2, 2, 2, 2, 2, 2, 2, 2, 2],  # Can be used to test staging ratio:\n",
        "                 norm_type='group',\n",
        "                 dim='3d',  # 2d or 3d\n",
        "                 grn=False,\n",
        "                 depths: Sequence[int] = (2, 2, 2, 2),\n",
        "                 num_heads: Sequence[int] = (3, 6, 12, 24),\n",
        "                 norm_name: Union[tuple, str] = \"instance\",\n",
        "                 drop_rate: float = 0.0,\n",
        "                 attn_drop_rate: float = 0.0,\n",
        "                 dropout_path_rate: float = 0.0,\n",
        "                 normalize: bool = True,\n",
        "                 use_checkpoint: bool = False,\n",
        "                 spatial_dims: int = 3,\n",
        "                 downsample=\"merging\",\n",
        "                 use_v2=False,\n",
        "                 expansion_type = \"nochange\",\n",
        "                 trans_dim = 30,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.do_ds = deep_supervision\n",
        "        assert checkpoint_style in [None, 'outside_block']\n",
        "        self.inside_block_checkpointing = False\n",
        "        self.outside_block_checkpointing = False\n",
        "        if checkpoint_style == 'outside_block':\n",
        "            self.outside_block_checkpointing = True\n",
        "        assert dim in ['2d', '3d']\n",
        "\n",
        "        if kernel_size is not None:\n",
        "            enc_kernel_size = kernel_size\n",
        "            dec_kernel_size = kernel_size\n",
        "\n",
        "        if dim == '2d':\n",
        "            conv = nn.Conv2d\n",
        "        elif dim == '3d':\n",
        "            conv = nn.Conv3d\n",
        "\n",
        "        self.stem = conv(in_channels, n_channels, kernel_size=1)\n",
        "        if type(exp_r) == int:\n",
        "            exp_r = [exp_r for i in range(len(block_counts))]\n",
        "\n",
        "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
        "        patch_sizes = ensure_tuple_rep(self.patch_size, spatial_dims)\n",
        "        window_size = ensure_tuple_rep(7, spatial_dims)\n",
        "\n",
        "        if spatial_dims not in (2, 3):\n",
        "            raise ValueError(\"spatial dimension should be 2 or 3.\")\n",
        "\n",
        "        self._check_input_size(img_size)\n",
        "\n",
        "        if not (0 <= drop_rate <= 1):\n",
        "            raise ValueError(\"dropout rate should be between 0 and 1.\")\n",
        "\n",
        "        if not (0 <= attn_drop_rate <= 1):\n",
        "            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n",
        "\n",
        "        if not (0 <= dropout_path_rate <= 1):\n",
        "            raise ValueError(\"drop path rate should be between 0 and 1.\")\n",
        "\n",
        "        if n_channels % 12 != 0:\n",
        "            raise ValueError(\"feature_size should be divisible by 12.\")\n",
        "\n",
        "        self.normalize = normalize\n",
        "        self.trans_dim = trans_dim\n",
        "        self.expansion_type = expansion_type\n",
        "\n",
        "        self.swinViT = SwinTransformer(\n",
        "            in_chans=in_channels,\n",
        "            embed_dim=n_channels,\n",
        "            window_size=window_size,\n",
        "            patch_size=patch_sizes,\n",
        "            depths=depths,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=4.0,\n",
        "            qkv_bias=True,\n",
        "            drop_rate=drop_rate,\n",
        "            attn_drop_rate=attn_drop_rate,\n",
        "            drop_path_rate=dropout_path_rate,\n",
        "            norm_layer=nn.LayerNorm,\n",
        "            use_checkpoint=use_checkpoint,\n",
        "            spatial_dims=spatial_dims,\n",
        "            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,\n",
        "            use_v2=use_v2,\n",
        "        )\n",
        "\n",
        "        self.encoder0 = UnetrBasicBlock(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=n_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "            res_block=True,\n",
        "        )\n",
        "\n",
        "        self.encoder1 = UnetrBasicBlock(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=n_channels,\n",
        "            out_channels=n_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "            res_block=True,\n",
        "        )\n",
        "\n",
        "        self.encoder2 = UnetrBasicBlock(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=2*n_channels,\n",
        "            out_channels=2*n_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "            res_block=True,\n",
        "        )\n",
        "\n",
        "        self.encoder3 = UnetrBasicBlock(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=4 * n_channels,\n",
        "            out_channels=4 * n_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "            res_block=True,\n",
        "        )\n",
        "\n",
        "        self.encoder4 = UnetrBasicBlock(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=8 * n_channels,\n",
        "            out_channels=8 * n_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "            res_block=True,\n",
        "        )\n",
        "\n",
        "        self.encoder5 = UnetrBasicBlock(\n",
        "            spatial_dims=spatial_dims,\n",
        "            in_channels=16 * n_channels,\n",
        "            out_channels=16 * n_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "            res_block=True,\n",
        "        )\n",
        "\n",
        "\n",
        "        if self.expansion_type == \"cat\":\n",
        "          self.up_4 = MedNeXtUpBlock(\n",
        "              in_channels=16 * n_channels + self.trans_dim,\n",
        "              out_channels=8 * n_channels,\n",
        "              exp_r=exp_r[5],\n",
        "              kernel_size=dec_kernel_size,\n",
        "              do_res=do_res_up_down,\n",
        "              norm_type=norm_type,\n",
        "              dim=dim,\n",
        "              grn=grn\n",
        "          )\n",
        "        else:\n",
        "          self.up_4 = MedNeXtUpBlock(\n",
        "              in_channels=16 * n_channels,\n",
        "              out_channels=8 * n_channels,\n",
        "              exp_r=exp_r[5],\n",
        "              kernel_size=dec_kernel_size,\n",
        "              do_res=do_res_up_down,\n",
        "              norm_type=norm_type,\n",
        "              dim=dim,\n",
        "              grn=grn\n",
        "          )\n",
        "\n",
        "        self.dec_block_4 = nn.Sequential(*[\n",
        "            MedNeXtBlock(\n",
        "                in_channels=n_channels * 8,\n",
        "                out_channels=n_channels * 8,\n",
        "                exp_r=exp_r[5],\n",
        "                kernel_size=dec_kernel_size,\n",
        "                do_res=do_res,\n",
        "                norm_type=norm_type,\n",
        "                dim=dim,\n",
        "                grn=grn\n",
        "            )\n",
        "            for i in range(block_counts[5])])\n",
        "\n",
        "        self.up_3 = MedNeXtUpBlock(\n",
        "            in_channels=8 * n_channels,\n",
        "            out_channels=4 * n_channels,\n",
        "            exp_r=exp_r[6],\n",
        "            kernel_size=dec_kernel_size,\n",
        "            do_res=do_res_up_down,\n",
        "            norm_type=norm_type,\n",
        "            dim=dim,\n",
        "            grn=grn\n",
        "        )\n",
        "\n",
        "        self.dec_block_3 = nn.Sequential(*[\n",
        "            MedNeXtBlock(\n",
        "                in_channels=n_channels * 4,\n",
        "                out_channels=n_channels * 4,\n",
        "                exp_r=exp_r[6],\n",
        "                kernel_size=dec_kernel_size,\n",
        "                do_res=do_res,\n",
        "                norm_type=norm_type,\n",
        "                dim=dim,\n",
        "                grn=grn\n",
        "            )\n",
        "            for i in range(block_counts[6])])\n",
        "\n",
        "        self.up_2 = MedNeXtUpBlock(\n",
        "            in_channels=4 * n_channels,\n",
        "            out_channels=2 * n_channels,\n",
        "            exp_r=exp_r[7],\n",
        "            kernel_size=dec_kernel_size,\n",
        "            do_res=do_res_up_down,\n",
        "            norm_type=norm_type,\n",
        "            dim=dim,\n",
        "            grn=grn\n",
        "        )\n",
        "\n",
        "        self.dec_block_2 = nn.Sequential(*[\n",
        "            MedNeXtBlock(\n",
        "                in_channels=n_channels * 2,\n",
        "                out_channels=n_channels * 2,\n",
        "                exp_r=exp_r[7],\n",
        "                kernel_size=dec_kernel_size,\n",
        "                do_res=do_res,\n",
        "                norm_type=norm_type,\n",
        "                dim=dim,\n",
        "                grn=grn\n",
        "            )\n",
        "            for i in range(block_counts[7])])\n",
        "\n",
        "        self.up_1 = MedNeXtUpBlock(\n",
        "            in_channels=2 * n_channels,\n",
        "            out_channels=n_channels,\n",
        "            exp_r=exp_r[8],\n",
        "            kernel_size=dec_kernel_size,\n",
        "            do_res=do_res_up_down,\n",
        "            norm_type=norm_type,\n",
        "            dim=dim,\n",
        "            grn=grn\n",
        "        )\n",
        "\n",
        "        self.dec_block_1 = nn.Sequential(*[\n",
        "            MedNeXtBlock(\n",
        "                in_channels=n_channels,\n",
        "                out_channels=n_channels,\n",
        "                exp_r=exp_r[8],\n",
        "                kernel_size=dec_kernel_size,\n",
        "                do_res=do_res,\n",
        "                norm_type=norm_type,\n",
        "                dim=dim,\n",
        "                grn=grn\n",
        "            )\n",
        "            for i in range(block_counts[8])])\n",
        "\n",
        "        self.up_0 = MedNeXtUpBlock(\n",
        "            in_channels=n_channels,\n",
        "            out_channels=n_channels,\n",
        "            exp_r=exp_r[9],\n",
        "            kernel_size=dec_kernel_size,\n",
        "            do_res=do_res_up_down,\n",
        "            norm_type=norm_type,\n",
        "            dim=dim,\n",
        "            grn=grn\n",
        "        )\n",
        "\n",
        "        self.dec_block_0 = nn.Sequential(*[\n",
        "            MedNeXtBlock(\n",
        "                in_channels=n_channels,\n",
        "                out_channels=n_channels,\n",
        "                exp_r=exp_r[9],\n",
        "                kernel_size=dec_kernel_size,\n",
        "                do_res=do_res,\n",
        "                norm_type=norm_type,\n",
        "                dim=dim,\n",
        "                grn=grn\n",
        "            )\n",
        "            for i in range(block_counts[9])])\n",
        "\n",
        "        self.out_0 = OutBlock(in_channels=n_channels, n_classes=n_classes, dim=dim)\n",
        "\n",
        "\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _check_input_size(self, spatial_shape):\n",
        "        img_size = np.array(spatial_shape)\n",
        "        remainder = (img_size % np.power(self.patch_size, 5)) > 0\n",
        "        if remainder.any():\n",
        "            wrong_dims = (np.where(remainder)[0] + 2).tolist()\n",
        "            raise ValueError(\n",
        "                f\"spatial dimensions {wrong_dims} of input image (spatial shape: {spatial_shape})\"\n",
        "                f\" must be divisible by {self.patch_size}**5.\"\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, support_data=None):\n",
        "        support_data = x[\"table\"]\n",
        "        x = x[\"inputs\"]\n",
        "\n",
        "        #print(support_data)\n",
        "\n",
        "        # x = (x[1] + support_data)\n",
        "        if self.expansion_type != \"nochange\":\n",
        "            flattened_tensors = [t.float().flatten() for t in support_data]\n",
        "            concatenated_tensor = torch.cat(flattened_tensors)\n",
        "            support_data = concatenated_tensor.view(-1, 59).to('cuda')\n",
        "\n",
        "        if not torch.jit.is_scripting():\n",
        "            self._check_input_size(x.shape[2:])\n",
        "        hidden_states_out = self.swinViT(x, self.normalize)\n",
        "\n",
        "        enc0 = self.encoder0(x) #n/2\n",
        "        #print(\"enc0:\", enc0.size())\n",
        "        enc1 = self.encoder1(hidden_states_out[0]) #n\n",
        "        #print(\"enc0:\", enc0.size())\n",
        "        enc2 = self.encoder2(hidden_states_out[1]) #2n\n",
        "        #print(\"enc1:\", enc1.size())\n",
        "        enc3 = self.encoder3(hidden_states_out[2]) #4n\n",
        "        #print(\"enc2:\", enc2.size())\n",
        "        enc4 = self.encoder4(hidden_states_out[3]) #8n\n",
        "        #print(\"enc3:\", enc3.size())\n",
        "        enc5 = self.encoder5(hidden_states_out[4]) #16n\n",
        "        #print(\"enc5:\", enc5.size())\n",
        "\n",
        "        if self.expansion_type == \"nochange\":\n",
        "          x = enc5\n",
        "        elif self.expansion_type == \"cross\":\n",
        "          attention_module = CrossAttention(enc5.size()[1], 59)\n",
        "          x = attention_module(enc5, support_data)\n",
        "        elif self.expansion_type == \"cat\":\n",
        "          catmixture_module = Catmixture(self.trans_dim)\n",
        "          x = catmixture_module(enc5, support_data)\n",
        "        elif self.expansion_type == \"add\":\n",
        "          addmixture_module = Addmixture(enc5.size()[1])\n",
        "          x = addmixture_module(enc5, support_data)\n",
        "\n",
        "        #print(\"x:\", x.size())\n",
        "        dec4 = self.up_4(x)\n",
        "        dec_x = enc4 + dec4\n",
        "        #print(\"dec4:\", dec_x.size())\n",
        "        x = self.dec_block_4(dec_x)\n",
        "        del enc4, dec4\n",
        "\n",
        "\n",
        "        dec3 = self.up_3(x)\n",
        "        dec_x = enc3 + dec3\n",
        "        #print(\"dec3:\", dec_x.size())\n",
        "        x = self.dec_block_3(dec_x)\n",
        "        del enc3, dec3\n",
        "\n",
        "\n",
        "        dec2 = self.up_2(x)\n",
        "        dec_x = enc2 + dec2\n",
        "        #print(\"dec2:\", dec_x.size())\n",
        "        x = self.dec_block_2(dec_x)\n",
        "        del enc2, dec2\n",
        "\n",
        "\n",
        "        dec1 = self.up_1(x)\n",
        "        dec_x = enc1 + dec1\n",
        "        #print(\"dec1:\", dec_x.size())\n",
        "        x = self.dec_block_1(dec_x)\n",
        "        del enc1, dec1\n",
        "\n",
        "        dec0 = self.up_0(x)\n",
        "        dec_x = enc0 + dec0\n",
        "        #print(\"dec0:\", dec_x.size())\n",
        "        x = self.dec_block_0(dec_x)  # lhz\n",
        "        del enc0, dec0, dec_x\n",
        "\n",
        "        x = self.out_0(x)\n",
        "\n",
        "        return x\n",
        "if __name__ == \"__main__\":\n",
        "    print('run network')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nGgqn4SVu7NX",
      "metadata": {
        "id": "nGgqn4SVu7NX"
      },
      "source": [
        "The following code constructs a training function and calls the aforementioned module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "af8b2ac9-01e0-42e2-8434-fca15dbf1184",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af8b2ac9-01e0-42e2-8434-fca15dbf1184",
        "outputId": "51dda628-b6b9-4fb2-f333-827538bb2fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run trainer\n"
          ]
        }
      ],
      "source": [
        "from typing import Any, Dict, Tuple\n",
        "\n",
        "import torch\n",
        "from ignite.engine import Engine\n",
        "from monai.engines import SupervisedTrainer\n",
        "from monai.engines.utils import CommonKeys as Keys\n",
        "from monai.engines.utils import IterationEvents\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "\n",
        "class DynUNetTrainer(SupervisedTrainer):\n",
        "    \"\"\"\n",
        "    This class inherits from SupervisedTrainer in MONAI, and is used with DynUNet\n",
        "    on Decathlon datasets.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def _iteration(self, engine: Engine, batchdata: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Callback function for the Supervised Training processing logic of 1 iteration in Ignite Engine.\n",
        "        Return below items in a dictionary:\n",
        "            - IMAGE: image Tensor data for model input, already moved to device.\n",
        "            - LABEL: label Tensor data corresponding to the image, already moved to device.\n",
        "            - PRED: prediction result of model.\n",
        "            - LOSS: loss value computed by loss function.\n",
        "\n",
        "        Args:\n",
        "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
        "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: When ``batchdata`` is None.\n",
        "\n",
        "        \"\"\"\n",
        "        if batchdata is None:\n",
        "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
        "        batchdata = batchdata[0]\n",
        "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
        "\n",
        "        table = batchdata[\"table\"]\n",
        "        if len(batch) == 2:\n",
        "            inputs, targets = batch\n",
        "            args: Tuple = ()\n",
        "            kwargs: Dict = {}\n",
        "            temp = {\"inputs\": inputs, \"table\": table}\n",
        "        else:\n",
        "            inputs, targets, args, kwargs = batch\n",
        "        # put iteration outputs into engine.state\n",
        "        engine.state.output = {Keys.IMAGE: inputs, Keys.LABEL: targets}\n",
        "\n",
        "        def _compute_pred_loss():\n",
        "            preds = self.inferer(temp, self.network, *args, **kwargs)\n",
        "            if len(preds.size()) - len(targets.size()) == 1:\n",
        "                # deep supervision mode, need to unbind feature maps first.\n",
        "                # print('run unbind')\n",
        "                preds = torch.unbind(preds, dim=1)\n",
        "            engine.state.output[Keys.PRED] = preds\n",
        "            del preds\n",
        "            engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
        "            # print('targets', targets.shape)\n",
        "            if 1:\n",
        "                engine.state.output[Keys.LOSS] = self.loss_function.forward(engine.state.output[Keys.PRED], targets)\n",
        "            else:\n",
        "                engine.state.output[Keys.LOSS] = sum(\n",
        "                    0.5**i * self.loss_function.forward(p, targets) for i, p in enumerate(engine.state.output[Keys.PRED])\n",
        "                )\n",
        "            engine.fire_event(IterationEvents.LOSS_COMPLETED)\n",
        "\n",
        "        self.network.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        if self.amp and self.scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                _compute_pred_loss()\n",
        "            self.scaler.scale(engine.state.output[Keys.LOSS]).backward()\n",
        "            self.scaler.unscale_(self.optimizer)\n",
        "            if isinstance(self.network, DistributedDataParallel):\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.module.parameters(), 12)\n",
        "            else:\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12)\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "        else:\n",
        "            _compute_pred_loss()\n",
        "            engine.state.output[Keys.LOSS].backward()\n",
        "            engine.fire_event(IterationEvents.BACKWARD_COMPLETED)\n",
        "            if isinstance(self.network, DistributedDataParallel):\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.module.parameters(), 12)\n",
        "            else:\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12)\n",
        "            self.optimizer.step()\n",
        "            engine.fire_event(IterationEvents.MODEL_COMPLETED)\n",
        "\n",
        "        return engine.state.output\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print('run trainer')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HtLbCWE-u4Xw",
      "metadata": {
        "id": "HtLbCWE-u4Xw"
      },
      "source": [
        "The following code constructs a training function and calls the aforementioned module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b807ab05-ca3f-42f3-b538-49eaf7b99e74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b807ab05-ca3f-42f3-b538-49eaf7b99e74",
        "outputId": "a11172a3-8e64-4361-c6df-cc93dd9db250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run main 2\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import os\n",
        "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
        "\n",
        "import ignite.distributed as idist\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from monai.config import print_config\n",
        "from monai.handlers import CheckpointSaver, LrScheduleHandler, MeanDice, StatsHandler, ValidationHandler, from_engine\n",
        "from monai.inferers import SimpleInferer#, SlidingWindowInferer\n",
        "# from inferer_self import SlidingWindowInferer\n",
        "from monai.losses import DiceCELoss\n",
        "from monai.utils import set_determinism\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "# from create_dataset import get_data\n",
        "# from create_network import get_network\n",
        "# from evaluator import DynUNetEvaluator\n",
        "# from task_params import data_loader_params, patch_size\n",
        "# from trainer import DynUNetTrainer\n",
        "\n",
        "\n",
        "def train(root_path='/root', max_epochs_value=250):\n",
        "    # load hyper parameters\n",
        "    task_id = '08'\n",
        "    fold = '0'\n",
        "    val_output_dir = \"./runs_{}_fold{}_{}/\".format(task_id, fold, 'expr')\n",
        "    log_filename = \"nnunet_task{}_fold{}.log\".format(task_id, fold)\n",
        "    log_filename = os.path.join(val_output_dir, log_filename)\n",
        "    interval = 10\n",
        "    learning_rate = 0.01\n",
        "    max_epochs = max_epochs_value\n",
        "    multi_gpu_flag = False\n",
        "    amp_flag = False\n",
        "    lr_decay_flag = False\n",
        "    sw_batch_size = 4\n",
        "    tta_val = False\n",
        "    batch_dice = False\n",
        "    window_mode = 'gaussian'\n",
        "    eval_overlap = 0.5\n",
        "    local_rank = 0\n",
        "    determinism_flag = False\n",
        "    determinism_seed = 0\n",
        "    if determinism_flag:\n",
        "        set_determinism(seed=determinism_seed)\n",
        "        if local_rank == 0:\n",
        "            print(\"Using deterministic training.\")\n",
        "\n",
        "    # transforms\n",
        "    train_batch_size = data_loader_params[task_id][\"batch_size\"]\n",
        "    if multi_gpu_flag:\n",
        "        dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
        "\n",
        "        device = torch.device(f\"cuda:{local_rank}\")\n",
        "        torch.cuda.set_device(device)\n",
        "    else:\n",
        "        device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "    properties, val_loader = get_data(root_path, mode=\"validation\")\n",
        "    _, train_loader = get_data(root_path, batch_size=train_batch_size, mode=\"train\")\n",
        "\n",
        "    # produce the network\n",
        "    checkpoint = None\n",
        "    net = get_network(properties, task_id, val_output_dir, checkpoint)\n",
        "    net = net.to(device)\n",
        "\n",
        "    if multi_gpu_flag:\n",
        "        net = DistributedDataParallel(module=net, device_ids=[device])\n",
        "\n",
        "    optimizer = torch.optim.SGD(\n",
        "        net.parameters(),\n",
        "        lr=learning_rate,\n",
        "        momentum=0.99,\n",
        "        weight_decay=3e-5,\n",
        "        nesterov=True,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: (1 - epoch / max_epochs) ** 0.9)\n",
        "    # produce evaluator\n",
        "    val_handlers = (\n",
        "        [\n",
        "            StatsHandler(output_transform=lambda x: None),\n",
        "            CheckpointSaver(save_dir=val_output_dir, save_dict={\"net\": net}, save_key_metric=True, save_interval=10, n_saved=3),\n",
        "        ]\n",
        "        if idist.get_rank() == 0\n",
        "        else None\n",
        "    )\n",
        "    print('new run epoch')\n",
        "    evaluator = DynUNetEvaluator(\n",
        "        device=device,\n",
        "        val_data_loader=val_loader,\n",
        "        network=net,\n",
        "        num_classes=5,\n",
        "        inferer=SlidingWindowInferer(\n",
        "            roi_size=patch_size[task_id],\n",
        "            sw_batch_size=sw_batch_size,\n",
        "            overlap=eval_overlap,\n",
        "            mode=window_mode,\n",
        "        ),\n",
        "        postprocessing=None,\n",
        "        key_val_metric={\n",
        "            \"val_mean_dice\": MeanDice(\n",
        "                include_background=False,\n",
        "                output_transform=from_engine([\"pred\", \"label\"]),\n",
        "            )\n",
        "        },\n",
        "        val_handlers=val_handlers,\n",
        "        amp=amp_flag,\n",
        "        tta_val=tta_val,\n",
        "    )\n",
        "\n",
        "    # produce trainer\n",
        "    loss = DiceCELoss(to_onehot_y=True, softmax=True, batch=batch_dice)\n",
        "    train_handlers = [ValidationHandler(validator=evaluator, interval=1, epoch_level=True)]\n",
        "    if lr_decay_flag:\n",
        "        train_handlers += [LrScheduleHandler(lr_scheduler=scheduler, print_lr=True)]\n",
        "    if idist.get_rank() == 0:\n",
        "        train_handlers += [\n",
        "            StatsHandler(\n",
        "                tag_name=\"train_loss\",\n",
        "                output_transform=from_engine([\"loss\"], first=True),\n",
        "            )\n",
        "        ]\n",
        "    logging.basicConfig(level=logging.INFO,  # 控制台打印的日志级别\n",
        "                        filename=log_filename,\n",
        "                        filemode='a',  ##模式，有w和a，w就是写模式，每次都会重新写日志，覆盖之前的日志\n",
        "                        # a是追加模式，默认如果不写的话，就是追加模式\n",
        "                        )\n",
        "    trainer = DynUNetTrainer(\n",
        "        device=device,\n",
        "        max_epochs=max_epochs,\n",
        "        train_data_loader=train_loader,\n",
        "        network=net, # output = net(x)\n",
        "        optimizer=optimizer,\n",
        "        loss_function=loss,\n",
        "        inferer=SimpleInferer(),\n",
        "        postprocessing=None,\n",
        "        key_train_metric=None,\n",
        "        train_handlers=train_handlers,\n",
        "        amp=amp_flag,\n",
        "    )\n",
        "\n",
        "    if local_rank > 0:\n",
        "        evaluator.logger.setLevel(logging.WARNING)\n",
        "        trainer.logger.setLevel(logging.WARNING)\n",
        "\n",
        "    trainer.run()\n",
        "    if multi_gpu_flag:\n",
        "        dist.destroy_process_group()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    print('run main 2')\n",
        "#     print_config()\n",
        "\n",
        "#     root_path = '/root/autodl-tmp/dataset/'\n",
        "#     train(root_path)\n",
        "#     print('train end')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86jeK2Nfvroo",
      "metadata": {
        "id": "86jeK2Nfvroo"
      },
      "source": [
        "The following two sections of code define the inference module and function in the style of dynUnet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b3634f0f-341e-4b42-8ba3-018879fefc5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3634f0f-341e-4b42-8ba3-018879fefc5d",
        "outputId": "96ec1ab2-010f-486f-9810-0dc19b88a7be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "infere run\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import Any, Dict, Optional, Tuple, Union\n",
        "from monai.transforms import SaveImage\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from ignite.engine import Engine\n",
        "from monai.data import decollate_batch\n",
        "# from monai.data.nifti_writer import write_nifti\n",
        "from monai.engines import SupervisedEvaluator\n",
        "from monai.engines.utils import IterationEvents\n",
        "from monai.inferers import Inferer\n",
        "from monai.networks.utils import eval_mode\n",
        "from monai.transforms import AsDiscrete\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from transforms import recovery_prediction\n",
        "import nrrd\n",
        "\n",
        "class DynUNetInferrer(SupervisedEvaluator):\n",
        "    \"\"\"\n",
        "    This class inherits from SupervisedEvaluator in MONAI, and is used with DynUNet\n",
        "    on Decathlon datasets. As a customized inferrer, some of the arguments from\n",
        "    SupervisedEvaluator are not supported. For example, the actual\n",
        "    post processing method used is hard coded in the `_iteration` function, thus the\n",
        "    argument `postprocessing` from SupervisedEvaluator is not exist. If you need\n",
        "    to change the post processing way, please modify the `_iteration` function directly.\n",
        "\n",
        "    Args:\n",
        "        device: an object representing the device on which to run.\n",
        "        val_data_loader: Ignite engine use data_loader to run, must be\n",
        "            torch.DataLoader.\n",
        "        network: use the network to run model forward.\n",
        "        output_dir: the path to save inferred outputs.\n",
        "        num_classes: the number of classes (output channels) for the task.\n",
        "        inferer: inference method that execute model forward on input data, like: SlidingWindow, etc.\n",
        "        amp: whether to enable auto-mixed-precision evaluation, default is False.\n",
        "        tta_val: whether to do the 8 flips (8 = 2 ** 3, where 3 represents the three dimensions)\n",
        "            test time augmentation, default is False.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        device: torch.device,\n",
        "        val_data_loader: DataLoader,\n",
        "        network: torch.nn.Module,\n",
        "        output_dir: str,\n",
        "        num_classes: Union[str, int],\n",
        "        inferer: Optional[Inferer] = None,\n",
        "        amp: bool = False,\n",
        "        tta_val: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__(\n",
        "            device=device,\n",
        "            val_data_loader=val_data_loader,\n",
        "            network=network,\n",
        "            inferer=inferer,\n",
        "            amp=amp,\n",
        "        )\n",
        "\n",
        "        if not isinstance(num_classes, int):\n",
        "            num_classes = int(num_classes)\n",
        "        self.post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
        "        self.output_dir = output_dir\n",
        "        self.tta_val = tta_val\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def _iteration(self, engine: Engine, batchdata: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        callback function for the Supervised Evaluation processing logic of 1 iteration in Ignite Engine.\n",
        "        Return below item in a dictionary:\n",
        "            - PRED: prediction result of model.\n",
        "\n",
        "        Args:\n",
        "            engine: Ignite Engine, it can be a trainer, validator or evaluator.\n",
        "            batchdata: input data for this iteration, usually can be dictionary or tuple of Tensor data.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: When ``batchdata`` is None.\n",
        "\n",
        "        \"\"\"\n",
        "        if batchdata is None:\n",
        "            raise ValueError(\"Must provide batch data for current iteration.\")\n",
        "        batch = self.prepare_batch(batchdata, engine.state.device, engine.non_blocking)\n",
        "        table = batchdata[\"table\"]\n",
        "        if len(batch) == 2:\n",
        "            inputs, _ = batch\n",
        "            args: Tuple = ()\n",
        "            kwargs: Dict = {}\n",
        "            temp = {\"inputs\": inputs, \"table\": table}\n",
        "        else:\n",
        "            inputs, _, args, kwargs = batch\n",
        "            temp = {\"inputs\": inputs, \"table\": table}\n",
        "\n",
        "        def _compute_pred():\n",
        "            ct = 1.0\n",
        "            pred = self.inferer(temp, self.network, *args, **kwargs).cpu()\n",
        "            pred = nn.functional.softmax(pred, dim=1)\n",
        "            if not self.tta_val:\n",
        "                return pred\n",
        "            else:\n",
        "                for dims in [[2], [3], [4], (2, 3), (2, 4), (3, 4), (2, 3, 4)]:\n",
        "                    flip_inputs = torch.flip(inputs, dims=dims)\n",
        "                    flip_pred = torch.flip(self.inferer(flip_inputs, self.network).cpu(), dims=dims)\n",
        "                    flip_pred = nn.functional.softmax(flip_pred, dim=1)\n",
        "                    del flip_inputs\n",
        "                    pred += flip_pred\n",
        "                    del flip_pred\n",
        "                    ct += 1\n",
        "                return pred / ct\n",
        "\n",
        "        # execute forward computation\n",
        "        with eval_mode(self.network):\n",
        "            if self.amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    predictions = _compute_pred()\n",
        "            else:\n",
        "                predictions = _compute_pred()\n",
        "\n",
        "        inputs = inputs.cpu()\n",
        "        predictions = self.post_pred(decollate_batch(predictions)[0])\n",
        "\n",
        "        affine = batchdata[\"image_meta_dict\"][\"affine\"].numpy()[0]\n",
        "        resample_flag = batchdata[\"resample_flag\"]\n",
        "        anisotrophy_flag = batchdata[\"anisotrophy_flag\"]\n",
        "        crop_shape = batchdata[\"crop_shape\"][0].tolist()\n",
        "        original_shape = batchdata[\"original_shape\"][0].tolist()\n",
        "\n",
        "        if resample_flag:\n",
        "            # convert the prediction back to the original (after cropped) shape\n",
        "            predictions = recovery_prediction(predictions.numpy(), [self.num_classes, *crop_shape], anisotrophy_flag)\n",
        "        else:\n",
        "            predictions = predictions.numpy()\n",
        "\n",
        "        predictions = np.argmax(predictions, axis=0)\n",
        "\n",
        "        # pad the prediction back to the original shape\n",
        "        predictions_org = np.zeros([*original_shape])\n",
        "        box_start, box_end = batchdata[\"bbox\"][0]\n",
        "        h_start, w_start, d_start = box_start\n",
        "        h_end, w_end, d_end = box_end\n",
        "        predictions_org[h_start:h_end, w_start:w_end, d_start:d_end] = predictions\n",
        "        del predictions\n",
        "\n",
        "        filename = batchdata[\"image_meta_dict\"][\"filename_or_obj\"][0].split(\"/\")[-1]\n",
        "\n",
        "        print(\"save {} with shape: {}, mean values: {}\".format(filename, predictions_org.shape, predictions_org.mean()))\n",
        "        # write_nifti(\n",
        "        #     data=predictions_org,\n",
        "        #     file_name=os.path.join(self.output_dir, filename),\n",
        "        #     affine=affine,\n",
        "        #     resample=False,\n",
        "        #     output_dtype=np.uint8,\n",
        "        # )\n",
        "        print(self.output_dir)\n",
        "        nrrd.write(os.path.join(self.output_dir, filename),predictions_org, header=None)\n",
        "        # save_transform = SaveImage(output_dir='/home/monai_project/work_0228/dynunet_pipeline/runs_05_fold0_expr', output_postfix='saved', output_ext='.nrrd')\n",
        "        # save_transform(predictions_org)\n",
        "        engine.fire_event(IterationEvents.FORWARD_COMPLETED)\n",
        "        return {\"pred\": predictions_org}\n",
        "if __name__ == \"__main__\":\n",
        "    print('infere run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8aa03f56-1f50-4666-92d8-027e0755af41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aa03f56-1f50-4666-92d8-027e0755af41",
        "outputId": "cea7a8a0-01b5-418b-be90-0cc537c97d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run inferenrce\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "# from create_dataset import get_data\n",
        "# from create_network import get_network\n",
        "# from inferrer import DynUNetInferrer\n",
        "# from task_params import patch_size, task_name\n",
        "\n",
        "\n",
        "def inference(checkpoint_name = None, root_path = None):\n",
        "    # load hyper parameters\n",
        "    task_id = '08'\n",
        "    checkpoint = checkpoint_name\n",
        "    val_output_dir = \"./runs_{}_fold{}_{}/\".format(task_id, '0', 'expr')\n",
        "    sw_batch_size = 4\n",
        "    infer_output_dir = os.path.join(val_output_dir, task_name[task_id])\n",
        "    window_mode = 'gaussian'\n",
        "    eval_overlap = 0.5\n",
        "    amp = False\n",
        "    tta_val = False\n",
        "    multi_gpu_flag = False\n",
        "    local_rank = 0\n",
        "\n",
        "    if not os.path.exists(infer_output_dir):\n",
        "        os.makedirs(infer_output_dir)\n",
        "\n",
        "    if multi_gpu_flag:\n",
        "        dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
        "        device = torch.device(f\"cuda:{local_rank}\")\n",
        "        torch.cuda.set_device(device)\n",
        "    else:\n",
        "        device = torch.device(\"cuda\")\n",
        "\n",
        "    properties, test_loader = get_data(root_path, mode=\"validation\")\n",
        "\n",
        "    net = get_network(properties, task_id, val_output_dir, checkpoint)\n",
        "    net = net.to(device)\n",
        "\n",
        "    if multi_gpu_flag:\n",
        "        net = DistributedDataParallel(module=net, device_ids=[device], find_unused_parameters=True)\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    inferrer = DynUNetInferrer(\n",
        "        device=device,\n",
        "        val_data_loader=test_loader,\n",
        "        network=net,\n",
        "        output_dir=infer_output_dir,\n",
        "        num_classes=5,\n",
        "        inferer=SlidingWindowInferer(\n",
        "            roi_size=patch_size[task_id],\n",
        "            sw_batch_size=sw_batch_size,\n",
        "            overlap=eval_overlap,\n",
        "            mode=window_mode,\n",
        "        ),\n",
        "        amp=amp,\n",
        "        tta_val=tta_val,\n",
        "    )\n",
        "\n",
        "    inferrer.run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print('run inferenrce')\n",
        "    # inference('net_key_metric=0.1072.pt', '/root/autodl-tmp/dataset/')\n",
        "    # print('infere end')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6RHjUKEGvhiw",
      "metadata": {
        "id": "6RHjUKEGvhiw"
      },
      "source": [
        "The following code outlines methods for cleaning tabular data and employing machine learning techniques for data imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e68da23f-8a76-4cd9-9bbf-870af4fcf929",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e68da23f-8a76-4cd9-9bbf-870af4fcf929",
        "outputId": "70b8af9b-438b-483b-ae09-7cfd24aa511a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "csv_computer\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import difflib\n",
        "from scipy import stats\n",
        "import time\n",
        "import category_encoders as encoders\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler,PolynomialFeatures,OneHotEncoder,OrdinalEncoder,LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
        "from sklearn.metrics import explained_variance_score,mean_absolute_error,mean_squared_error,median_absolute_error,r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import os\n",
        "from lce import LCERegressor,LCEClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
        "from sklearn.svm import SVR,SVC,LinearSVC,LinearSVR\n",
        "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier,TabNetRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier,LGBMRegressor\n",
        "from xgboost import XGBClassifier,XGBRegressor\n",
        "from catboost import CatBoostRegressor,CatBoostClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier,GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score,roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import Levenshtein\n",
        "import matplotlib.pyplot as plot\n",
        "def csv_support_computer(input_csv_path = './drive/My Drive/HCC/HCC-TACE-Seg_clinical_data-V2.xlsx', save_csv_path='support_.csv', input_load_data_path=''):\n",
        "    print(\"package is loaded\")\n",
        "    regression_model={\"LR\":LinearRegression(),\n",
        "         \"KNN\":KNeighborsRegressor(),\n",
        "         \"SVR\":SVR(),\n",
        "         \"DT\":DecisionTreeRegressor(),\n",
        "         \"RF\":RandomForestRegressor(),\n",
        "         \"GPR\":GaussianProcessRegressor(kernel=1.0 * RBF(1.0)),\n",
        "         \"LGBM\":LGBMRegressor(),\n",
        "         \"XGB\":XGBRegressor(),\n",
        "         \"LCE\": LCERegressor(),\n",
        "         \"CatBoost\":CatBoostRegressor(verbose=False),\n",
        "         \"MLP\":MLPRegressor(),\n",
        "         \"TabNet\":TabNetRegressor()\n",
        "    }\n",
        "    classification_model={\"LR\":LogisticRegression(),\n",
        "         \"KNN\":KNeighborsClassifier(),\n",
        "         \"SVC\":SVC(),\n",
        "         \"DT\":DecisionTreeClassifier(),\n",
        "         \"RF\":RandomForestClassifier(),\n",
        "         \"GPC\":GaussianProcessClassifier(kernel=1.0 * RBF(1.0)),\n",
        "         \"LGBM\":LGBMClassifier(),\n",
        "         \"XGB\":XGBClassifier(),\n",
        "         \"LCE\": LCEClassifier(),\n",
        "         \"CatBoost\":CatBoostClassifier(verbose=False),\n",
        "         \"MLP\":MLPClassifier(),\n",
        "         \"TabNet\":TabNetClassifier(),\n",
        "         \"GNB\":GaussianNB(),\n",
        "         \"BNB\":BernoulliNB(),\n",
        "    }\n",
        "    encoders_model={}\n",
        "    def del_row(dataset,precentage_threshold=0.5):\n",
        "        data = dataset.copy()\n",
        "        rows_null = data.isnull().sum(axis=1)\n",
        "        threshold = precentage_threshold * cols\n",
        "        print(threshold)\n",
        "        del_rows = rows_null[rows_null >= threshold].index.tolist()\n",
        "        print(del_rows)  ###check the deleted row\n",
        "        data = data.drop(index=del_rows)\n",
        "        print(\"rows_null\",rows_null)\n",
        "        return data\n",
        "\n",
        "    def del_col(dataset,precentage_threshold=0.805):\n",
        "        data=dataset.copy()\n",
        "        col_null = data.isnull().sum(axis=0)\n",
        "        threshold = precentage_threshold * rows\n",
        "        del_cols = col_null[col_null > threshold].index.tolist()\n",
        "        print(data[del_cols].columns.values)  ###check the deleted cols\n",
        "        data = data.drop(columns=del_cols)\n",
        "        print(\"cols_null\", col_null)\n",
        "        return data\n",
        "\n",
        "    def check_diff_type(data):\n",
        "        names = data.columns.values\n",
        "        has_diff_type=False\n",
        "        for name in names:\n",
        "            ser=data[name].dropna()\n",
        "            ser = pd.to_numeric(ser, errors='coerce').fillna(ser)\n",
        "            # print(ser.unique())\n",
        "            ser=ser.to_frame()\n",
        "            #print(ser.applymap(type).nunique())\n",
        "            if ser.applymap(type).nunique()[name]>1:\n",
        "                has_diff_type=True\n",
        "                print(\"mixed_type_cols:\",name)\n",
        "        if has_diff_type==False:\n",
        "            print(\"has not mixed cols!\")\n",
        "\n",
        "    def NAcheck(data):\n",
        "        names = data.columns.values\n",
        "        rows = data.shape[0]\n",
        "        cols = data.shape[1]\n",
        "        print(\"shape\", data.shape, \"size\", data.size)\n",
        "        ###print shape and size for check\n",
        "        checknull = data.isnull().any(axis=0)\n",
        "        sts=data.isnull().sum()\n",
        "        plot.rcParams[\"figure.figsize\"] = (30, 8)\n",
        "        plot.bar(sts.index,sts.values)\n",
        "        plot.xlabel(\"Name\")\n",
        "        plot.ylabel(\"Missing Value\")\n",
        "        plot.title(\"Missing Stat\")\n",
        "        plot.xticks(rotation=90)\n",
        "        plot.tight_layout()\n",
        "        namelist = []  ###list of null col\n",
        "        namelist_full=[]\n",
        "        for name in names:\n",
        "            # print(name)\n",
        "            if checknull[name] == True:\n",
        "                # print(\"nhanes_data['\"+name+\"']\")\n",
        "                namelist.append(name)\n",
        "            else:\n",
        "                namelist_full.append(name)\n",
        "\n",
        "        print(\"missing_feature:\",len(namelist),namelist)\n",
        "        print(\"complete_feature:\", len(namelist_full), namelist_full)\n",
        "        return rows,cols,namelist,namelist_full\n",
        "    def auto_outliter(dataset):\n",
        "        df=dataset\n",
        "        print(df.describe())\n",
        "        for name in df.columns.values:\n",
        "            if df[name].dtype == \"float64\":\n",
        "                ###IQR\n",
        "                Q1 = df[name].quantile(0.25)\n",
        "                Q3 = df[name].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                outliers = df[(df[name] < (Q1 - 1.5 * IQR)) | (df[name] > (Q3 + 1.5 * IQR))]\n",
        "                print(name, \"outliter(IQR):\", outliers.shape)\n",
        "                ###Z-score\n",
        "                z_scores = stats.zscore(df[name])\n",
        "                abs_z_scores = abs(z_scores)\n",
        "                outliers = df[(abs_z_scores > 3)]\n",
        "                print(name,\"outliter(Z Score):\",outliers.shape)\n",
        "                ### visualisation\n",
        "                df.boxplot(column=name)\n",
        "                # plt.show()\n",
        "            elif df[name].dtype == \"object\":\n",
        "                stat = df[name].value_counts()\n",
        "                ###assert when number of a class less than 5\n",
        "                print(\"check the outiler\",stat[stat < 5].index)\n",
        "                ###check string similar\n",
        "                string_similar(stat.index)\n",
        "                ###\n",
        "                # visualisation\n",
        "                stat.plot(kind='bar')\n",
        "                # set title and label\n",
        "                plt.title(name+' Distribution')\n",
        "                plt.xlabel(name)\n",
        "                plt.ylabel('Count')\n",
        "                plt.tight_layout()\n",
        "                # plt.show()\n",
        "                #print(\"name\",df[name].value_counts())\n",
        "    def ML_Imputer(dataset,target=\"edu\",type=\"classification\",method=\"CatBoost\"):\n",
        "        print(\"target:\",target)\n",
        "        data=dataset.copy()\n",
        "        # X=data[data.columns[data.notnull().all(axis=0)]]\n",
        "        # y=data[data.columns[data.isnull().any(axis=0)]]\n",
        "        # print(\"Shape\",X.shape,y.shape)\n",
        "        # y=y[target]\n",
        "\n",
        "        ###pre-interpolation\n",
        "        y = data[target]\n",
        "        X=data.drop(columns=target)\n",
        "        for name in X.columns.values:\n",
        "            if X[name].dtype == \"object\":\n",
        "                X[name]=X[name].fillna(\"UNKNOWN\")\n",
        "            else:\n",
        "                X[name] = X[name].fillna(data[name].median())\n",
        "        ###\n",
        "        ###one hot encode\n",
        "        X=pd.get_dummies(X)\n",
        "\n",
        "        ###Split the dataset based on the presence or absence of null values\n",
        "        X_full=X[y.notnull()]\n",
        "        X_null=X[y.isnull()]\n",
        "        y_full=y[y.notnull()]\n",
        "        y_null=y[y.isnull()]\n",
        "        #print(X_full,X_null,y_full,y_null)\n",
        "        ###one hot encode\n",
        "        X_full=pd.get_dummies(X_full)\n",
        "        ###choose task\n",
        "        # print(type)\n",
        "        if type==\"classification\":\n",
        "            clf=classification_model[method]\n",
        "            ###define a encoder\n",
        "            encoder = LabelEncoder()  # 忽略未知的类别\n",
        "            ###encoder for target\n",
        "            y_full = encoder.fit_transform(y_full)\n",
        "\n",
        "            ###split dataset\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)\n",
        "            X_train=np.array(X_train)\n",
        "            X_test=np.array(X_test)\n",
        "            y_train=np.array(y_train)\n",
        "            y_test=np.array(y_test)\n",
        "            ###pretrain\n",
        "            clf.fit(X_train,y_train)\n",
        "            ###test model\n",
        "            y_pred_encoded = clf.predict(X_test)\n",
        "            # inv encode\n",
        "            y_pred_encoded = encoder.inverse_transform(y_pred_encoded)\n",
        "            y_test = encoder.inverse_transform(y_test)\n",
        "            #print(\"compare:\",y_test,y_pred_encoded)\n",
        "            ###calculate the acc\n",
        "            accuracy = accuracy_score(y_test, y_pred_encoded)\n",
        "            print(\"Accuracy:\", accuracy)\n",
        "            ###choose the method of fill\n",
        "            threshold=0.49\n",
        "            if accuracy>=threshold:\n",
        "                    model=classification_model[method]\n",
        "                    model.fit(X_full,y_full)\n",
        "                    y_output=model.predict(X_null)\n",
        "                    y_output = encoder.inverse_transform(y_output)\n",
        "                    #print(\"output:\",y_output)\n",
        "                    data.loc[data[target].isnull(), target] = y_output\n",
        "                    data[target].to_csv(target+\"_check.csv\")\n",
        "            else:\n",
        "                    print(\"Accuracy is\",accuracy,\"that not enough\")\n",
        "                    data[target]=data[target].fillna(\"Unknown\")\n",
        "                    data[target].to_csv(target+\"_check.csv\")\n",
        "\n",
        "        else:\n",
        "            ###initial regression model\n",
        "            rg=regression_model[method]\n",
        "            ###split dataset\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2)\n",
        "            X_train = np.array(X_train)\n",
        "            X_test = np.array(X_test)\n",
        "            y_train = np.array(y_train)\n",
        "            y_test = np.array(y_test)\n",
        "            ###Normalise or standardise\n",
        "            scaler_X = StandardScaler()\n",
        "            # X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "            # X_test_scaled = scaler_X.transform(X_test)\n",
        "            ###No normalise or standardise\n",
        "            X_train_scaled=X_train\n",
        "            X_test_scaled=X_test\n",
        "            ###MinMax scaler for target\n",
        "            scaler_y = MinMaxScaler()\n",
        "            ###\n",
        "            # y_train_scaled=scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "            ###\n",
        "            ###disable MinMax scaler for target\n",
        "            y_train_scaled=y_train.reshape(-1,1)\n",
        "            ###training the regression model\n",
        "            rg.fit(X_train_scaled,y_train_scaled)\n",
        "            ###predict\n",
        "            y_pred = rg.predict(X_test_scaled)\n",
        "            ###inv encoder\n",
        "            # y_pred=scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
        "            #print(\"compare:\",y_test, y_pred)\n",
        "\n",
        "            # calculate R2 and MSE for regression model\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            print(\"Mean Squared Error:\", mse)\n",
        "            print(\"R2 Score:\", r2)\n",
        "            # calculate R2 and MSE for useing mean\n",
        "            y_pred_alter_mean=[y_train.mean()]*y_test.shape[0]\n",
        "            y_pred_alter_mean=np.array(y_pred_alter_mean)\n",
        "            mse_mean = mean_squared_error(y_test, y_pred_alter_mean)\n",
        "            r2 = r2_score(y_test, y_pred_alter_mean)\n",
        "            print(\"Mean Squared Error:\", mse_mean)\n",
        "            print(\"R2 Score:\", r2)\n",
        "            ###calculate R2 and MSE for useing median\n",
        "            y_pred_alter_median = [np.median(y_train)] * y_test.shape[0]\n",
        "            y_pred_alter_median = np.array(y_pred_alter_median)\n",
        "            mse_median = mean_squared_error(y_test, y_pred_alter_median)\n",
        "            r2 = r2_score(y_test, y_pred_alter_median)\n",
        "            print(\"Mean Squared Error:\", mse_median)\n",
        "            print(\"R2 Score:\", r2)\n",
        "            if (mse<mse_mean) and (mse<mse_median):\n",
        "                model=regression_model[method]\n",
        "                scaler_X_full = StandardScaler()\n",
        "                # X_full = scaler_X.fit_transform(X_full)\n",
        "                # X_null = scaler_X.transform(X_null)\n",
        "                scaler_y_full=MinMaxScaler()\n",
        "                #y_full = scaler_y.fit_transform(y_full.reshape(-1, 1))\n",
        "                ###re train the model\n",
        "                model.fit(X_full,y_full)\n",
        "                y_output=model.predict(X_null)\n",
        "                ###inv encoder\n",
        "                # y_output=scaler_y_full.inverse_transform(y_output.reshape(-1, 1))\n",
        "                #print(\"output:\",y_output)\n",
        "                data.loc[data[target].isnull(), target] = y_output\n",
        "                data[target].to_csv(target+\"_check.csv\")\n",
        "\n",
        "            else:\n",
        "                ###using median\n",
        "                print(\"Mse is\", mse, \"that not enough\")\n",
        "                print(data[target].median())\n",
        "                data[target] = data[target].fillna(data[target].median())\n",
        "                data[target].to_csv(target + \"_check.csv\")\n",
        "        return data\n",
        "\n",
        "    def auto_fill(dataset):\n",
        "        data=dataset.copy()\n",
        "        data_with_null_cols=data[data.columns[data.isnull().any(axis=0)]]\n",
        "        missing_value=data_with_null_cols.isnull().sum()\n",
        "        missing_value = missing_value.sort_values()\n",
        "        print(missing_value,data_with_null_cols.dtypes)\n",
        "        name_list_after_sort=missing_value.index\n",
        "        for name in name_list_after_sort:\n",
        "            if data_with_null_cols[name].dtype==\"object\":\n",
        "                print(data_with_null_cols[name].dtype)\n",
        "                data=ML_Imputer(dataset=data,target=name,type=\"classification\")\n",
        "            elif data_with_null_cols[name].dtype==\"float64\":\n",
        "                data=ML_Imputer(dataset=data,target=name,type=\"regression\")\n",
        "            elif data_with_null_cols[name].dtype==\"int64\":\n",
        "                print(name,\"can not handle\")\n",
        "            else:\n",
        "                # data=ML_Imputer(dataset=data,target=name,type=\"regression\")\n",
        "                # data[name] = data[name].round(0)\n",
        "                print(name,\"can not handle\",data_with_null_cols[name].dtype)\n",
        "                # print(name,\"Warning\")\n",
        "        return data\n",
        "    data = pd.read_excel(input_csv_path,index_col=0)\n",
        "    data = data.replace(to_replace=\"Cisplastin\",value=\"Cisplatin\")\n",
        "    # data = data.replace(to_replace=\"No biopsy\",value=np.nan)\n",
        "    # data = data.replace(to_replace=\"NOT STATED\",value=np.nan)\n",
        "    # Define a function to encode hepatitis\n",
        "    def encode_hepatitis(row):\n",
        "        if 'HCV' in row and 'HBV' in row:  # Assuming presence of both is indicated somehow, e.g., \"HCV and HBV\"\n",
        "            return pd.Series([1, 1])\n",
        "        elif 'HCV' in row:\n",
        "            return pd.Series([1, 0])\n",
        "        elif 'HBV' in row:\n",
        "            return pd.Series([0, 1])\n",
        "        else:\n",
        "            return pd.Series([0, 0])\n",
        "\n",
        "    # Adjust the encode_chemotherapy function to handle non-string values\n",
        "    def encode_chemotherapy(row):\n",
        "        if pd.isnull(row) or not isinstance(row, str):\n",
        "            return pd.Series([0, 0, 0])\n",
        "        cisplatin = 1 if 'Cisplatin' in row else 0\n",
        "        doxorubicin = 1 if 'doxorubicin' in row else 0\n",
        "        mitomycin_c = 1 if 'Mitomycin-C' in row else 0\n",
        "        return pd.Series([cisplatin, doxorubicin, mitomycin_c])\n",
        "\n",
        "    # Apply encoding for hepatitis\n",
        "    data[['HCV', 'HBV']] = data['hepatitis'].apply(encode_hepatitis)\n",
        "\n",
        "    # Re-apply encoding for chemotherapy with the adjusted function\n",
        "    data[['Cisplatin', 'Doxorubicin', 'Mitomycin-C']] = data['chemotherapy'].apply(encode_chemotherapy)\n",
        "\n",
        "    # Display the first few rows to verify the encoding\n",
        "    print(data[['hepatitis', 'HCV', 'HBV', 'chemotherapy', 'Cisplatin', 'Doxorubicin', 'Mitomycin-C']].head())\n",
        "    data.drop(['hepatitis','chemotherapy'], axis=1, inplace=True)\n",
        "    data.to_csv(\"data_encode.csv\")\n",
        "\n",
        "    data = pd.read_csv('data_encode.csv',index_col=0)\n",
        "    string_values_and_codes={}\n",
        "    # string_values_and_codes[\"hepatitis\"]={'HBV only': 1, 'HCV and HBV': 3, 'HCV only': 2, 'No virus': 0}\n",
        "    # string_values_and_codes[\"chemotherapy\"]= {'Cisplastin': 1,\n",
        "    #                       'Cisplatin, doxorubicin, Mitomycin-C': 4,\n",
        "    #                       'Cisplatin, Mitomycin-C': 3,\n",
        "    #                       'doxorubicin LC beads': 2,\n",
        "    #                       \"NA\":np.nan }\n",
        "    string_values_and_codes[\"Pathology\"]= {'Moderately differentiated': 3.0,\n",
        "                        'Moderately-poorly differentiated': 2.0,\n",
        "                        'No biopsy': np.nan,\n",
        "                        'NOT STATED': np.nan,\n",
        "                        'Poorly differentiated': 1.0,\n",
        "                        'Well differentiated': 5.0,\n",
        "                        'Well-moderately differentiated':4.0 }\n",
        "    string_values_and_codes[\"agegp\"]= {'41-50': 1, '51-60': 2, '61-70': 3, '<=40': 0, '>70': 4}\n",
        "    string_values_and_codes[\"CPS\"]= {'A': 1, 'B': 2, 'C': 3}\n",
        "    string_values_and_codes[\"tumor_nodul\"]= {'multinodular': 1, 'uninodular': 0}\n",
        "    string_values_and_codes[\"T_involvment\"]= {'< or = 50%': 1, '>50%': 2}\n",
        "    string_values_and_codes[\"AFP_group\"]= {'<400': 0, '>=400': 1}\n",
        "    string_values_and_codes[\"CLIP_Score\"]= {'Stage_0': 1, 'Stage_1': 2, 'Stage_2': 3, 'Stage_3': 4, 'Stage_4': 5, 'Stage_5': 6}\n",
        "    string_values_and_codes[\"CLIP\"]= {'Stage_ 0-2': 1, 'Stage_3': 2, 'Stage_4-6': 3}\n",
        "    string_values_and_codes[\"Okuda\"]= {'Stage I': 1, 'Stage II': 2, 'Stage III': 3}\n",
        "    string_values_and_codes[\"TNM\"]= {'Stage-I': 1, 'Stage-II': 2, 'Stage-IIIA': 3, 'Stage-IIIB': 4, 'Stage-IIIC': 5, 'Stage-IVA': 6, 'Stage-IVB': 7}\n",
        "    string_values_and_codes[\"BCLC\"]= {'Stage-A': 1, 'Stage-B': 2, 'Stage-C': 3, 'Stage-D': 4}\n",
        "\n",
        "    # 使用之前生成的编码字典转换DataFrame中的字符串列为对应的编码\n",
        "    for col, codes in string_values_and_codes.items():\n",
        "        # 如果某个值在字典中找不到对应的编码，则返回NaN\n",
        "        data[col] = data[col].map(codes).astype(pd.Int64Dtype())\n",
        "\n",
        "\n",
        "\n",
        "    rows,cols,namelist,namelist_full=NAcheck(data)\n",
        "    print(rows,cols,namelist,namelist_full)\n",
        "    data['Pathology'] = pd.to_numeric(data['Pathology'], errors='coerce').astype('float64')\n",
        "    # data['Pathology']=data['Pathology'].astype('float64')\n",
        "    # print(data['Pathology'].dtypes)\n",
        "    data.to_csv(\"data_cleaned.csv\")\n",
        "\n",
        "\n",
        "    data=auto_fill(data)\n",
        "    data.to_csv(save_csv_path)\n",
        "    data.to_csv(input_load_data_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('csv_computer')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "znAYGrW4vIYX",
      "metadata": {
        "id": "znAYGrW4vIYX"
      },
      "source": [
        "The complete training and inference process. To run on Colab, select an A100 GPU and high RAM runtime. and you should adjust file paths (data_path) according to the test environment. The default epoch is 1000(max_epochs_value=1000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a46329d9-db6f-4759-95c6-dbcf79641171",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a46329d9-db6f-4759-95c6-dbcf79641171",
        "outputId": "93a90785-d627-4071-d7bb-4363bd3d197e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "package is loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        hepatitis  HCV  HBV                         chemotherapy  Cisplatin  \\\n",
            "TCIA_ID                                                                       \n",
            "HCC_001  HCV only    1    0  Cisplatin, doxorubicin, Mitomycin-C          1   \n",
            "HCC_002  HBV only    0    1  Cisplatin, doxorubicin, Mitomycin-C          1   \n",
            "HCC_003  HCV only    1    0                                  NaN          0   \n",
            "HCC_004  No virus    0    0                                  NaN          0   \n",
            "HCC_005  No virus    0    0                                  NaN          0   \n",
            "\n",
            "         Doxorubicin  Mitomycin-C  \n",
            "TCIA_ID                            \n",
            "HCC_001            1            1  \n",
            "HCC_002            1            1  \n",
            "HCC_003            0            0  \n",
            "HCC_004            0            0  \n",
            "HCC_005            0            0  \n",
            "shape (105, 59) size 6195\n",
            "missing_feature: 24 ['Interval_FU', 'Pathology', 'Tr_Size', '1_EASL_BL', '1_EASL_FU', '1_EASL', '1_RECIST_BL', '1_RECIST_FU', '1_RECIST', '2_RECIST_BL', '2_RECIST_FU', '2_RECIST', '3_RECIST_BL', '3_RECIST_FU', '3_RECIST', '1_mRECIST_BL', '1_mRECIST_FU', '1_mRECIST', '2_mRECIST_BL', '2_mRECIST_FU', '2_mRECIST', '3_mRECIST_BL', '3_mRECIST_FU', '3_mRECIST']\n",
            "complete_feature: 35 ['Interval_BL', 'TTP', 'Death_1_StillAliveorLostToFU_0', 'Censored_0_progressed_1', 'OS', 'age', 'agegp', 'Sex', 'Smoking', 'Alcohol', 'fhx_can', 'fhx_livc', 'Diabetes', 'Personal history of cancer', 'Evidence_of_cirh', 'PS_bclc_0_0_1-2_1_3-4_3', 'CPS', 'tumor_nodul', 'Vascular invasion', 'Metastasis', 'Lymphnodes', 'Portal Vein Thrombosis', 'T_involvment', 'AFP', 'AFP_group', 'CLIP_Score', 'CLIP', 'Okuda', 'TNM', 'BCLC', 'HCV', 'HBV', 'Cisplatin', 'Doxorubicin', 'Mitomycin-C']\n",
            "105 59 ['Interval_FU', 'Pathology', 'Tr_Size', '1_EASL_BL', '1_EASL_FU', '1_EASL', '1_RECIST_BL', '1_RECIST_FU', '1_RECIST', '2_RECIST_BL', '2_RECIST_FU', '2_RECIST', '3_RECIST_BL', '3_RECIST_FU', '3_RECIST', '1_mRECIST_BL', '1_mRECIST_FU', '1_mRECIST', '2_mRECIST_BL', '2_mRECIST_FU', '2_mRECIST', '3_mRECIST_BL', '3_mRECIST_FU', '3_mRECIST'] ['Interval_BL', 'TTP', 'Death_1_StillAliveorLostToFU_0', 'Censored_0_progressed_1', 'OS', 'age', 'agegp', 'Sex', 'Smoking', 'Alcohol', 'fhx_can', 'fhx_livc', 'Diabetes', 'Personal history of cancer', 'Evidence_of_cirh', 'PS_bclc_0_0_1-2_1_3-4_3', 'CPS', 'tumor_nodul', 'Vascular invasion', 'Metastasis', 'Lymphnodes', 'Portal Vein Thrombosis', 'T_involvment', 'AFP', 'AFP_group', 'CLIP_Score', 'CLIP', 'Okuda', 'TNM', 'BCLC', 'HCV', 'HBV', 'Cisplatin', 'Doxorubicin', 'Mitomycin-C']\n",
            "Interval_FU      8\n",
            "2_mRECIST        9\n",
            "2_mRECIST_FU     9\n",
            "2_mRECIST_BL     9\n",
            "1_mRECIST        9\n",
            "1_mRECIST_FU     9\n",
            "1_mRECIST_BL     9\n",
            "2_RECIST_FU      9\n",
            "2_RECIST_BL      9\n",
            "2_RECIST         9\n",
            "1_RECIST_FU      9\n",
            "1_RECIST_BL      9\n",
            "1_EASL           9\n",
            "1_EASL_FU        9\n",
            "1_EASL_BL        9\n",
            "1_RECIST         9\n",
            "3_mRECIST_FU    10\n",
            "3_RECIST_BL     10\n",
            "3_RECIST_FU     10\n",
            "3_RECIST        10\n",
            "3_mRECIST_BL    10\n",
            "3_mRECIST       10\n",
            "Pathology       22\n",
            "Tr_Size         52\n",
            "dtype: int64 Interval_FU     float64\n",
            "Pathology       float64\n",
            "Tr_Size         float64\n",
            "1_EASL_BL       float64\n",
            "1_EASL_FU       float64\n",
            "1_EASL          float64\n",
            "1_RECIST_BL     float64\n",
            "1_RECIST_FU     float64\n",
            "1_RECIST        float64\n",
            "2_RECIST_BL     float64\n",
            "2_RECIST_FU     float64\n",
            "2_RECIST        float64\n",
            "3_RECIST_BL     float64\n",
            "3_RECIST_FU     float64\n",
            "3_RECIST        float64\n",
            "1_mRECIST_BL    float64\n",
            "1_mRECIST_FU    float64\n",
            "1_mRECIST       float64\n",
            "2_mRECIST_BL    float64\n",
            "2_mRECIST_FU    float64\n",
            "2_mRECIST       float64\n",
            "3_mRECIST_BL    float64\n",
            "3_mRECIST_FU    float64\n",
            "3_mRECIST       float64\n",
            "dtype: object\n",
            "target: Interval_FU\n",
            "Mean Squared Error: 97.11906376457716\n",
            "R2 Score: -0.31176967464228866\n",
            "Mean Squared Error: 84.05003560634171\n",
            "R2 Score: -0.1352486688737764\n",
            "Mean Squared Error: 77.32727\n",
            "R2 Score: -0.0444454865706152\n",
            "Mse is 97.11906376457716 that not enough\n",
            "9.0\n",
            "target: 2_mRECIST\n",
            "Mean Squared Error: 0.2633346899142464\n",
            "R2 Score: 0.6380279176436476\n",
            "Mean Squared Error: 0.7369806094182826\n",
            "R2 Score: -0.013031765523412453\n",
            "Mean Squared Error: 0.75\n",
            "R2 Score: -0.030927835051546282\n",
            "target: 2_mRECIST_FU\n",
            "Mean Squared Error: 223.47938491159994\n",
            "R2 Score: 0.8431225459933067\n",
            "Mean Squared Error: 1425.7344529085872\n",
            "R2 Score: -0.0008332139915216974\n",
            "Mean Squared Error: 1651.05\n",
            "R2 Score: -0.158999612157545\n",
            "target: 2_mRECIST_BL\n",
            "Mean Squared Error: 1631.6409773242065\n",
            "R2 Score: 0.6084562830379616\n",
            "Mean Squared Error: 4168.122610803324\n",
            "R2 Score: -0.00022139825382150313\n",
            "Mean Squared Error: 4248.2\n",
            "R2 Score: -0.01943751199846422\n",
            "target: 1_mRECIST\n",
            "Mean Squared Error: 0.304078331629862\n",
            "R2 Score: 0.6658479872199319\n",
            "Mean Squared Error: 0.9234072022160664\n",
            "R2 Score: -0.014733189248424372\n",
            "Mean Squared Error: 1.0\n",
            "R2 Score: -0.09890109890109877\n",
            "target: 1_mRECIST_FU\n",
            "Mean Squared Error: 1109.9048848488296\n",
            "R2 Score: 0.6468027018634823\n",
            "Mean Squared Error: 3349.2723438365656\n",
            "R2 Score: -0.06581560160225486\n",
            "Mean Squared Error: 3572.1829999999995\n",
            "R2 Score: -0.13675090656173028\n",
            "target: 1_mRECIST_BL\n",
            "Mean Squared Error: 192.5576019102353\n",
            "R2 Score: 0.7957549662197723\n",
            "Mean Squared Error: 1189.9950941828254\n",
            "R2 Score: -0.26222276242814657\n",
            "Mean Squared Error: 971.507\n",
            "R2 Score: -0.030473365186734602\n",
            "target: 2_RECIST_FU\n",
            "Mean Squared Error: 133.84279326760446\n",
            "R2 Score: 0.9385813321397978\n",
            "Mean Squared Error: 2227.2707756232694\n",
            "R2 Score: -0.02206477213331537\n",
            "Mean Squared Error: 2224.75\n",
            "R2 Score: -0.020908021911836494\n",
            "target: 2_RECIST_BL\n",
            "Mean Squared Error: 352.53319189101416\n",
            "R2 Score: 0.8763009964670981\n",
            "Mean Squared Error: 2892.4173130193913\n",
            "R2 Score: -0.014909085588805882\n",
            "Mean Squared Error: 2861.15\n",
            "R2 Score: -0.003937819470846371\n",
            "target: 2_RECIST\n",
            "Mean Squared Error: 0.065567098909153\n",
            "R2 Score: 0.27147667878718884\n",
            "Mean Squared Error: 0.09542936288088642\n",
            "R2 Score: -0.0603262542320715\n",
            "Mean Squared Error: 0.1\n",
            "R2 Score: -0.11111111111111116\n",
            "target: 1_RECIST_FU\n",
            "Mean Squared Error: 830.673980188051\n",
            "R2 Score: 0.6824460364269732\n",
            "Mean Squared Error: 2912.434181440443\n",
            "R2 Score: -0.113379063290997\n",
            "Mean Squared Error: 3700.8949999999995\n",
            "R2 Score: -0.4147955805134802\n",
            "target: 1_RECIST_BL\n",
            "Mean Squared Error: 1267.7698947087952\n",
            "R2 Score: 0.5779901720076095\n",
            "Mean Squared Error: 3394.6649044321334\n",
            "R2 Score: -0.13000155500637733\n",
            "Mean Squared Error: 3792.049\n",
            "R2 Score: -0.2622810755387901\n",
            "target: 1_EASL\n",
            "Mean Squared Error: 0.0006560145699322022\n",
            "R2 Score: 0.9948547876868062\n",
            "Mean Squared Error: 0.13504155124653744\n",
            "R2 Score: -0.05914942154147007\n",
            "Mean Squared Error: 0.15\n",
            "R2 Score: -0.17647058823529416\n",
            "target: 1_EASL_FU\n",
            "Mean Squared Error: 1221053.2556382366\n",
            "R2 Score: -0.444878199721253\n",
            "Mean Squared Error: 3714248.5644741827\n",
            "R2 Score: -3.395088219432001\n",
            "Mean Squared Error: 846924.16862\n",
            "R2 Score: -0.0021694487161931786\n",
            "Mse is 1221053.2556382366 that not enough\n",
            "842.925\n",
            "target: 1_EASL_BL\n",
            "Mean Squared Error: 7389659.155359236\n",
            "R2 Score: 0.8015523832243955\n",
            "Mean Squared Error: 37238161.04460707\n",
            "R2 Score: -2.234975196024891e-05\n",
            "Mean Squared Error: 41165377.306375\n",
            "R2 Score: -0.10548684971405042\n",
            "target: 1_RECIST\n",
            "Mean Squared Error: 0.14541756944788578\n",
            "R2 Score: 0.2729121527605711\n",
            "Mean Squared Error: 0.2001731301939058\n",
            "R2 Score: -0.0008656509695290637\n",
            "Mean Squared Error: 0.2\n",
            "R2 Score: 0.0\n",
            "target: 3_mRECIST_FU\n",
            "Mean Squared Error: 296.93517603797164\n",
            "R2 Score: 0.8418067104499021\n",
            "Mean Squared Error: 1900.8059020083103\n",
            "R2 Score: -0.012661222719140719\n",
            "Mean Squared Error: 2130.3526315789477\n",
            "R2 Score: -0.13495307355597874\n",
            "target: 3_RECIST_BL\n",
            "Mean Squared Error: 156.2596768451555\n",
            "R2 Score: 0.8192230415779886\n",
            "Mean Squared Error: 1544.0094615650967\n",
            "R2 Score: -0.7862659124345883\n",
            "Mean Squared Error: 1134.8942105263154\n",
            "R2 Score: -0.31296011646691024\n",
            "target: 3_RECIST_FU\n",
            "Mean Squared Error: 333.77739623665565\n",
            "R2 Score: 0.7807229702475179\n",
            "Mean Squared Error: 1592.1145983379504\n",
            "R2 Score: -0.04594907889323352\n",
            "Mean Squared Error: 1682.7277631578947\n",
            "R2 Score: -0.1054779321415873\n",
            "target: 3_RECIST\n",
            "Mean Squared Error: 0.10559618809780083\n",
            "R2 Score: 0.4705524457874153\n",
            "Mean Squared Error: 0.20792936288088645\n",
            "R2 Score: -0.04253472222222232\n",
            "Mean Squared Error: 0.21052631578947367\n",
            "R2 Score: -0.05555555555555558\n",
            "target: 3_mRECIST_BL\n",
            "Mean Squared Error: 208.26215236552503\n",
            "R2 Score: 0.8489342036814023\n",
            "Mean Squared Error: 1401.1938382963986\n",
            "R2 Score: -0.016375085797861955\n",
            "Mean Squared Error: 1444.0584210526315\n",
            "R2 Score: -0.04746749627372071\n",
            "target: 3_mRECIST\n",
            "Mean Squared Error: 0.2900095945855555\n",
            "R2 Score: 0.5284078214171823\n",
            "Mean Squared Error: 0.6151315789473684\n",
            "R2 Score: -0.00028153153153120947\n",
            "Mean Squared Error: 0.6842105263157895\n",
            "R2 Score: -0.11261261261261235\n",
            "target: Pathology\n",
            "Mean Squared Error: 2.2224898727994353\n",
            "R2 Score: -1.4329529289357454\n",
            "Mean Squared Error: 1.723815697077729\n",
            "R2 Score: -0.8870558199070591\n",
            "Mean Squared Error: 2.588235294117647\n",
            "R2 Score: -1.833333333333333\n",
            "Mse is 2.2224898727994353 that not enough\n",
            "3.0\n",
            "target: Tr_Size\n",
            "Mean Squared Error: 10.15244886922141\n",
            "R2 Score: 0.4073836360420129\n",
            "Mean Squared Error: 17.184809317666463\n",
            "R2 Score: -0.003107658490266063\n",
            "Mean Squared Error: 17.429090909090913\n",
            "R2 Score: -0.017366806244331956\n",
            "排除问题数据: HCC_009/103 LIVER 3 PHASE (C-A-P).nrrd\n",
            "83 22\n",
            "0 \n",
            "1 \n",
            "2 \n",
            "3 \n",
            "4 \n",
            "5 \n",
            "6 \n",
            "7 \n",
            "8 \n",
            "9 \n",
            "10 \n",
            "11 \n",
            "12 \n",
            "13 \n",
            "14 \n",
            "15 \n",
            "16 \n",
            "17 \n",
            "18 \n",
            "19 \n",
            "20 \n",
            "21 \n",
            "22 \n",
            "23 \n",
            "24 \n",
            "25 \n",
            "26 \n",
            "27 \n",
            "28 \n",
            "29 \n",
            "30 \n",
            "31 \n",
            "502 002\n",
            "32 \n",
            "33 \n",
            "34 \n",
            "35 \n",
            "36 \n",
            "37 \n",
            "38 \n",
            "39 \n",
            "40 \n",
            "600 100\n",
            "41 \n",
            "42 \n",
            "43 \n",
            "44 \n",
            "45 \n",
            "46 \n",
            "47 \n",
            "48 \n",
            "49 \n",
            "50 \n",
            "51 \n",
            "52 \n",
            "53 \n",
            "54 \n",
            "55 \n",
            "56 \n",
            "57 \n",
            "58 \n",
            "59 \n",
            "60 \n",
            "61 \n",
            "62 \n",
            "63 \n",
            "64 \n",
            "65 \n",
            "66 \n",
            "67 \n",
            "68 \n",
            "69 \n",
            "70 \n",
            "71 \n",
            "72 \n",
            "507 007\n",
            "73 \n",
            "74 \n",
            "75 \n",
            "604 104\n",
            "76 \n",
            "77 \n",
            "78 \n",
            "503 003\n",
            "79 \n",
            "80 \n",
            "596 096\n",
            "81 \n",
            "82 \n",
            "end\n",
            "MONAI version: 1.3.0\n",
            "Numpy version: 1.23.3\n",
            "Pytorch version: 2.1.0+cu121\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
            "MONAI __file__: /usr/local/lib/python3.10/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: 0.4.13\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 4.0.2\n",
            "scikit-image version: 0.19.3\n",
            "scipy version: 1.11.4\n",
            "Pillow version: 9.4.0\n",
            "Tensorboard version: 2.15.2\n",
            "gdown version: 4.7.3\n",
            "TorchVision version: 0.16.0+cu121\n",
            "tqdm version: 4.66.2\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.5\n",
            "pandas version: 1.5.0\n",
            "einops version: 0.7.0\n",
            "transformers version: 4.38.1\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: 1.0.0\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n",
            "run SwinNext cross\n",
            "new run epoch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-05 17:28:21,564 - INFO - Epoch: 1/2, Iter: 1/41 -- train_loss: 2.2383 \n",
            "2024-03-05 17:28:37,880 - INFO - Epoch: 1/2, Iter: 2/41 -- train_loss: 1.9757 \n",
            "2024-03-05 17:29:00,977 - INFO - Epoch: 1/2, Iter: 3/41 -- train_loss: 2.0166 \n",
            "2024-03-05 17:29:15,816 - INFO - Epoch: 1/2, Iter: 4/41 -- train_loss: 1.8588 \n",
            "2024-03-05 17:29:34,773 - INFO - Epoch: 1/2, Iter: 5/41 -- train_loss: 1.8385 \n",
            "2024-03-05 17:29:44,827 - INFO - Epoch: 1/2, Iter: 6/41 -- train_loss: 1.4815 \n",
            "2024-03-05 17:30:01,919 - INFO - Epoch: 1/2, Iter: 7/41 -- train_loss: 1.4118 \n",
            "2024-03-05 17:30:15,376 - INFO - Epoch: 1/2, Iter: 8/41 -- train_loss: 1.3669 \n",
            "2024-03-05 17:30:33,177 - INFO - Epoch: 1/2, Iter: 9/41 -- train_loss: 1.4292 \n",
            "2024-03-05 17:30:58,813 - INFO - Epoch: 1/2, Iter: 10/41 -- train_loss: 1.2171 \n",
            "2024-03-05 17:31:19,505 - INFO - Epoch: 1/2, Iter: 11/41 -- train_loss: 1.3683 \n",
            "2024-03-05 17:31:40,996 - INFO - Epoch: 1/2, Iter: 12/41 -- train_loss: 1.1542 \n",
            "2024-03-05 17:32:03,724 - INFO - Epoch: 1/2, Iter: 13/41 -- train_loss: 1.1785 \n",
            "2024-03-05 17:32:25,128 - INFO - Epoch: 1/2, Iter: 14/41 -- train_loss: 1.1950 \n",
            "2024-03-05 17:32:39,967 - INFO - Epoch: 1/2, Iter: 15/41 -- train_loss: 1.1675 \n",
            "2024-03-05 17:32:59,706 - INFO - Epoch: 1/2, Iter: 16/41 -- train_loss: 1.2890 \n",
            "2024-03-05 17:33:17,196 - INFO - Epoch: 1/2, Iter: 17/41 -- train_loss: 1.0336 \n",
            "2024-03-05 17:33:41,299 - INFO - Epoch: 1/2, Iter: 18/41 -- train_loss: 1.1429 \n",
            "2024-03-05 17:33:55,780 - INFO - Epoch: 1/2, Iter: 19/41 -- train_loss: 1.1683 \n",
            "2024-03-05 17:34:09,995 - INFO - Epoch: 1/2, Iter: 20/41 -- train_loss: 1.6460 \n",
            "2024-03-05 17:34:24,260 - INFO - Epoch: 1/2, Iter: 21/41 -- train_loss: 1.7913 \n",
            "2024-03-05 17:34:45,198 - INFO - Epoch: 1/2, Iter: 22/41 -- train_loss: 1.0274 \n",
            "2024-03-05 17:35:04,719 - INFO - Epoch: 1/2, Iter: 23/41 -- train_loss: 1.3547 \n",
            "2024-03-05 17:35:23,538 - INFO - Epoch: 1/2, Iter: 24/41 -- train_loss: 1.5923 \n",
            "2024-03-05 17:35:39,397 - INFO - Epoch: 1/2, Iter: 25/41 -- train_loss: 1.2602 \n",
            "2024-03-05 17:36:00,187 - INFO - Epoch: 1/2, Iter: 26/41 -- train_loss: 1.7106 \n",
            "2024-03-05 17:36:15,497 - INFO - Epoch: 1/2, Iter: 27/41 -- train_loss: 1.1059 \n",
            "2024-03-05 17:36:37,781 - INFO - Epoch: 1/2, Iter: 28/41 -- train_loss: 1.6074 \n",
            "2024-03-05 17:36:52,093 - INFO - Epoch: 1/2, Iter: 29/41 -- train_loss: 1.0817 \n",
            "2024-03-05 17:37:07,442 - INFO - Epoch: 1/2, Iter: 30/41 -- train_loss: 1.3224 \n",
            "2024-03-05 17:37:22,372 - INFO - Epoch: 1/2, Iter: 31/41 -- train_loss: 1.0474 \n",
            "2024-03-05 17:37:38,140 - INFO - Epoch: 1/2, Iter: 32/41 -- train_loss: 0.9818 \n",
            "2024-03-05 17:38:00,225 - INFO - Epoch: 1/2, Iter: 33/41 -- train_loss: 1.6248 \n",
            "2024-03-05 17:38:20,961 - INFO - Epoch: 1/2, Iter: 34/41 -- train_loss: 1.8831 \n",
            "2024-03-05 17:38:44,714 - INFO - Epoch: 1/2, Iter: 35/41 -- train_loss: 1.3837 \n",
            "2024-03-05 17:39:06,630 - INFO - Epoch: 1/2, Iter: 36/41 -- train_loss: 2.7110 \n",
            "2024-03-05 17:39:32,710 - INFO - Epoch: 1/2, Iter: 37/41 -- train_loss: 1.1547 \n",
            "2024-03-05 17:39:49,575 - INFO - Epoch: 1/2, Iter: 38/41 -- train_loss: 1.1427 \n",
            "2024-03-05 17:40:12,661 - INFO - Epoch: 1/2, Iter: 39/41 -- train_loss: 1.0993 \n",
            "2024-03-05 17:40:27,436 - INFO - Epoch: 1/2, Iter: 40/41 -- train_loss: 1.1085 \n",
            "2024-03-05 17:40:44,826 - INFO - Epoch: 1/2, Iter: 41/41 -- train_loss: 1.2024 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.utils generate_spatial_bounding_box:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-05 17:45:27,210 - INFO - Epoch[1] Metrics -- val_mean_dice: 0.0996 \n",
            "2024-03-05 17:45:27,212 - INFO - Key metric: val_mean_dice best value: 0.09959621727466583 at epoch: 1\n",
            "2024-03-05 17:45:44,671 - INFO - Epoch: 2/2, Iter: 1/41 -- train_loss: 1.1867 \n",
            "2024-03-05 17:45:58,433 - INFO - Epoch: 2/2, Iter: 2/41 -- train_loss: 1.1913 \n",
            "2024-03-05 17:46:14,461 - INFO - Epoch: 2/2, Iter: 3/41 -- train_loss: 1.4787 \n",
            "2024-03-05 17:46:36,721 - INFO - Epoch: 2/2, Iter: 4/41 -- train_loss: 1.4172 \n",
            "2024-03-05 17:46:55,119 - INFO - Epoch: 2/2, Iter: 5/41 -- train_loss: 1.1540 \n",
            "2024-03-05 17:47:15,959 - INFO - Epoch: 2/2, Iter: 6/41 -- train_loss: 1.0461 \n",
            "2024-03-05 17:47:43,498 - INFO - Epoch: 2/2, Iter: 7/41 -- train_loss: 1.0623 \n",
            "2024-03-05 17:48:02,855 - INFO - Epoch: 2/2, Iter: 8/41 -- train_loss: 1.1820 \n",
            "2024-03-05 17:48:20,654 - INFO - Epoch: 2/2, Iter: 9/41 -- train_loss: 1.0753 \n",
            "2024-03-05 17:48:37,279 - INFO - Epoch: 2/2, Iter: 10/41 -- train_loss: 1.2247 \n",
            "2024-03-05 17:48:54,659 - INFO - Epoch: 2/2, Iter: 11/41 -- train_loss: 1.2695 \n",
            "2024-03-05 17:49:13,515 - INFO - Epoch: 2/2, Iter: 12/41 -- train_loss: 1.1408 \n",
            "2024-03-05 17:49:36,614 - INFO - Epoch: 2/2, Iter: 13/41 -- train_loss: 1.1424 \n",
            "2024-03-05 17:49:49,039 - INFO - Epoch: 2/2, Iter: 14/41 -- train_loss: 1.1909 \n",
            "2024-03-05 17:50:07,473 - INFO - Epoch: 2/2, Iter: 15/41 -- train_loss: 1.2980 \n",
            "2024-03-05 17:50:21,785 - INFO - Epoch: 2/2, Iter: 16/41 -- train_loss: 1.6548 \n",
            "2024-03-05 17:50:37,736 - INFO - Epoch: 2/2, Iter: 17/41 -- train_loss: 1.1156 \n",
            "2024-03-05 17:50:58,624 - INFO - Epoch: 2/2, Iter: 18/41 -- train_loss: 1.0443 \n",
            "2024-03-05 17:51:18,126 - INFO - Epoch: 2/2, Iter: 19/41 -- train_loss: 0.9846 \n",
            "2024-03-05 17:51:37,853 - INFO - Epoch: 2/2, Iter: 20/41 -- train_loss: 1.1373 \n",
            "2024-03-05 17:51:57,720 - INFO - Epoch: 2/2, Iter: 21/41 -- train_loss: 1.7933 \n",
            "2024-03-05 17:52:18,721 - INFO - Epoch: 2/2, Iter: 22/41 -- train_loss: 0.9701 \n",
            "2024-03-05 17:52:32,531 - INFO - Epoch: 2/2, Iter: 23/41 -- train_loss: 1.2453 \n",
            "2024-03-05 17:52:53,308 - INFO - Epoch: 2/2, Iter: 24/41 -- train_loss: 0.8961 \n",
            "2024-03-05 17:53:17,187 - INFO - Epoch: 2/2, Iter: 25/41 -- train_loss: 1.0704 \n",
            "2024-03-05 17:53:33,342 - INFO - Epoch: 2/2, Iter: 26/41 -- train_loss: 1.2425 \n",
            "2024-03-05 17:53:48,822 - INFO - Epoch: 2/2, Iter: 27/41 -- train_loss: 1.0444 \n",
            "2024-03-05 17:54:05,697 - INFO - Epoch: 2/2, Iter: 28/41 -- train_loss: 1.1084 \n",
            "2024-03-05 17:54:24,609 - INFO - Epoch: 2/2, Iter: 29/41 -- train_loss: 1.1383 \n",
            "2024-03-05 17:54:44,243 - INFO - Epoch: 2/2, Iter: 30/41 -- train_loss: 1.9043 \n",
            "2024-03-05 17:55:00,073 - INFO - Epoch: 2/2, Iter: 31/41 -- train_loss: 1.3929 \n",
            "2024-03-05 17:55:16,827 - INFO - Epoch: 2/2, Iter: 32/41 -- train_loss: 1.3544 \n",
            "2024-03-05 17:55:38,943 - INFO - Epoch: 2/2, Iter: 33/41 -- train_loss: 1.2191 \n",
            "2024-03-05 17:55:53,907 - INFO - Epoch: 2/2, Iter: 34/41 -- train_loss: 1.1690 \n",
            "2024-03-05 17:56:13,364 - INFO - Epoch: 2/2, Iter: 35/41 -- train_loss: 1.1504 \n",
            "2024-03-05 17:56:35,963 - INFO - Epoch: 2/2, Iter: 36/41 -- train_loss: 1.6006 \n",
            "2024-03-05 17:57:01,884 - INFO - Epoch: 2/2, Iter: 37/41 -- train_loss: 1.2967 \n",
            "2024-03-05 17:57:20,889 - INFO - Epoch: 2/2, Iter: 38/41 -- train_loss: 1.6110 \n",
            "2024-03-05 17:57:53,314 - INFO - Epoch: 2/2, Iter: 39/41 -- train_loss: 1.2403 \n",
            "2024-03-05 17:58:14,937 - INFO - Epoch: 2/2, Iter: 40/41 -- train_loss: 1.5083 \n",
            "2024-03-05 17:58:36,072 - INFO - Epoch: 2/2, Iter: 41/41 -- train_loss: 1.1872 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.utils generate_spatial_bounding_box:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-05 18:03:29,545 - INFO - Epoch[2] Metrics -- val_mean_dice: 0.1352 \n",
            "2024-03-05 18:03:29,547 - INFO - Key metric: val_mean_dice best value: 0.1351882368326187 at epoch: 2\n",
            "train end\n",
            "['net_key_metric=0.1352.pt', 'support_data.csv', 'Dataset247_monaiData', 'net_key_metric=0.1264.pt']\n",
            "net_key_metric=0.1264.pt\n",
            "run SwinNext cross\n",
            "load prepath: ./runs_08_fold0_expr/net_key_metric=0.1264.pt\n",
            "pretrained checkpoint: ./runs_08_fold0_expr/net_key_metric=0.1264.pt loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n",
            "/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.utils generate_spatial_bounding_box:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
            "  warn_deprecated(argname, msg, warning_category)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "save HCC_067_0000.nrrd with shape: (512, 512, 75), mean values: 0.12852437337239583\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_030_0000.nrrd with shape: (512, 512, 89), mean values: 0.09447586402464449\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_037_0000.nrrd with shape: (512, 512, 103), mean values: 0.08643766977254627\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_063_0000.nrrd with shape: (512, 512, 99), mean values: 0.06714872880415483\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_101_0000.nrrd with shape: (512, 512, 91), mean values: 0.1338785611666166\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_073_0000.nrrd with shape: (512, 512, 105), mean values: 0.1330745152064732\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_039_0000.nrrd with shape: (512, 512, 77), mean values: 0.09715018334326805\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_064_0000.nrrd with shape: (512, 512, 109), mean values: 0.12550361003350774\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_088_0000.nrrd with shape: (512, 512, 81), mean values: 0.06127129072024499\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_024_0000.nrrd with shape: (512, 512, 65), mean values: 0.09884796142578126\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_012_0000.nrrd with shape: (512, 512, 71), mean values: 0.1004270634181063\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_068_0000.nrrd with shape: (512, 512, 71), mean values: 0.0957948389187665\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_079_0000.nrrd with shape: (512, 512, 75), mean values: 0.09900426228841146\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_099_0000.nrrd with shape: (512, 512, 109), mean values: 0.10965588770875144\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_062_0000.nrrd with shape: (512, 512, 111), mean values: 0.07728679760082348\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_093_0000.nrrd with shape: (512, 512, 103), mean values: 0.05743700786701684\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_034_0000.nrrd with shape: (512, 512, 71), mean values: 0.06277041368081536\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_060_0000.nrrd with shape: (512, 512, 91), mean values: 0.09926974118410886\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_105_0000.nrrd with shape: (512, 512, 81), mean values: 0.11254670884874132\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_019_0000.nrrd with shape: (512, 512, 67), mean values: 0.15622244308243938\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_052_0000.nrrd with shape: (512, 512, 121), mean values: 0.10399539411560563\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "save HCC_083_0000.nrrd with shape: (512, 512, 95), mean values: 0.0806651466771176\n",
            "./runs_08_fold0_expr/Dataset247_monaiData\n",
            "前往./ ./runs_08_fold0_expr/ /Dataset247_monaiData 查看结果\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 3000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "def load_data(data_path,label_name,train_list):\n",
        "    data_dicts = []\n",
        "    for path in train_list:\n",
        "      image_path=os.path.join(data_path,path)\n",
        "      split_path=path.split('/')\n",
        "      patients=split_path[0]\n",
        "      # print(patients)\n",
        "      if patients in ['HCC_017', 'HCC_025', 'HCC_009', 'HCC_008']:\n",
        "          print('排除问题数据:', path)\n",
        "          continue\n",
        "      # if path == 'HCC_008/103 3 PHASE LIVER (ABD) - acquisitionNumber 1.nrrd':\n",
        "      #     print('排除问题数据*:', path)\n",
        "      #     continue\n",
        "\n",
        "      label_path=os.path.join(data_path,patients, label_name)\n",
        "\n",
        "      data_dicts.append({'image': image_path, 'label': label_path})\n",
        "    return data_dicts\n",
        "\n",
        "def model_selection(file_path):\n",
        "\n",
        "    file_list = os.listdir(file_path)\n",
        "    print(file_list)\n",
        "    max_dice_value = 0\n",
        "    select_index = 1\n",
        "    for idx, file_name in enumerate(file_list):\n",
        "\n",
        "        if 'net_key_metric' in file_name:\n",
        "            # print('ok new')\n",
        "            dice_value_str = file_name.split('=')[1]\n",
        "            # print(dice_value_str)\n",
        "            dice_value = float(dice_value_str.split('.p')[0])\n",
        "            # print(dice_value)\n",
        "            if dice_value> max_dice_value:\n",
        "                select_index = idx\n",
        "\n",
        "    select_name = file_list[select_index]\n",
        "\n",
        "    return select_name\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open('./Train.txt', 'r') as filetrain:\n",
        "        train_list = filetrain.read().split(',')\n",
        "\n",
        "    with open('./Test.txt', 'r') as filetest:\n",
        "        test_list = filetest.read().split(',')\n",
        "\n",
        "    # base_path = '/root/autodl-tmp/dataset'\n",
        "    data_path = './drive/MyDrive/HCC'\n",
        "    base_path = data_path.replace('/HCC', '')\n",
        "    table_save_path = os.path.join(base_path, 'support_data.csv')\n",
        "    input_csv_path = os.path.join(data_path, 'HCC-TACE-Seg_clinical_data-V2.xlsx')\n",
        "\n",
        "    input_base_path_1 = \"./runs_{}_fold{}_{}/\".format('08', '0', 'expr')\n",
        "    os.makedirs(input_base_path_1, exist_ok=True)\n",
        "    input_load_data_path = os.path.join(input_base_path_1, 'support_data.csv')\n",
        "    csv_support_computer(input_csv_path, table_save_path, input_load_data_path)\n",
        "    label_name = \"Segmentation.seg.nrrd\"\n",
        "    data_monai_dataset = os.path.join(base_path, 'Dataset247_monaiData')\n",
        "    os.makedirs(data_monai_dataset, exist_ok=True)\n",
        "    save_dataPath_label = os.path.join(data_monai_dataset, 'labelsTr')\n",
        "    os.makedirs(save_dataPath_label, exist_ok=True)\n",
        "    save_dataPath_image = os.path.join(data_monai_dataset, 'imagesTr')\n",
        "    os.makedirs(save_dataPath_image, exist_ok=True)\n",
        "    save_dataPath_imageTs = os.path.join(data_monai_dataset, 'imagesTs')\n",
        "    os.makedirs(save_dataPath_imageTs, exist_ok=True)\n",
        "    save_dataPath_labelTs = os.path.join(data_monai_dataset, 'labelsTs')\n",
        "    os.makedirs(save_dataPath_labelTs, exist_ok=True)\n",
        "    json_save_file = os.path.join(data_monai_dataset, 'dataset.json')\n",
        "\n",
        "    train_data_dicts = load_data(data_path, label_name, train_list)\n",
        "    test_data_dicts = load_data(data_path, label_name, test_list)\n",
        "    print(len(train_data_dicts), len(test_data_dicts))\n",
        "\n",
        "    json_save_file_net = os.path.join(base_path, 'dataset_task08.json')\n",
        "    dataset_json_dict = {\n",
        "        \"modality\": {\n",
        "            \"0\": \"CT\"\n",
        "        },\n",
        "        \"labels\": {\n",
        "            \"0\": \"background\",\n",
        "            \"1\": \"liver\",\n",
        "            \"2\": \"tumor\",\n",
        "            \"3\": \"portal_vein\",\n",
        "            \"4\": \"abdominal aorta\"\n",
        "        },\n",
        "        \"file_ending\": \".nrrd\",\n",
        "        \"name\": \"hccMonai\",\n",
        "        \"reference\": \"none\",\n",
        "        \"release\": \"prerelease\",\n",
        "        \"overwrite_image_reader_writer\": \"SimpleITKIO\"\n",
        "    }\n",
        "\n",
        "\n",
        "    check_dict = {}\n",
        "    train_list = []\n",
        "    for idx, train_dict in enumerate(train_data_dicts):\n",
        "\n",
        "        print(idx, '')\n",
        "        tag_value = train_dict['image'].split('/')[-2].split('_')[1]\n",
        "        # if  tag_value == '030':\n",
        "        #     print('end')\n",
        "            # continue\n",
        "        if tag_value in check_dict.keys():\n",
        "\n",
        "            tag_name = str(int(tag_value) + 400 + check_dict[tag_value] * 100)\n",
        "            check_dict[tag_value] += 1\n",
        "            print(tag_name, tag_value)\n",
        "        else:\n",
        "            check_dict[tag_value] = 1\n",
        "            tag_name = str(tag_value).zfill(3)\n",
        "\n",
        "        nii_label_path = os.path.join(save_dataPath_label, 'HCC_' + tag_name  + '.nrrd')\n",
        "        nii_image_path = os.path.join(save_dataPath_image, 'HCC_' + tag_name + '_0000.nrrd')\n",
        "\n",
        "        shutil.copy2(train_dict['image'], nii_image_path)\n",
        "        shutil.copy2(train_dict['label'], nii_label_path)\n",
        "\n",
        "        train_info_dict = {'image': nii_image_path,\n",
        "                           'label': nii_label_path}\n",
        "        train_list.append(copy.deepcopy(train_info_dict))\n",
        "\n",
        "    dataset_json_dict['training'] = train_list\n",
        "\n",
        "\n",
        "\n",
        "    check_dict = {}\n",
        "    val_list = []\n",
        "    for idx, test_dict in enumerate(test_data_dicts):\n",
        "        tag_value = test_dict['image'].split('/')[-2].split('_')[1]\n",
        "\n",
        "        if tag_value in check_dict.keys():\n",
        "\n",
        "            tag_name = str(int(tag_value) + 400 + check_dict[tag_value] * 100)\n",
        "\n",
        "            check_dict[tag_value] += 1\n",
        "            print(tag_name, tag_value)\n",
        "        else:\n",
        "            check_dict[tag_value] = 1\n",
        "            tag_name = str(tag_value).zfill(3)\n",
        "\n",
        "        nii_image_path = os.path.join(save_dataPath_imageTs, 'HCC_' + tag_name + '_0000.nrrd')\n",
        "        nii_label_path = os.path.join(save_dataPath_labelTs, 'HCC_' + tag_name + '.nrrd')\n",
        "\n",
        "        val_info_dict = {'image': nii_image_path,\n",
        "                           'label': nii_label_path}\n",
        "        shutil.copy2(test_dict['image'], nii_image_path)\n",
        "        shutil.copy2(test_dict['label'], nii_label_path)\n",
        "\n",
        "        val_list.append(copy.deepcopy(val_info_dict))\n",
        "\n",
        "    with open(json_save_file, 'w') as json_file:\n",
        "        json.dump(dataset_json_dict, json_file)\n",
        "\n",
        "    dataset_json_dict['train_fold0'] = train_list\n",
        "    dataset_json_dict['validation_fold0'] = val_list\n",
        "\n",
        "    with open(json_save_file_net, 'w') as json_file:\n",
        "        json.dump(dataset_json_dict, json_file)\n",
        "    print('end')\n",
        "\n",
        "    # 开始模型训练\n",
        "    print_config()\n",
        "    root_path = base_path\n",
        "    train(root_path, max_epochs_value=1000)\n",
        "    print('train end')\n",
        "\n",
        "    # 开始模型推理\n",
        "    model_save_path = \"./runs_{}_fold{}_{}/\".format('08', '0', 'expr')\n",
        "    model_name = model_selection(model_save_path)\n",
        "    print(model_name)\n",
        "    inference(model_name, root_path)\n",
        "    print('前往./',model_save_path, '/Dataset247_monaiData 查看结果')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
